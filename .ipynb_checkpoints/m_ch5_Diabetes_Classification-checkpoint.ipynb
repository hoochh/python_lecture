{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad20be4-c72b-4415-972d-96c1cce41387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac79dc90-1172-4ee3-a63b-55fb13203d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded_data.shape =  (759, 9)\n",
      "training_x_data.shape =  (532, 8)\n",
      "training_t_data.shape =  (532, 1)\n",
      "test_x_data.shape =  (227, 8)\n",
      "test_t_data.shape =  (227, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "loaded_data=np.loadtxt('data/diabetes.csv', delimiter=',')\n",
    "\n",
    "#training data / test data 분리\n",
    "seperation_rate=0.3 # 분리비율\n",
    "test_data_num=int(len(loaded_data)*seperation_rate)\n",
    "\n",
    "np.random.shuffle(loaded_data)\n",
    "\n",
    "test_data=loaded_data[0:test_data_num]\n",
    "training_data=loaded_data[test_data_num:]\n",
    "\n",
    "# training_x_data / training_t_data 생성\n",
    "training_x_data=training_data[:,0:-1]\n",
    "training_t_data=training_data[:,[-1]]\n",
    "\n",
    "#test_x_data/test_t_data 생성\n",
    "test_x_data=test_data[:,0:-1]\n",
    "test_t_data=test_data[:,[-1]]\n",
    "\n",
    "print(\"loaded_data.shape = \", loaded_data.shape)\n",
    "print(\"training_x_data.shape = \", training_x_data.shape)\n",
    "print(\"training_t_data.shape = \", training_t_data.shape)\n",
    "\n",
    "print(\"test_x_data.shape = \", test_x_data.shape)\n",
    "print(\"test_t_data.shape = \", test_t_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01f6a89-5b07-434d-8b90-678ceca6ab1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#logistic Regression을 keras 이용하여 생성\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "# 노드 1개인 출력층 생성\n",
    "model.add(Dense(training_t_data.shape[1], #2차원 데이터를 1차원으로 펼침\n",
    "                input_shape=(training_x_data.shape[1],), \n",
    "                activation='sigmoid'))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eb9065b-d760-402d-bf3c-0fcc3c16c285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 학습을 위한 optimizer, 손실함수 loss 정의\n",
    "model.compile(optimizer=SGD(learning_rate=0.01), #SGD \n",
    "              loss='binary_crossentropy', #분류하는 값이 두 가지\n",
    "              metrics=['accuracy']) #정확도 metrics 사용\n",
    "model.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2eb2b5-596d-4d2f-89cb-f6fd5915714b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "14/14 - 1s - loss: 0.7487 - accuracy: 0.4565 - val_loss: 0.7633 - val_accuracy: 0.3645 - 695ms/epoch - 50ms/step\n",
      "Epoch 2/500\n",
      "14/14 - 0s - loss: 0.7356 - accuracy: 0.4871 - val_loss: 0.7464 - val_accuracy: 0.4112 - 37ms/epoch - 3ms/step\n",
      "Epoch 3/500\n",
      "14/14 - 0s - loss: 0.7243 - accuracy: 0.5059 - val_loss: 0.7310 - val_accuracy: 0.4673 - 39ms/epoch - 3ms/step\n",
      "Epoch 4/500\n",
      "14/14 - 0s - loss: 0.7140 - accuracy: 0.4965 - val_loss: 0.7179 - val_accuracy: 0.5140 - 38ms/epoch - 3ms/step\n",
      "Epoch 5/500\n",
      "14/14 - 0s - loss: 0.7054 - accuracy: 0.5224 - val_loss: 0.7053 - val_accuracy: 0.5421 - 39ms/epoch - 3ms/step\n",
      "Epoch 6/500\n",
      "14/14 - 0s - loss: 0.6973 - accuracy: 0.5247 - val_loss: 0.6945 - val_accuracy: 0.5701 - 36ms/epoch - 3ms/step\n",
      "Epoch 7/500\n",
      "14/14 - 0s - loss: 0.6903 - accuracy: 0.5459 - val_loss: 0.6843 - val_accuracy: 0.5794 - 35ms/epoch - 2ms/step\n",
      "Epoch 8/500\n",
      "14/14 - 0s - loss: 0.6840 - accuracy: 0.5412 - val_loss: 0.6757 - val_accuracy: 0.5981 - 33ms/epoch - 2ms/step\n",
      "Epoch 9/500\n",
      "14/14 - 0s - loss: 0.6785 - accuracy: 0.5576 - val_loss: 0.6669 - val_accuracy: 0.5888 - 33ms/epoch - 2ms/step\n",
      "Epoch 10/500\n",
      "14/14 - 0s - loss: 0.6732 - accuracy: 0.5718 - val_loss: 0.6599 - val_accuracy: 0.5981 - 36ms/epoch - 3ms/step\n",
      "Epoch 11/500\n",
      "14/14 - 0s - loss: 0.6689 - accuracy: 0.5812 - val_loss: 0.6530 - val_accuracy: 0.6262 - 36ms/epoch - 3ms/step\n",
      "Epoch 12/500\n",
      "14/14 - 0s - loss: 0.6647 - accuracy: 0.5788 - val_loss: 0.6473 - val_accuracy: 0.6449 - 34ms/epoch - 2ms/step\n",
      "Epoch 13/500\n",
      "14/14 - 0s - loss: 0.6613 - accuracy: 0.5859 - val_loss: 0.6416 - val_accuracy: 0.6542 - 39ms/epoch - 3ms/step\n",
      "Epoch 14/500\n",
      "14/14 - 0s - loss: 0.6579 - accuracy: 0.5929 - val_loss: 0.6361 - val_accuracy: 0.6542 - 39ms/epoch - 3ms/step\n",
      "Epoch 15/500\n",
      "14/14 - 0s - loss: 0.6547 - accuracy: 0.5835 - val_loss: 0.6319 - val_accuracy: 0.6449 - 37ms/epoch - 3ms/step\n",
      "Epoch 16/500\n",
      "14/14 - 0s - loss: 0.6521 - accuracy: 0.5835 - val_loss: 0.6281 - val_accuracy: 0.6449 - 35ms/epoch - 2ms/step\n",
      "Epoch 17/500\n",
      "14/14 - 0s - loss: 0.6498 - accuracy: 0.5788 - val_loss: 0.6244 - val_accuracy: 0.6542 - 38ms/epoch - 3ms/step\n",
      "Epoch 18/500\n",
      "14/14 - 0s - loss: 0.6475 - accuracy: 0.5929 - val_loss: 0.6217 - val_accuracy: 0.6636 - 35ms/epoch - 2ms/step\n",
      "Epoch 19/500\n",
      "14/14 - 0s - loss: 0.6457 - accuracy: 0.6000 - val_loss: 0.6189 - val_accuracy: 0.6636 - 37ms/epoch - 3ms/step\n",
      "Epoch 20/500\n",
      "14/14 - 0s - loss: 0.6439 - accuracy: 0.6000 - val_loss: 0.6165 - val_accuracy: 0.6636 - 34ms/epoch - 2ms/step\n",
      "Epoch 21/500\n",
      "14/14 - 0s - loss: 0.6422 - accuracy: 0.6024 - val_loss: 0.6139 - val_accuracy: 0.6636 - 35ms/epoch - 2ms/step\n",
      "Epoch 22/500\n",
      "14/14 - 0s - loss: 0.6406 - accuracy: 0.6047 - val_loss: 0.6115 - val_accuracy: 0.6636 - 35ms/epoch - 2ms/step\n",
      "Epoch 23/500\n",
      "14/14 - 0s - loss: 0.6390 - accuracy: 0.6212 - val_loss: 0.6088 - val_accuracy: 0.6636 - 34ms/epoch - 2ms/step\n",
      "Epoch 24/500\n",
      "14/14 - 0s - loss: 0.6373 - accuracy: 0.6212 - val_loss: 0.6069 - val_accuracy: 0.6729 - 35ms/epoch - 2ms/step\n",
      "Epoch 25/500\n",
      "14/14 - 0s - loss: 0.6359 - accuracy: 0.6259 - val_loss: 0.6047 - val_accuracy: 0.6729 - 33ms/epoch - 2ms/step\n",
      "Epoch 26/500\n",
      "14/14 - 0s - loss: 0.6345 - accuracy: 0.6306 - val_loss: 0.6024 - val_accuracy: 0.6636 - 35ms/epoch - 2ms/step\n",
      "Epoch 27/500\n",
      "14/14 - 0s - loss: 0.6331 - accuracy: 0.6259 - val_loss: 0.6003 - val_accuracy: 0.6636 - 36ms/epoch - 3ms/step\n",
      "Epoch 28/500\n",
      "14/14 - 0s - loss: 0.6317 - accuracy: 0.6353 - val_loss: 0.5986 - val_accuracy: 0.6636 - 34ms/epoch - 2ms/step\n",
      "Epoch 29/500\n",
      "14/14 - 0s - loss: 0.6306 - accuracy: 0.6353 - val_loss: 0.5968 - val_accuracy: 0.6636 - 35ms/epoch - 2ms/step\n",
      "Epoch 30/500\n",
      "14/14 - 0s - loss: 0.6294 - accuracy: 0.6353 - val_loss: 0.5953 - val_accuracy: 0.6636 - 36ms/epoch - 3ms/step\n",
      "Epoch 31/500\n",
      "14/14 - 0s - loss: 0.6282 - accuracy: 0.6353 - val_loss: 0.5935 - val_accuracy: 0.6729 - 34ms/epoch - 2ms/step\n",
      "Epoch 32/500\n",
      "14/14 - 0s - loss: 0.6271 - accuracy: 0.6329 - val_loss: 0.5917 - val_accuracy: 0.6822 - 37ms/epoch - 3ms/step\n",
      "Epoch 33/500\n",
      "14/14 - 0s - loss: 0.6260 - accuracy: 0.6306 - val_loss: 0.5903 - val_accuracy: 0.6822 - 37ms/epoch - 3ms/step\n",
      "Epoch 34/500\n",
      "14/14 - 0s - loss: 0.6249 - accuracy: 0.6306 - val_loss: 0.5892 - val_accuracy: 0.6822 - 35ms/epoch - 2ms/step\n",
      "Epoch 35/500\n",
      "14/14 - 0s - loss: 0.6239 - accuracy: 0.6306 - val_loss: 0.5879 - val_accuracy: 0.6822 - 35ms/epoch - 2ms/step\n",
      "Epoch 36/500\n",
      "14/14 - 0s - loss: 0.6227 - accuracy: 0.6306 - val_loss: 0.5865 - val_accuracy: 0.6822 - 37ms/epoch - 3ms/step\n",
      "Epoch 37/500\n",
      "14/14 - 0s - loss: 0.6218 - accuracy: 0.6282 - val_loss: 0.5851 - val_accuracy: 0.6822 - 34ms/epoch - 2ms/step\n",
      "Epoch 38/500\n",
      "14/14 - 0s - loss: 0.6207 - accuracy: 0.6329 - val_loss: 0.5840 - val_accuracy: 0.6822 - 34ms/epoch - 2ms/step\n",
      "Epoch 39/500\n",
      "14/14 - 0s - loss: 0.6197 - accuracy: 0.6282 - val_loss: 0.5831 - val_accuracy: 0.6916 - 36ms/epoch - 3ms/step\n",
      "Epoch 40/500\n",
      "14/14 - 0s - loss: 0.6188 - accuracy: 0.6282 - val_loss: 0.5822 - val_accuracy: 0.6916 - 33ms/epoch - 2ms/step\n",
      "Epoch 41/500\n",
      "14/14 - 0s - loss: 0.6179 - accuracy: 0.6306 - val_loss: 0.5807 - val_accuracy: 0.6916 - 41ms/epoch - 3ms/step\n",
      "Epoch 42/500\n",
      "14/14 - 0s - loss: 0.6169 - accuracy: 0.6306 - val_loss: 0.5799 - val_accuracy: 0.6916 - 36ms/epoch - 3ms/step\n",
      "Epoch 43/500\n",
      "14/14 - 0s - loss: 0.6161 - accuracy: 0.6306 - val_loss: 0.5790 - val_accuracy: 0.6916 - 35ms/epoch - 2ms/step\n",
      "Epoch 44/500\n",
      "14/14 - 0s - loss: 0.6151 - accuracy: 0.6329 - val_loss: 0.5778 - val_accuracy: 0.6916 - 36ms/epoch - 3ms/step\n",
      "Epoch 45/500\n",
      "14/14 - 0s - loss: 0.6143 - accuracy: 0.6329 - val_loss: 0.5768 - val_accuracy: 0.6916 - 36ms/epoch - 3ms/step\n",
      "Epoch 46/500\n",
      "14/14 - 0s - loss: 0.6134 - accuracy: 0.6353 - val_loss: 0.5760 - val_accuracy: 0.6916 - 33ms/epoch - 2ms/step\n",
      "Epoch 47/500\n",
      "14/14 - 0s - loss: 0.6125 - accuracy: 0.6376 - val_loss: 0.5752 - val_accuracy: 0.6916 - 33ms/epoch - 2ms/step\n",
      "Epoch 48/500\n",
      "14/14 - 0s - loss: 0.6117 - accuracy: 0.6353 - val_loss: 0.5742 - val_accuracy: 0.6916 - 76ms/epoch - 5ms/step\n",
      "Epoch 49/500\n",
      "14/14 - 0s - loss: 0.6109 - accuracy: 0.6353 - val_loss: 0.5734 - val_accuracy: 0.7009 - 38ms/epoch - 3ms/step\n",
      "Epoch 50/500\n",
      "14/14 - 0s - loss: 0.6100 - accuracy: 0.6353 - val_loss: 0.5729 - val_accuracy: 0.7009 - 36ms/epoch - 3ms/step\n",
      "Epoch 51/500\n",
      "14/14 - 0s - loss: 0.6093 - accuracy: 0.6353 - val_loss: 0.5720 - val_accuracy: 0.7009 - 34ms/epoch - 2ms/step\n",
      "Epoch 52/500\n",
      "14/14 - 0s - loss: 0.6084 - accuracy: 0.6353 - val_loss: 0.5716 - val_accuracy: 0.7009 - 35ms/epoch - 2ms/step\n",
      "Epoch 53/500\n",
      "14/14 - 0s - loss: 0.6076 - accuracy: 0.6353 - val_loss: 0.5707 - val_accuracy: 0.7009 - 35ms/epoch - 2ms/step\n",
      "Epoch 54/500\n",
      "14/14 - 0s - loss: 0.6068 - accuracy: 0.6353 - val_loss: 0.5700 - val_accuracy: 0.7009 - 33ms/epoch - 2ms/step\n",
      "Epoch 55/500\n",
      "14/14 - 0s - loss: 0.6060 - accuracy: 0.6376 - val_loss: 0.5695 - val_accuracy: 0.7009 - 36ms/epoch - 3ms/step\n",
      "Epoch 56/500\n",
      "14/14 - 0s - loss: 0.6052 - accuracy: 0.6353 - val_loss: 0.5688 - val_accuracy: 0.7009 - 36ms/epoch - 3ms/step\n",
      "Epoch 57/500\n",
      "14/14 - 0s - loss: 0.6044 - accuracy: 0.6353 - val_loss: 0.5683 - val_accuracy: 0.7009 - 35ms/epoch - 2ms/step\n",
      "Epoch 58/500\n",
      "14/14 - 0s - loss: 0.6036 - accuracy: 0.6400 - val_loss: 0.5677 - val_accuracy: 0.7009 - 36ms/epoch - 3ms/step\n",
      "Epoch 59/500\n",
      "14/14 - 0s - loss: 0.6028 - accuracy: 0.6400 - val_loss: 0.5671 - val_accuracy: 0.7009 - 35ms/epoch - 2ms/step\n",
      "Epoch 60/500\n",
      "14/14 - 0s - loss: 0.6020 - accuracy: 0.6400 - val_loss: 0.5663 - val_accuracy: 0.7009 - 36ms/epoch - 3ms/step\n",
      "Epoch 61/500\n",
      "14/14 - 0s - loss: 0.6012 - accuracy: 0.6400 - val_loss: 0.5657 - val_accuracy: 0.7009 - 32ms/epoch - 2ms/step\n",
      "Epoch 62/500\n",
      "14/14 - 0s - loss: 0.6004 - accuracy: 0.6400 - val_loss: 0.5653 - val_accuracy: 0.7009 - 30ms/epoch - 2ms/step\n",
      "Epoch 63/500\n",
      "14/14 - 0s - loss: 0.5997 - accuracy: 0.6424 - val_loss: 0.5645 - val_accuracy: 0.7009 - 30ms/epoch - 2ms/step\n",
      "Epoch 64/500\n",
      "14/14 - 0s - loss: 0.5988 - accuracy: 0.6424 - val_loss: 0.5640 - val_accuracy: 0.7009 - 31ms/epoch - 2ms/step\n",
      "Epoch 65/500\n",
      "14/14 - 0s - loss: 0.5981 - accuracy: 0.6447 - val_loss: 0.5633 - val_accuracy: 0.6916 - 32ms/epoch - 2ms/step\n",
      "Epoch 66/500\n",
      "14/14 - 0s - loss: 0.5974 - accuracy: 0.6471 - val_loss: 0.5628 - val_accuracy: 0.6916 - 31ms/epoch - 2ms/step\n",
      "Epoch 67/500\n",
      "14/14 - 0s - loss: 0.5967 - accuracy: 0.6494 - val_loss: 0.5621 - val_accuracy: 0.6916 - 32ms/epoch - 2ms/step\n",
      "Epoch 68/500\n",
      "14/14 - 0s - loss: 0.5959 - accuracy: 0.6494 - val_loss: 0.5612 - val_accuracy: 0.7103 - 35ms/epoch - 3ms/step\n",
      "Epoch 69/500\n",
      "14/14 - 0s - loss: 0.5952 - accuracy: 0.6494 - val_loss: 0.5608 - val_accuracy: 0.7009 - 34ms/epoch - 2ms/step\n",
      "Epoch 70/500\n",
      "14/14 - 0s - loss: 0.5945 - accuracy: 0.6494 - val_loss: 0.5602 - val_accuracy: 0.7009 - 37ms/epoch - 3ms/step\n",
      "Epoch 71/500\n",
      "14/14 - 0s - loss: 0.5938 - accuracy: 0.6518 - val_loss: 0.5595 - val_accuracy: 0.7009 - 34ms/epoch - 2ms/step\n",
      "Epoch 72/500\n",
      "14/14 - 0s - loss: 0.5931 - accuracy: 0.6518 - val_loss: 0.5589 - val_accuracy: 0.7103 - 33ms/epoch - 2ms/step\n",
      "Epoch 73/500\n",
      "14/14 - 0s - loss: 0.5923 - accuracy: 0.6541 - val_loss: 0.5583 - val_accuracy: 0.7103 - 33ms/epoch - 2ms/step\n",
      "Epoch 74/500\n",
      "14/14 - 0s - loss: 0.5917 - accuracy: 0.6541 - val_loss: 0.5579 - val_accuracy: 0.7103 - 33ms/epoch - 2ms/step\n",
      "Epoch 75/500\n",
      "14/14 - 0s - loss: 0.5909 - accuracy: 0.6518 - val_loss: 0.5574 - val_accuracy: 0.7196 - 32ms/epoch - 2ms/step\n",
      "Epoch 76/500\n",
      "14/14 - 0s - loss: 0.5902 - accuracy: 0.6518 - val_loss: 0.5568 - val_accuracy: 0.7290 - 32ms/epoch - 2ms/step\n",
      "Epoch 77/500\n",
      "14/14 - 0s - loss: 0.5895 - accuracy: 0.6518 - val_loss: 0.5564 - val_accuracy: 0.7290 - 35ms/epoch - 2ms/step\n",
      "Epoch 78/500\n",
      "14/14 - 0s - loss: 0.5888 - accuracy: 0.6471 - val_loss: 0.5560 - val_accuracy: 0.7290 - 33ms/epoch - 2ms/step\n",
      "Epoch 79/500\n",
      "14/14 - 0s - loss: 0.5881 - accuracy: 0.6471 - val_loss: 0.5552 - val_accuracy: 0.7290 - 33ms/epoch - 2ms/step\n",
      "Epoch 80/500\n",
      "14/14 - 0s - loss: 0.5875 - accuracy: 0.6494 - val_loss: 0.5546 - val_accuracy: 0.7290 - 34ms/epoch - 2ms/step\n",
      "Epoch 81/500\n",
      "14/14 - 0s - loss: 0.5868 - accuracy: 0.6494 - val_loss: 0.5544 - val_accuracy: 0.7290 - 34ms/epoch - 2ms/step\n",
      "Epoch 82/500\n",
      "14/14 - 0s - loss: 0.5860 - accuracy: 0.6494 - val_loss: 0.5539 - val_accuracy: 0.7290 - 34ms/epoch - 2ms/step\n",
      "Epoch 83/500\n",
      "14/14 - 0s - loss: 0.5854 - accuracy: 0.6541 - val_loss: 0.5532 - val_accuracy: 0.7290 - 34ms/epoch - 2ms/step\n",
      "Epoch 84/500\n",
      "14/14 - 0s - loss: 0.5847 - accuracy: 0.6541 - val_loss: 0.5526 - val_accuracy: 0.7290 - 34ms/epoch - 2ms/step\n",
      "Epoch 85/500\n",
      "14/14 - 0s - loss: 0.5842 - accuracy: 0.6541 - val_loss: 0.5522 - val_accuracy: 0.7290 - 33ms/epoch - 2ms/step\n",
      "Epoch 86/500\n",
      "14/14 - 0s - loss: 0.5835 - accuracy: 0.6541 - val_loss: 0.5515 - val_accuracy: 0.7290 - 33ms/epoch - 2ms/step\n",
      "Epoch 87/500\n",
      "14/14 - 0s - loss: 0.5829 - accuracy: 0.6565 - val_loss: 0.5510 - val_accuracy: 0.7290 - 33ms/epoch - 2ms/step\n",
      "Epoch 88/500\n",
      "14/14 - 0s - loss: 0.5823 - accuracy: 0.6565 - val_loss: 0.5509 - val_accuracy: 0.7290 - 32ms/epoch - 2ms/step\n",
      "Epoch 89/500\n",
      "14/14 - 0s - loss: 0.5815 - accuracy: 0.6565 - val_loss: 0.5506 - val_accuracy: 0.7290 - 33ms/epoch - 2ms/step\n",
      "Epoch 90/500\n",
      "14/14 - 0s - loss: 0.5809 - accuracy: 0.6612 - val_loss: 0.5503 - val_accuracy: 0.7290 - 35ms/epoch - 3ms/step\n",
      "Epoch 91/500\n",
      "14/14 - 0s - loss: 0.5804 - accuracy: 0.6612 - val_loss: 0.5496 - val_accuracy: 0.7290 - 33ms/epoch - 2ms/step\n",
      "Epoch 92/500\n",
      "14/14 - 0s - loss: 0.5797 - accuracy: 0.6612 - val_loss: 0.5490 - val_accuracy: 0.7290 - 34ms/epoch - 2ms/step\n",
      "Epoch 93/500\n",
      "14/14 - 0s - loss: 0.5790 - accuracy: 0.6612 - val_loss: 0.5486 - val_accuracy: 0.7290 - 32ms/epoch - 2ms/step\n",
      "Epoch 94/500\n",
      "14/14 - 0s - loss: 0.5785 - accuracy: 0.6635 - val_loss: 0.5480 - val_accuracy: 0.7383 - 31ms/epoch - 2ms/step\n",
      "Epoch 95/500\n",
      "14/14 - 0s - loss: 0.5779 - accuracy: 0.6635 - val_loss: 0.5476 - val_accuracy: 0.7383 - 31ms/epoch - 2ms/step\n",
      "Epoch 96/500\n",
      "14/14 - 0s - loss: 0.5773 - accuracy: 0.6659 - val_loss: 0.5472 - val_accuracy: 0.7383 - 32ms/epoch - 2ms/step\n",
      "Epoch 97/500\n",
      "14/14 - 0s - loss: 0.5767 - accuracy: 0.6659 - val_loss: 0.5469 - val_accuracy: 0.7383 - 31ms/epoch - 2ms/step\n",
      "Epoch 98/500\n",
      "14/14 - 0s - loss: 0.5761 - accuracy: 0.6682 - val_loss: 0.5465 - val_accuracy: 0.7383 - 33ms/epoch - 2ms/step\n",
      "Epoch 99/500\n",
      "14/14 - 0s - loss: 0.5755 - accuracy: 0.6682 - val_loss: 0.5458 - val_accuracy: 0.7383 - 34ms/epoch - 2ms/step\n",
      "Epoch 100/500\n",
      "14/14 - 0s - loss: 0.5749 - accuracy: 0.6659 - val_loss: 0.5454 - val_accuracy: 0.7383 - 35ms/epoch - 2ms/step\n",
      "Epoch 101/500\n",
      "14/14 - 0s - loss: 0.5743 - accuracy: 0.6682 - val_loss: 0.5451 - val_accuracy: 0.7383 - 33ms/epoch - 2ms/step\n",
      "Epoch 102/500\n",
      "14/14 - 0s - loss: 0.5737 - accuracy: 0.6706 - val_loss: 0.5446 - val_accuracy: 0.7477 - 32ms/epoch - 2ms/step\n",
      "Epoch 103/500\n",
      "14/14 - 0s - loss: 0.5731 - accuracy: 0.6682 - val_loss: 0.5440 - val_accuracy: 0.7477 - 31ms/epoch - 2ms/step\n",
      "Epoch 104/500\n",
      "14/14 - 0s - loss: 0.5726 - accuracy: 0.6706 - val_loss: 0.5435 - val_accuracy: 0.7570 - 31ms/epoch - 2ms/step\n",
      "Epoch 105/500\n",
      "14/14 - 0s - loss: 0.5720 - accuracy: 0.6706 - val_loss: 0.5431 - val_accuracy: 0.7570 - 31ms/epoch - 2ms/step\n",
      "Epoch 106/500\n",
      "14/14 - 0s - loss: 0.5715 - accuracy: 0.6682 - val_loss: 0.5425 - val_accuracy: 0.7570 - 32ms/epoch - 2ms/step\n",
      "Epoch 107/500\n",
      "14/14 - 0s - loss: 0.5709 - accuracy: 0.6706 - val_loss: 0.5420 - val_accuracy: 0.7570 - 35ms/epoch - 2ms/step\n",
      "Epoch 108/500\n",
      "14/14 - 0s - loss: 0.5704 - accuracy: 0.6682 - val_loss: 0.5416 - val_accuracy: 0.7570 - 34ms/epoch - 2ms/step\n",
      "Epoch 109/500\n",
      "14/14 - 0s - loss: 0.5699 - accuracy: 0.6706 - val_loss: 0.5414 - val_accuracy: 0.7570 - 33ms/epoch - 2ms/step\n",
      "Epoch 110/500\n",
      "14/14 - 0s - loss: 0.5693 - accuracy: 0.6706 - val_loss: 0.5408 - val_accuracy: 0.7570 - 32ms/epoch - 2ms/step\n",
      "Epoch 111/500\n",
      "14/14 - 0s - loss: 0.5688 - accuracy: 0.6706 - val_loss: 0.5404 - val_accuracy: 0.7570 - 32ms/epoch - 2ms/step\n",
      "Epoch 112/500\n",
      "14/14 - 0s - loss: 0.5683 - accuracy: 0.6753 - val_loss: 0.5397 - val_accuracy: 0.7570 - 32ms/epoch - 2ms/step\n",
      "Epoch 113/500\n",
      "14/14 - 0s - loss: 0.5678 - accuracy: 0.6753 - val_loss: 0.5393 - val_accuracy: 0.7664 - 33ms/epoch - 2ms/step\n",
      "Epoch 114/500\n",
      "14/14 - 0s - loss: 0.5672 - accuracy: 0.6753 - val_loss: 0.5388 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 115/500\n",
      "14/14 - 0s - loss: 0.5666 - accuracy: 0.6776 - val_loss: 0.5386 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 116/500\n",
      "14/14 - 0s - loss: 0.5661 - accuracy: 0.6753 - val_loss: 0.5385 - val_accuracy: 0.7664 - 33ms/epoch - 2ms/step\n",
      "Epoch 117/500\n",
      "14/14 - 0s - loss: 0.5656 - accuracy: 0.6800 - val_loss: 0.5382 - val_accuracy: 0.7570 - 32ms/epoch - 2ms/step\n",
      "Epoch 118/500\n",
      "14/14 - 0s - loss: 0.5651 - accuracy: 0.6776 - val_loss: 0.5378 - val_accuracy: 0.7570 - 34ms/epoch - 2ms/step\n",
      "Epoch 119/500\n",
      "14/14 - 0s - loss: 0.5646 - accuracy: 0.6776 - val_loss: 0.5375 - val_accuracy: 0.7477 - 34ms/epoch - 2ms/step\n",
      "Epoch 120/500\n",
      "14/14 - 0s - loss: 0.5641 - accuracy: 0.6776 - val_loss: 0.5370 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 121/500\n",
      "14/14 - 0s - loss: 0.5637 - accuracy: 0.6800 - val_loss: 0.5366 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 122/500\n",
      "14/14 - 0s - loss: 0.5631 - accuracy: 0.6800 - val_loss: 0.5363 - val_accuracy: 0.7477 - 31ms/epoch - 2ms/step\n",
      "Epoch 123/500\n",
      "14/14 - 0s - loss: 0.5626 - accuracy: 0.6800 - val_loss: 0.5359 - val_accuracy: 0.7477 - 32ms/epoch - 2ms/step\n",
      "Epoch 124/500\n",
      "14/14 - 0s - loss: 0.5621 - accuracy: 0.6800 - val_loss: 0.5354 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 125/500\n",
      "14/14 - 0s - loss: 0.5617 - accuracy: 0.6800 - val_loss: 0.5352 - val_accuracy: 0.7477 - 32ms/epoch - 2ms/step\n",
      "Epoch 126/500\n",
      "14/14 - 0s - loss: 0.5612 - accuracy: 0.6824 - val_loss: 0.5350 - val_accuracy: 0.7477 - 32ms/epoch - 2ms/step\n",
      "Epoch 127/500\n",
      "14/14 - 0s - loss: 0.5607 - accuracy: 0.6800 - val_loss: 0.5346 - val_accuracy: 0.7477 - 35ms/epoch - 2ms/step\n",
      "Epoch 128/500\n",
      "14/14 - 0s - loss: 0.5602 - accuracy: 0.6847 - val_loss: 0.5345 - val_accuracy: 0.7477 - 32ms/epoch - 2ms/step\n",
      "Epoch 129/500\n",
      "14/14 - 0s - loss: 0.5597 - accuracy: 0.6847 - val_loss: 0.5343 - val_accuracy: 0.7477 - 31ms/epoch - 2ms/step\n",
      "Epoch 130/500\n",
      "14/14 - 0s - loss: 0.5593 - accuracy: 0.6847 - val_loss: 0.5339 - val_accuracy: 0.7477 - 34ms/epoch - 2ms/step\n",
      "Epoch 131/500\n",
      "14/14 - 0s - loss: 0.5588 - accuracy: 0.6847 - val_loss: 0.5337 - val_accuracy: 0.7477 - 32ms/epoch - 2ms/step\n",
      "Epoch 132/500\n",
      "14/14 - 0s - loss: 0.5583 - accuracy: 0.6847 - val_loss: 0.5334 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 133/500\n",
      "14/14 - 0s - loss: 0.5578 - accuracy: 0.6871 - val_loss: 0.5331 - val_accuracy: 0.7477 - 32ms/epoch - 2ms/step\n",
      "Epoch 134/500\n",
      "14/14 - 0s - loss: 0.5574 - accuracy: 0.6894 - val_loss: 0.5325 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 135/500\n",
      "14/14 - 0s - loss: 0.5569 - accuracy: 0.6894 - val_loss: 0.5321 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 136/500\n",
      "14/14 - 0s - loss: 0.5564 - accuracy: 0.6894 - val_loss: 0.5317 - val_accuracy: 0.7477 - 32ms/epoch - 2ms/step\n",
      "Epoch 137/500\n",
      "14/14 - 0s - loss: 0.5560 - accuracy: 0.6894 - val_loss: 0.5313 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 138/500\n",
      "14/14 - 0s - loss: 0.5555 - accuracy: 0.6918 - val_loss: 0.5313 - val_accuracy: 0.7477 - 31ms/epoch - 2ms/step\n",
      "Epoch 139/500\n",
      "14/14 - 0s - loss: 0.5551 - accuracy: 0.6941 - val_loss: 0.5305 - val_accuracy: 0.7477 - 31ms/epoch - 2ms/step\n",
      "Epoch 140/500\n",
      "14/14 - 0s - loss: 0.5546 - accuracy: 0.6941 - val_loss: 0.5302 - val_accuracy: 0.7477 - 32ms/epoch - 2ms/step\n",
      "Epoch 141/500\n",
      "14/14 - 0s - loss: 0.5542 - accuracy: 0.6965 - val_loss: 0.5301 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 142/500\n",
      "14/14 - 0s - loss: 0.5538 - accuracy: 0.6941 - val_loss: 0.5300 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 143/500\n",
      "14/14 - 0s - loss: 0.5534 - accuracy: 0.6965 - val_loss: 0.5298 - val_accuracy: 0.7477 - 34ms/epoch - 2ms/step\n",
      "Epoch 144/500\n",
      "14/14 - 0s - loss: 0.5529 - accuracy: 0.6965 - val_loss: 0.5297 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 145/500\n",
      "14/14 - 0s - loss: 0.5525 - accuracy: 0.6965 - val_loss: 0.5293 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 146/500\n",
      "14/14 - 0s - loss: 0.5520 - accuracy: 0.6988 - val_loss: 0.5288 - val_accuracy: 0.7477 - 34ms/epoch - 2ms/step\n",
      "Epoch 147/500\n",
      "14/14 - 0s - loss: 0.5517 - accuracy: 0.6988 - val_loss: 0.5283 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 148/500\n",
      "14/14 - 0s - loss: 0.5512 - accuracy: 0.6988 - val_loss: 0.5278 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 149/500\n",
      "14/14 - 0s - loss: 0.5508 - accuracy: 0.7012 - val_loss: 0.5275 - val_accuracy: 0.7477 - 34ms/epoch - 2ms/step\n",
      "Epoch 150/500\n",
      "14/14 - 0s - loss: 0.5504 - accuracy: 0.7012 - val_loss: 0.5269 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 151/500\n",
      "14/14 - 0s - loss: 0.5499 - accuracy: 0.7035 - val_loss: 0.5267 - val_accuracy: 0.7477 - 36ms/epoch - 3ms/step\n",
      "Epoch 152/500\n",
      "14/14 - 0s - loss: 0.5495 - accuracy: 0.6988 - val_loss: 0.5261 - val_accuracy: 0.7477 - 34ms/epoch - 2ms/step\n",
      "Epoch 153/500\n",
      "14/14 - 0s - loss: 0.5491 - accuracy: 0.7035 - val_loss: 0.5257 - val_accuracy: 0.7477 - 36ms/epoch - 3ms/step\n",
      "Epoch 154/500\n",
      "14/14 - 0s - loss: 0.5487 - accuracy: 0.7059 - val_loss: 0.5256 - val_accuracy: 0.7477 - 34ms/epoch - 2ms/step\n",
      "Epoch 155/500\n",
      "14/14 - 0s - loss: 0.5483 - accuracy: 0.7082 - val_loss: 0.5251 - val_accuracy: 0.7477 - 34ms/epoch - 2ms/step\n",
      "Epoch 156/500\n",
      "14/14 - 0s - loss: 0.5480 - accuracy: 0.7082 - val_loss: 0.5247 - val_accuracy: 0.7477 - 33ms/epoch - 2ms/step\n",
      "Epoch 157/500\n",
      "14/14 - 0s - loss: 0.5475 - accuracy: 0.7082 - val_loss: 0.5246 - val_accuracy: 0.7570 - 33ms/epoch - 2ms/step\n",
      "Epoch 158/500\n",
      "14/14 - 0s - loss: 0.5472 - accuracy: 0.7106 - val_loss: 0.5241 - val_accuracy: 0.7570 - 33ms/epoch - 2ms/step\n",
      "Epoch 159/500\n",
      "14/14 - 0s - loss: 0.5468 - accuracy: 0.7106 - val_loss: 0.5238 - val_accuracy: 0.7570 - 36ms/epoch - 3ms/step\n",
      "Epoch 160/500\n",
      "14/14 - 0s - loss: 0.5464 - accuracy: 0.7106 - val_loss: 0.5236 - val_accuracy: 0.7570 - 33ms/epoch - 2ms/step\n",
      "Epoch 161/500\n",
      "14/14 - 0s - loss: 0.5460 - accuracy: 0.7129 - val_loss: 0.5234 - val_accuracy: 0.7570 - 35ms/epoch - 2ms/step\n",
      "Epoch 162/500\n",
      "14/14 - 0s - loss: 0.5456 - accuracy: 0.7129 - val_loss: 0.5232 - val_accuracy: 0.7570 - 33ms/epoch - 2ms/step\n",
      "Epoch 163/500\n",
      "14/14 - 0s - loss: 0.5452 - accuracy: 0.7129 - val_loss: 0.5229 - val_accuracy: 0.7570 - 32ms/epoch - 2ms/step\n",
      "Epoch 164/500\n",
      "14/14 - 0s - loss: 0.5448 - accuracy: 0.7129 - val_loss: 0.5225 - val_accuracy: 0.7570 - 33ms/epoch - 2ms/step\n",
      "Epoch 165/500\n",
      "14/14 - 0s - loss: 0.5444 - accuracy: 0.7129 - val_loss: 0.5221 - val_accuracy: 0.7570 - 34ms/epoch - 2ms/step\n",
      "Epoch 166/500\n",
      "14/14 - 0s - loss: 0.5440 - accuracy: 0.7129 - val_loss: 0.5219 - val_accuracy: 0.7570 - 33ms/epoch - 2ms/step\n",
      "Epoch 167/500\n",
      "14/14 - 0s - loss: 0.5436 - accuracy: 0.7153 - val_loss: 0.5216 - val_accuracy: 0.7570 - 33ms/epoch - 2ms/step\n",
      "Epoch 168/500\n",
      "14/14 - 0s - loss: 0.5432 - accuracy: 0.7129 - val_loss: 0.5215 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 169/500\n",
      "14/14 - 0s - loss: 0.5429 - accuracy: 0.7176 - val_loss: 0.5213 - val_accuracy: 0.7664 - 32ms/epoch - 2ms/step\n",
      "Epoch 170/500\n",
      "14/14 - 0s - loss: 0.5425 - accuracy: 0.7176 - val_loss: 0.5212 - val_accuracy: 0.7664 - 33ms/epoch - 2ms/step\n",
      "Epoch 171/500\n",
      "14/14 - 0s - loss: 0.5422 - accuracy: 0.7176 - val_loss: 0.5209 - val_accuracy: 0.7664 - 33ms/epoch - 2ms/step\n",
      "Epoch 172/500\n",
      "14/14 - 0s - loss: 0.5418 - accuracy: 0.7153 - val_loss: 0.5206 - val_accuracy: 0.7664 - 35ms/epoch - 2ms/step\n",
      "Epoch 173/500\n",
      "14/14 - 0s - loss: 0.5414 - accuracy: 0.7176 - val_loss: 0.5204 - val_accuracy: 0.7664 - 36ms/epoch - 3ms/step\n",
      "Epoch 174/500\n",
      "14/14 - 0s - loss: 0.5410 - accuracy: 0.7176 - val_loss: 0.5199 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 175/500\n",
      "14/14 - 0s - loss: 0.5407 - accuracy: 0.7176 - val_loss: 0.5197 - val_accuracy: 0.7664 - 36ms/epoch - 3ms/step\n",
      "Epoch 176/500\n",
      "14/14 - 0s - loss: 0.5403 - accuracy: 0.7176 - val_loss: 0.5192 - val_accuracy: 0.7664 - 35ms/epoch - 2ms/step\n",
      "Epoch 177/500\n",
      "14/14 - 0s - loss: 0.5399 - accuracy: 0.7200 - val_loss: 0.5188 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 178/500\n",
      "14/14 - 0s - loss: 0.5396 - accuracy: 0.7200 - val_loss: 0.5189 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 179/500\n",
      "14/14 - 0s - loss: 0.5392 - accuracy: 0.7200 - val_loss: 0.5188 - val_accuracy: 0.7664 - 33ms/epoch - 2ms/step\n",
      "Epoch 180/500\n",
      "14/14 - 0s - loss: 0.5389 - accuracy: 0.7224 - val_loss: 0.5184 - val_accuracy: 0.7664 - 31ms/epoch - 2ms/step\n",
      "Epoch 181/500\n",
      "14/14 - 0s - loss: 0.5385 - accuracy: 0.7224 - val_loss: 0.5181 - val_accuracy: 0.7664 - 31ms/epoch - 2ms/step\n",
      "Epoch 182/500\n",
      "14/14 - 0s - loss: 0.5382 - accuracy: 0.7224 - val_loss: 0.5179 - val_accuracy: 0.7664 - 30ms/epoch - 2ms/step\n",
      "Epoch 183/500\n",
      "14/14 - 0s - loss: 0.5379 - accuracy: 0.7224 - val_loss: 0.5177 - val_accuracy: 0.7664 - 32ms/epoch - 2ms/step\n",
      "Epoch 184/500\n",
      "14/14 - 0s - loss: 0.5375 - accuracy: 0.7247 - val_loss: 0.5174 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 185/500\n",
      "14/14 - 0s - loss: 0.5372 - accuracy: 0.7271 - val_loss: 0.5172 - val_accuracy: 0.7664 - 32ms/epoch - 2ms/step\n",
      "Epoch 186/500\n",
      "14/14 - 0s - loss: 0.5368 - accuracy: 0.7247 - val_loss: 0.5172 - val_accuracy: 0.7664 - 31ms/epoch - 2ms/step\n",
      "Epoch 187/500\n",
      "14/14 - 0s - loss: 0.5365 - accuracy: 0.7271 - val_loss: 0.5170 - val_accuracy: 0.7664 - 35ms/epoch - 2ms/step\n",
      "Epoch 188/500\n",
      "14/14 - 0s - loss: 0.5362 - accuracy: 0.7271 - val_loss: 0.5167 - val_accuracy: 0.7664 - 33ms/epoch - 2ms/step\n",
      "Epoch 189/500\n",
      "14/14 - 0s - loss: 0.5359 - accuracy: 0.7247 - val_loss: 0.5163 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 190/500\n",
      "14/14 - 0s - loss: 0.5355 - accuracy: 0.7271 - val_loss: 0.5161 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 191/500\n",
      "14/14 - 0s - loss: 0.5352 - accuracy: 0.7271 - val_loss: 0.5157 - val_accuracy: 0.7664 - 32ms/epoch - 2ms/step\n",
      "Epoch 192/500\n",
      "14/14 - 0s - loss: 0.5349 - accuracy: 0.7271 - val_loss: 0.5154 - val_accuracy: 0.7664 - 32ms/epoch - 2ms/step\n",
      "Epoch 193/500\n",
      "14/14 - 0s - loss: 0.5345 - accuracy: 0.7271 - val_loss: 0.5152 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 194/500\n",
      "14/14 - 0s - loss: 0.5342 - accuracy: 0.7271 - val_loss: 0.5147 - val_accuracy: 0.7664 - 33ms/epoch - 2ms/step\n",
      "Epoch 195/500\n",
      "14/14 - 0s - loss: 0.5339 - accuracy: 0.7271 - val_loss: 0.5145 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 196/500\n",
      "14/14 - 0s - loss: 0.5336 - accuracy: 0.7271 - val_loss: 0.5143 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 197/500\n",
      "14/14 - 0s - loss: 0.5332 - accuracy: 0.7271 - val_loss: 0.5142 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 198/500\n",
      "14/14 - 0s - loss: 0.5329 - accuracy: 0.7294 - val_loss: 0.5141 - val_accuracy: 0.7664 - 37ms/epoch - 3ms/step\n",
      "Epoch 199/500\n",
      "14/14 - 0s - loss: 0.5326 - accuracy: 0.7318 - val_loss: 0.5137 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 200/500\n",
      "14/14 - 0s - loss: 0.5323 - accuracy: 0.7294 - val_loss: 0.5135 - val_accuracy: 0.7664 - 31ms/epoch - 2ms/step\n",
      "Epoch 201/500\n",
      "14/14 - 0s - loss: 0.5321 - accuracy: 0.7294 - val_loss: 0.5133 - val_accuracy: 0.7664 - 30ms/epoch - 2ms/step\n",
      "Epoch 202/500\n",
      "14/14 - 0s - loss: 0.5317 - accuracy: 0.7318 - val_loss: 0.5132 - val_accuracy: 0.7664 - 33ms/epoch - 2ms/step\n",
      "Epoch 203/500\n",
      "14/14 - 0s - loss: 0.5314 - accuracy: 0.7341 - val_loss: 0.5129 - val_accuracy: 0.7664 - 32ms/epoch - 2ms/step\n",
      "Epoch 204/500\n",
      "14/14 - 0s - loss: 0.5311 - accuracy: 0.7318 - val_loss: 0.5125 - val_accuracy: 0.7664 - 32ms/epoch - 2ms/step\n",
      "Epoch 205/500\n",
      "14/14 - 0s - loss: 0.5308 - accuracy: 0.7341 - val_loss: 0.5127 - val_accuracy: 0.7664 - 33ms/epoch - 2ms/step\n",
      "Epoch 206/500\n",
      "14/14 - 0s - loss: 0.5305 - accuracy: 0.7318 - val_loss: 0.5123 - val_accuracy: 0.7664 - 36ms/epoch - 3ms/step\n",
      "Epoch 207/500\n",
      "14/14 - 0s - loss: 0.5302 - accuracy: 0.7341 - val_loss: 0.5123 - val_accuracy: 0.7664 - 31ms/epoch - 2ms/step\n",
      "Epoch 208/500\n",
      "14/14 - 0s - loss: 0.5299 - accuracy: 0.7365 - val_loss: 0.5120 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 209/500\n",
      "14/14 - 0s - loss: 0.5296 - accuracy: 0.7365 - val_loss: 0.5120 - val_accuracy: 0.7757 - 31ms/epoch - 2ms/step\n",
      "Epoch 210/500\n",
      "14/14 - 0s - loss: 0.5293 - accuracy: 0.7318 - val_loss: 0.5117 - val_accuracy: 0.7757 - 31ms/epoch - 2ms/step\n",
      "Epoch 211/500\n",
      "14/14 - 0s - loss: 0.5290 - accuracy: 0.7318 - val_loss: 0.5113 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 212/500\n",
      "14/14 - 0s - loss: 0.5287 - accuracy: 0.7341 - val_loss: 0.5113 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 213/500\n",
      "14/14 - 0s - loss: 0.5284 - accuracy: 0.7318 - val_loss: 0.5109 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 214/500\n",
      "14/14 - 0s - loss: 0.5281 - accuracy: 0.7341 - val_loss: 0.5107 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 215/500\n",
      "14/14 - 0s - loss: 0.5279 - accuracy: 0.7341 - val_loss: 0.5104 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 216/500\n",
      "14/14 - 0s - loss: 0.5275 - accuracy: 0.7341 - val_loss: 0.5102 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 217/500\n",
      "14/14 - 0s - loss: 0.5273 - accuracy: 0.7341 - val_loss: 0.5099 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 218/500\n",
      "14/14 - 0s - loss: 0.5270 - accuracy: 0.7341 - val_loss: 0.5100 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 219/500\n",
      "14/14 - 0s - loss: 0.5267 - accuracy: 0.7341 - val_loss: 0.5098 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 220/500\n",
      "14/14 - 0s - loss: 0.5265 - accuracy: 0.7318 - val_loss: 0.5093 - val_accuracy: 0.7757 - 35ms/epoch - 3ms/step\n",
      "Epoch 221/500\n",
      "14/14 - 0s - loss: 0.5262 - accuracy: 0.7341 - val_loss: 0.5091 - val_accuracy: 0.7757 - 35ms/epoch - 2ms/step\n",
      "Epoch 222/500\n",
      "14/14 - 0s - loss: 0.5258 - accuracy: 0.7341 - val_loss: 0.5092 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 223/500\n",
      "14/14 - 0s - loss: 0.5256 - accuracy: 0.7341 - val_loss: 0.5090 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 224/500\n",
      "14/14 - 0s - loss: 0.5253 - accuracy: 0.7341 - val_loss: 0.5090 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 225/500\n",
      "14/14 - 0s - loss: 0.5251 - accuracy: 0.7341 - val_loss: 0.5092 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 226/500\n",
      "14/14 - 0s - loss: 0.5249 - accuracy: 0.7318 - val_loss: 0.5090 - val_accuracy: 0.7664 - 33ms/epoch - 2ms/step\n",
      "Epoch 227/500\n",
      "14/14 - 0s - loss: 0.5246 - accuracy: 0.7318 - val_loss: 0.5088 - val_accuracy: 0.7664 - 35ms/epoch - 2ms/step\n",
      "Epoch 228/500\n",
      "14/14 - 0s - loss: 0.5243 - accuracy: 0.7341 - val_loss: 0.5084 - val_accuracy: 0.7664 - 33ms/epoch - 2ms/step\n",
      "Epoch 229/500\n",
      "14/14 - 0s - loss: 0.5240 - accuracy: 0.7341 - val_loss: 0.5083 - val_accuracy: 0.7664 - 34ms/epoch - 2ms/step\n",
      "Epoch 230/500\n",
      "14/14 - 0s - loss: 0.5238 - accuracy: 0.7341 - val_loss: 0.5083 - val_accuracy: 0.7664 - 35ms/epoch - 3ms/step\n",
      "Epoch 231/500\n",
      "14/14 - 0s - loss: 0.5235 - accuracy: 0.7388 - val_loss: 0.5079 - val_accuracy: 0.7664 - 33ms/epoch - 2ms/step\n",
      "Epoch 232/500\n",
      "14/14 - 0s - loss: 0.5232 - accuracy: 0.7388 - val_loss: 0.5078 - val_accuracy: 0.7664 - 35ms/epoch - 2ms/step\n",
      "Epoch 233/500\n",
      "14/14 - 0s - loss: 0.5230 - accuracy: 0.7388 - val_loss: 0.5078 - val_accuracy: 0.7664 - 37ms/epoch - 3ms/step\n",
      "Epoch 234/500\n",
      "14/14 - 0s - loss: 0.5227 - accuracy: 0.7412 - val_loss: 0.5076 - val_accuracy: 0.7757 - 67ms/epoch - 5ms/step\n",
      "Epoch 235/500\n",
      "14/14 - 0s - loss: 0.5225 - accuracy: 0.7412 - val_loss: 0.5077 - val_accuracy: 0.7664 - 31ms/epoch - 2ms/step\n",
      "Epoch 236/500\n",
      "14/14 - 0s - loss: 0.5222 - accuracy: 0.7435 - val_loss: 0.5076 - val_accuracy: 0.7664 - 31ms/epoch - 2ms/step\n",
      "Epoch 237/500\n",
      "14/14 - 0s - loss: 0.5220 - accuracy: 0.7435 - val_loss: 0.5074 - val_accuracy: 0.7664 - 30ms/epoch - 2ms/step\n",
      "Epoch 238/500\n",
      "14/14 - 0s - loss: 0.5217 - accuracy: 0.7435 - val_loss: 0.5070 - val_accuracy: 0.7664 - 31ms/epoch - 2ms/step\n",
      "Epoch 239/500\n",
      "14/14 - 0s - loss: 0.5215 - accuracy: 0.7435 - val_loss: 0.5068 - val_accuracy: 0.7664 - 31ms/epoch - 2ms/step\n",
      "Epoch 240/500\n",
      "14/14 - 0s - loss: 0.5213 - accuracy: 0.7459 - val_loss: 0.5064 - val_accuracy: 0.7664 - 30ms/epoch - 2ms/step\n",
      "Epoch 241/500\n",
      "14/14 - 0s - loss: 0.5210 - accuracy: 0.7435 - val_loss: 0.5063 - val_accuracy: 0.7664 - 32ms/epoch - 2ms/step\n",
      "Epoch 242/500\n",
      "14/14 - 0s - loss: 0.5207 - accuracy: 0.7459 - val_loss: 0.5059 - val_accuracy: 0.7664 - 31ms/epoch - 2ms/step\n",
      "Epoch 243/500\n",
      "14/14 - 0s - loss: 0.5205 - accuracy: 0.7459 - val_loss: 0.5057 - val_accuracy: 0.7664 - 31ms/epoch - 2ms/step\n",
      "Epoch 244/500\n",
      "14/14 - 0s - loss: 0.5203 - accuracy: 0.7435 - val_loss: 0.5058 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 245/500\n",
      "14/14 - 0s - loss: 0.5200 - accuracy: 0.7435 - val_loss: 0.5057 - val_accuracy: 0.7757 - 31ms/epoch - 2ms/step\n",
      "Epoch 246/500\n",
      "14/14 - 0s - loss: 0.5198 - accuracy: 0.7435 - val_loss: 0.5054 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 247/500\n",
      "14/14 - 0s - loss: 0.5196 - accuracy: 0.7459 - val_loss: 0.5050 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 248/500\n",
      "14/14 - 0s - loss: 0.5193 - accuracy: 0.7459 - val_loss: 0.5047 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 249/500\n",
      "14/14 - 0s - loss: 0.5191 - accuracy: 0.7459 - val_loss: 0.5046 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 250/500\n",
      "14/14 - 0s - loss: 0.5188 - accuracy: 0.7459 - val_loss: 0.5045 - val_accuracy: 0.7757 - 35ms/epoch - 2ms/step\n",
      "Epoch 251/500\n",
      "14/14 - 0s - loss: 0.5186 - accuracy: 0.7435 - val_loss: 0.5044 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 252/500\n",
      "14/14 - 0s - loss: 0.5184 - accuracy: 0.7435 - val_loss: 0.5041 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 253/500\n",
      "14/14 - 0s - loss: 0.5181 - accuracy: 0.7459 - val_loss: 0.5039 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 254/500\n",
      "14/14 - 0s - loss: 0.5179 - accuracy: 0.7459 - val_loss: 0.5038 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 255/500\n",
      "14/14 - 0s - loss: 0.5177 - accuracy: 0.7459 - val_loss: 0.5034 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 256/500\n",
      "14/14 - 0s - loss: 0.5174 - accuracy: 0.7435 - val_loss: 0.5033 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 257/500\n",
      "14/14 - 0s - loss: 0.5172 - accuracy: 0.7435 - val_loss: 0.5032 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 258/500\n",
      "14/14 - 0s - loss: 0.5170 - accuracy: 0.7459 - val_loss: 0.5029 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 259/500\n",
      "14/14 - 0s - loss: 0.5168 - accuracy: 0.7435 - val_loss: 0.5028 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 260/500\n",
      "14/14 - 0s - loss: 0.5165 - accuracy: 0.7459 - val_loss: 0.5026 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 261/500\n",
      "14/14 - 0s - loss: 0.5163 - accuracy: 0.7459 - val_loss: 0.5026 - val_accuracy: 0.7850 - 35ms/epoch - 2ms/step\n",
      "Epoch 262/500\n",
      "14/14 - 0s - loss: 0.5161 - accuracy: 0.7459 - val_loss: 0.5023 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 263/500\n",
      "14/14 - 0s - loss: 0.5159 - accuracy: 0.7459 - val_loss: 0.5020 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 264/500\n",
      "14/14 - 0s - loss: 0.5157 - accuracy: 0.7459 - val_loss: 0.5017 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 265/500\n",
      "14/14 - 0s - loss: 0.5155 - accuracy: 0.7459 - val_loss: 0.5014 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 266/500\n",
      "14/14 - 0s - loss: 0.5153 - accuracy: 0.7435 - val_loss: 0.5014 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 267/500\n",
      "14/14 - 0s - loss: 0.5150 - accuracy: 0.7459 - val_loss: 0.5014 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 268/500\n",
      "14/14 - 0s - loss: 0.5148 - accuracy: 0.7459 - val_loss: 0.5011 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 269/500\n",
      "14/14 - 0s - loss: 0.5146 - accuracy: 0.7459 - val_loss: 0.5009 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 270/500\n",
      "14/14 - 0s - loss: 0.5144 - accuracy: 0.7482 - val_loss: 0.5006 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 271/500\n",
      "14/14 - 0s - loss: 0.5142 - accuracy: 0.7459 - val_loss: 0.5002 - val_accuracy: 0.7944 - 33ms/epoch - 2ms/step\n",
      "Epoch 272/500\n",
      "14/14 - 0s - loss: 0.5140 - accuracy: 0.7482 - val_loss: 0.5000 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 273/500\n",
      "14/14 - 0s - loss: 0.5137 - accuracy: 0.7459 - val_loss: 0.5000 - val_accuracy: 0.7850 - 36ms/epoch - 3ms/step\n",
      "Epoch 274/500\n",
      "14/14 - 0s - loss: 0.5135 - accuracy: 0.7482 - val_loss: 0.4997 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 275/500\n",
      "14/14 - 0s - loss: 0.5133 - accuracy: 0.7459 - val_loss: 0.4994 - val_accuracy: 0.7944 - 36ms/epoch - 3ms/step\n",
      "Epoch 276/500\n",
      "14/14 - 0s - loss: 0.5131 - accuracy: 0.7435 - val_loss: 0.4994 - val_accuracy: 0.7850 - 35ms/epoch - 2ms/step\n",
      "Epoch 277/500\n",
      "14/14 - 0s - loss: 0.5129 - accuracy: 0.7459 - val_loss: 0.4991 - val_accuracy: 0.7944 - 34ms/epoch - 2ms/step\n",
      "Epoch 278/500\n",
      "14/14 - 0s - loss: 0.5127 - accuracy: 0.7459 - val_loss: 0.4990 - val_accuracy: 0.7944 - 35ms/epoch - 2ms/step\n",
      "Epoch 279/500\n",
      "14/14 - 0s - loss: 0.5126 - accuracy: 0.7459 - val_loss: 0.4994 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 280/500\n",
      "14/14 - 0s - loss: 0.5123 - accuracy: 0.7482 - val_loss: 0.4992 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 281/500\n",
      "14/14 - 0s - loss: 0.5121 - accuracy: 0.7482 - val_loss: 0.4992 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 282/500\n",
      "14/14 - 0s - loss: 0.5119 - accuracy: 0.7482 - val_loss: 0.4991 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 283/500\n",
      "14/14 - 0s - loss: 0.5117 - accuracy: 0.7482 - val_loss: 0.4988 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 284/500\n",
      "14/14 - 0s - loss: 0.5115 - accuracy: 0.7482 - val_loss: 0.4987 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 285/500\n",
      "14/14 - 0s - loss: 0.5113 - accuracy: 0.7482 - val_loss: 0.4986 - val_accuracy: 0.7850 - 35ms/epoch - 2ms/step\n",
      "Epoch 286/500\n",
      "14/14 - 0s - loss: 0.5111 - accuracy: 0.7482 - val_loss: 0.4987 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 287/500\n",
      "14/14 - 0s - loss: 0.5109 - accuracy: 0.7482 - val_loss: 0.4985 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 288/500\n",
      "14/14 - 0s - loss: 0.5108 - accuracy: 0.7482 - val_loss: 0.4983 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 289/500\n",
      "14/14 - 0s - loss: 0.5106 - accuracy: 0.7482 - val_loss: 0.4982 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 290/500\n",
      "14/14 - 0s - loss: 0.5104 - accuracy: 0.7506 - val_loss: 0.4981 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 291/500\n",
      "14/14 - 0s - loss: 0.5102 - accuracy: 0.7529 - val_loss: 0.4981 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 292/500\n",
      "14/14 - 0s - loss: 0.5100 - accuracy: 0.7529 - val_loss: 0.4980 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 293/500\n",
      "14/14 - 0s - loss: 0.5098 - accuracy: 0.7506 - val_loss: 0.4980 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 294/500\n",
      "14/14 - 0s - loss: 0.5097 - accuracy: 0.7506 - val_loss: 0.4980 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 295/500\n",
      "14/14 - 0s - loss: 0.5095 - accuracy: 0.7529 - val_loss: 0.4977 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 296/500\n",
      "14/14 - 0s - loss: 0.5092 - accuracy: 0.7506 - val_loss: 0.4975 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 297/500\n",
      "14/14 - 0s - loss: 0.5091 - accuracy: 0.7529 - val_loss: 0.4974 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 298/500\n",
      "14/14 - 0s - loss: 0.5089 - accuracy: 0.7529 - val_loss: 0.4972 - val_accuracy: 0.7850 - 36ms/epoch - 3ms/step\n",
      "Epoch 299/500\n",
      "14/14 - 0s - loss: 0.5087 - accuracy: 0.7529 - val_loss: 0.4972 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 300/500\n",
      "14/14 - 0s - loss: 0.5086 - accuracy: 0.7529 - val_loss: 0.4970 - val_accuracy: 0.7850 - 35ms/epoch - 2ms/step\n",
      "Epoch 301/500\n",
      "14/14 - 0s - loss: 0.5084 - accuracy: 0.7529 - val_loss: 0.4970 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 302/500\n",
      "14/14 - 0s - loss: 0.5081 - accuracy: 0.7529 - val_loss: 0.4967 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 303/500\n",
      "14/14 - 0s - loss: 0.5080 - accuracy: 0.7529 - val_loss: 0.4967 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 304/500\n",
      "14/14 - 0s - loss: 0.5078 - accuracy: 0.7529 - val_loss: 0.4965 - val_accuracy: 0.7850 - 36ms/epoch - 3ms/step\n",
      "Epoch 305/500\n",
      "14/14 - 0s - loss: 0.5076 - accuracy: 0.7529 - val_loss: 0.4965 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 306/500\n",
      "14/14 - 0s - loss: 0.5075 - accuracy: 0.7529 - val_loss: 0.4963 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 307/500\n",
      "14/14 - 0s - loss: 0.5073 - accuracy: 0.7553 - val_loss: 0.4964 - val_accuracy: 0.7850 - 35ms/epoch - 3ms/step\n",
      "Epoch 308/500\n",
      "14/14 - 0s - loss: 0.5071 - accuracy: 0.7529 - val_loss: 0.4962 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 309/500\n",
      "14/14 - 0s - loss: 0.5069 - accuracy: 0.7529 - val_loss: 0.4962 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 310/500\n",
      "14/14 - 0s - loss: 0.5067 - accuracy: 0.7529 - val_loss: 0.4961 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 311/500\n",
      "14/14 - 0s - loss: 0.5066 - accuracy: 0.7576 - val_loss: 0.4961 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 312/500\n",
      "14/14 - 0s - loss: 0.5064 - accuracy: 0.7576 - val_loss: 0.4958 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 313/500\n",
      "14/14 - 0s - loss: 0.5063 - accuracy: 0.7553 - val_loss: 0.4955 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 314/500\n",
      "14/14 - 0s - loss: 0.5060 - accuracy: 0.7553 - val_loss: 0.4952 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 315/500\n",
      "14/14 - 0s - loss: 0.5059 - accuracy: 0.7576 - val_loss: 0.4951 - val_accuracy: 0.7850 - 35ms/epoch - 2ms/step\n",
      "Epoch 316/500\n",
      "14/14 - 0s - loss: 0.5057 - accuracy: 0.7576 - val_loss: 0.4950 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 317/500\n",
      "14/14 - 0s - loss: 0.5055 - accuracy: 0.7576 - val_loss: 0.4950 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 318/500\n",
      "14/14 - 0s - loss: 0.5054 - accuracy: 0.7576 - val_loss: 0.4948 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 319/500\n",
      "14/14 - 0s - loss: 0.5052 - accuracy: 0.7576 - val_loss: 0.4945 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 320/500\n",
      "14/14 - 0s - loss: 0.5050 - accuracy: 0.7576 - val_loss: 0.4943 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 321/500\n",
      "14/14 - 0s - loss: 0.5049 - accuracy: 0.7576 - val_loss: 0.4942 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 322/500\n",
      "14/14 - 0s - loss: 0.5048 - accuracy: 0.7576 - val_loss: 0.4940 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 323/500\n",
      "14/14 - 0s - loss: 0.5046 - accuracy: 0.7576 - val_loss: 0.4940 - val_accuracy: 0.7850 - 35ms/epoch - 2ms/step\n",
      "Epoch 324/500\n",
      "14/14 - 0s - loss: 0.5044 - accuracy: 0.7576 - val_loss: 0.4938 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 325/500\n",
      "14/14 - 0s - loss: 0.5043 - accuracy: 0.7576 - val_loss: 0.4938 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 326/500\n",
      "14/14 - 0s - loss: 0.5041 - accuracy: 0.7576 - val_loss: 0.4936 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 327/500\n",
      "14/14 - 0s - loss: 0.5039 - accuracy: 0.7576 - val_loss: 0.4935 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 328/500\n",
      "14/14 - 0s - loss: 0.5038 - accuracy: 0.7600 - val_loss: 0.4934 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 329/500\n",
      "14/14 - 0s - loss: 0.5036 - accuracy: 0.7600 - val_loss: 0.4933 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 330/500\n",
      "14/14 - 0s - loss: 0.5035 - accuracy: 0.7600 - val_loss: 0.4932 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 331/500\n",
      "14/14 - 0s - loss: 0.5033 - accuracy: 0.7600 - val_loss: 0.4932 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 332/500\n",
      "14/14 - 0s - loss: 0.5032 - accuracy: 0.7600 - val_loss: 0.4929 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 333/500\n",
      "14/14 - 0s - loss: 0.5030 - accuracy: 0.7600 - val_loss: 0.4929 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 334/500\n",
      "14/14 - 0s - loss: 0.5028 - accuracy: 0.7600 - val_loss: 0.4928 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 335/500\n",
      "14/14 - 0s - loss: 0.5027 - accuracy: 0.7600 - val_loss: 0.4928 - val_accuracy: 0.7850 - 35ms/epoch - 2ms/step\n",
      "Epoch 336/500\n",
      "14/14 - 0s - loss: 0.5025 - accuracy: 0.7600 - val_loss: 0.4927 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 337/500\n",
      "14/14 - 0s - loss: 0.5024 - accuracy: 0.7624 - val_loss: 0.4927 - val_accuracy: 0.7850 - 36ms/epoch - 3ms/step\n",
      "Epoch 338/500\n",
      "14/14 - 0s - loss: 0.5023 - accuracy: 0.7624 - val_loss: 0.4926 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 339/500\n",
      "14/14 - 0s - loss: 0.5021 - accuracy: 0.7624 - val_loss: 0.4926 - val_accuracy: 0.7757 - 31ms/epoch - 2ms/step\n",
      "Epoch 340/500\n",
      "14/14 - 0s - loss: 0.5020 - accuracy: 0.7600 - val_loss: 0.4924 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 341/500\n",
      "14/14 - 0s - loss: 0.5018 - accuracy: 0.7600 - val_loss: 0.4922 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 342/500\n",
      "14/14 - 0s - loss: 0.5016 - accuracy: 0.7624 - val_loss: 0.4921 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 343/500\n",
      "14/14 - 0s - loss: 0.5015 - accuracy: 0.7600 - val_loss: 0.4920 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 344/500\n",
      "14/14 - 0s - loss: 0.5014 - accuracy: 0.7600 - val_loss: 0.4920 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 345/500\n",
      "14/14 - 0s - loss: 0.5012 - accuracy: 0.7600 - val_loss: 0.4918 - val_accuracy: 0.7757 - 62ms/epoch - 4ms/step\n",
      "Epoch 346/500\n",
      "14/14 - 0s - loss: 0.5010 - accuracy: 0.7600 - val_loss: 0.4919 - val_accuracy: 0.7757 - 40ms/epoch - 3ms/step\n",
      "Epoch 347/500\n",
      "14/14 - 0s - loss: 0.5009 - accuracy: 0.7647 - val_loss: 0.4918 - val_accuracy: 0.7757 - 29ms/epoch - 2ms/step\n",
      "Epoch 348/500\n",
      "14/14 - 0s - loss: 0.5008 - accuracy: 0.7600 - val_loss: 0.4917 - val_accuracy: 0.7757 - 30ms/epoch - 2ms/step\n",
      "Epoch 349/500\n",
      "14/14 - 0s - loss: 0.5006 - accuracy: 0.7624 - val_loss: 0.4918 - val_accuracy: 0.7757 - 31ms/epoch - 2ms/step\n",
      "Epoch 350/500\n",
      "14/14 - 0s - loss: 0.5005 - accuracy: 0.7647 - val_loss: 0.4916 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 351/500\n",
      "14/14 - 0s - loss: 0.5004 - accuracy: 0.7647 - val_loss: 0.4914 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 352/500\n",
      "14/14 - 0s - loss: 0.5002 - accuracy: 0.7647 - val_loss: 0.4914 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 353/500\n",
      "14/14 - 0s - loss: 0.5001 - accuracy: 0.7647 - val_loss: 0.4912 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 354/500\n",
      "14/14 - 0s - loss: 0.4999 - accuracy: 0.7647 - val_loss: 0.4911 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 355/500\n",
      "14/14 - 0s - loss: 0.4998 - accuracy: 0.7647 - val_loss: 0.4913 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 356/500\n",
      "14/14 - 0s - loss: 0.4996 - accuracy: 0.7647 - val_loss: 0.4912 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 357/500\n",
      "14/14 - 0s - loss: 0.4995 - accuracy: 0.7647 - val_loss: 0.4911 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 358/500\n",
      "14/14 - 0s - loss: 0.4994 - accuracy: 0.7647 - val_loss: 0.4908 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 359/500\n",
      "14/14 - 0s - loss: 0.4993 - accuracy: 0.7647 - val_loss: 0.4907 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 360/500\n",
      "14/14 - 0s - loss: 0.4991 - accuracy: 0.7647 - val_loss: 0.4907 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 361/500\n",
      "14/14 - 0s - loss: 0.4990 - accuracy: 0.7647 - val_loss: 0.4907 - val_accuracy: 0.7757 - 35ms/epoch - 2ms/step\n",
      "Epoch 362/500\n",
      "14/14 - 0s - loss: 0.4988 - accuracy: 0.7647 - val_loss: 0.4906 - val_accuracy: 0.7757 - 35ms/epoch - 2ms/step\n",
      "Epoch 363/500\n",
      "14/14 - 0s - loss: 0.4987 - accuracy: 0.7647 - val_loss: 0.4905 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 364/500\n",
      "14/14 - 0s - loss: 0.4986 - accuracy: 0.7647 - val_loss: 0.4906 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 365/500\n",
      "14/14 - 0s - loss: 0.4985 - accuracy: 0.7647 - val_loss: 0.4904 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 366/500\n",
      "14/14 - 0s - loss: 0.4983 - accuracy: 0.7624 - val_loss: 0.4903 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 367/500\n",
      "14/14 - 0s - loss: 0.4982 - accuracy: 0.7624 - val_loss: 0.4900 - val_accuracy: 0.7757 - 31ms/epoch - 2ms/step\n",
      "Epoch 368/500\n",
      "14/14 - 0s - loss: 0.4981 - accuracy: 0.7624 - val_loss: 0.4900 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 369/500\n",
      "14/14 - 0s - loss: 0.4979 - accuracy: 0.7624 - val_loss: 0.4900 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 370/500\n",
      "14/14 - 0s - loss: 0.4978 - accuracy: 0.7624 - val_loss: 0.4899 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 371/500\n",
      "14/14 - 0s - loss: 0.4977 - accuracy: 0.7624 - val_loss: 0.4900 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 372/500\n",
      "14/14 - 0s - loss: 0.4975 - accuracy: 0.7624 - val_loss: 0.4898 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 373/500\n",
      "14/14 - 0s - loss: 0.4974 - accuracy: 0.7624 - val_loss: 0.4897 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 374/500\n",
      "14/14 - 0s - loss: 0.4973 - accuracy: 0.7624 - val_loss: 0.4896 - val_accuracy: 0.7757 - 35ms/epoch - 2ms/step\n",
      "Epoch 375/500\n",
      "14/14 - 0s - loss: 0.4971 - accuracy: 0.7624 - val_loss: 0.4893 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 376/500\n",
      "14/14 - 0s - loss: 0.4970 - accuracy: 0.7647 - val_loss: 0.4892 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 377/500\n",
      "14/14 - 0s - loss: 0.4969 - accuracy: 0.7624 - val_loss: 0.4889 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 378/500\n",
      "14/14 - 0s - loss: 0.4967 - accuracy: 0.7647 - val_loss: 0.4888 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 379/500\n",
      "14/14 - 0s - loss: 0.4966 - accuracy: 0.7647 - val_loss: 0.4887 - val_accuracy: 0.7757 - 31ms/epoch - 2ms/step\n",
      "Epoch 380/500\n",
      "14/14 - 0s - loss: 0.4965 - accuracy: 0.7624 - val_loss: 0.4885 - val_accuracy: 0.7757 - 31ms/epoch - 2ms/step\n",
      "Epoch 381/500\n",
      "14/14 - 0s - loss: 0.4964 - accuracy: 0.7647 - val_loss: 0.4885 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 382/500\n",
      "14/14 - 0s - loss: 0.4963 - accuracy: 0.7647 - val_loss: 0.4886 - val_accuracy: 0.7757 - 31ms/epoch - 2ms/step\n",
      "Epoch 383/500\n",
      "14/14 - 0s - loss: 0.4962 - accuracy: 0.7647 - val_loss: 0.4884 - val_accuracy: 0.7757 - 31ms/epoch - 2ms/step\n",
      "Epoch 384/500\n",
      "14/14 - 0s - loss: 0.4960 - accuracy: 0.7647 - val_loss: 0.4883 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 385/500\n",
      "14/14 - 0s - loss: 0.4959 - accuracy: 0.7647 - val_loss: 0.4883 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 386/500\n",
      "14/14 - 0s - loss: 0.4958 - accuracy: 0.7647 - val_loss: 0.4882 - val_accuracy: 0.7757 - 31ms/epoch - 2ms/step\n",
      "Epoch 387/500\n",
      "14/14 - 0s - loss: 0.4957 - accuracy: 0.7647 - val_loss: 0.4881 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 388/500\n",
      "14/14 - 0s - loss: 0.4955 - accuracy: 0.7647 - val_loss: 0.4880 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 389/500\n",
      "14/14 - 0s - loss: 0.4955 - accuracy: 0.7647 - val_loss: 0.4880 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 390/500\n",
      "14/14 - 0s - loss: 0.4953 - accuracy: 0.7647 - val_loss: 0.4879 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 391/500\n",
      "14/14 - 0s - loss: 0.4952 - accuracy: 0.7647 - val_loss: 0.4878 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 392/500\n",
      "14/14 - 0s - loss: 0.4951 - accuracy: 0.7647 - val_loss: 0.4877 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 393/500\n",
      "14/14 - 0s - loss: 0.4950 - accuracy: 0.7647 - val_loss: 0.4875 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 394/500\n",
      "14/14 - 0s - loss: 0.4949 - accuracy: 0.7647 - val_loss: 0.4873 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 395/500\n",
      "14/14 - 0s - loss: 0.4948 - accuracy: 0.7647 - val_loss: 0.4875 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 396/500\n",
      "14/14 - 0s - loss: 0.4946 - accuracy: 0.7647 - val_loss: 0.4876 - val_accuracy: 0.7757 - 35ms/epoch - 2ms/step\n",
      "Epoch 397/500\n",
      "14/14 - 0s - loss: 0.4945 - accuracy: 0.7647 - val_loss: 0.4874 - val_accuracy: 0.7757 - 34ms/epoch - 2ms/step\n",
      "Epoch 398/500\n",
      "14/14 - 0s - loss: 0.4944 - accuracy: 0.7647 - val_loss: 0.4872 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 399/500\n",
      "14/14 - 0s - loss: 0.4943 - accuracy: 0.7647 - val_loss: 0.4872 - val_accuracy: 0.7757 - 32ms/epoch - 2ms/step\n",
      "Epoch 400/500\n",
      "14/14 - 0s - loss: 0.4942 - accuracy: 0.7647 - val_loss: 0.4872 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 401/500\n",
      "14/14 - 0s - loss: 0.4941 - accuracy: 0.7647 - val_loss: 0.4870 - val_accuracy: 0.7757 - 31ms/epoch - 2ms/step\n",
      "Epoch 402/500\n",
      "14/14 - 0s - loss: 0.4940 - accuracy: 0.7647 - val_loss: 0.4869 - val_accuracy: 0.7757 - 33ms/epoch - 2ms/step\n",
      "Epoch 403/500\n",
      "14/14 - 0s - loss: 0.4939 - accuracy: 0.7647 - val_loss: 0.4871 - val_accuracy: 0.7850 - 35ms/epoch - 2ms/step\n",
      "Epoch 404/500\n",
      "14/14 - 0s - loss: 0.4937 - accuracy: 0.7624 - val_loss: 0.4870 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 405/500\n",
      "14/14 - 0s - loss: 0.4937 - accuracy: 0.7624 - val_loss: 0.4871 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 406/500\n",
      "14/14 - 0s - loss: 0.4935 - accuracy: 0.7624 - val_loss: 0.4867 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 407/500\n",
      "14/14 - 0s - loss: 0.4934 - accuracy: 0.7647 - val_loss: 0.4867 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 408/500\n",
      "14/14 - 0s - loss: 0.4933 - accuracy: 0.7624 - val_loss: 0.4867 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 409/500\n",
      "14/14 - 0s - loss: 0.4932 - accuracy: 0.7624 - val_loss: 0.4864 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 410/500\n",
      "14/14 - 0s - loss: 0.4932 - accuracy: 0.7647 - val_loss: 0.4863 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 411/500\n",
      "14/14 - 0s - loss: 0.4930 - accuracy: 0.7647 - val_loss: 0.4862 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 412/500\n",
      "14/14 - 0s - loss: 0.4929 - accuracy: 0.7647 - val_loss: 0.4860 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 413/500\n",
      "14/14 - 0s - loss: 0.4928 - accuracy: 0.7647 - val_loss: 0.4859 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 414/500\n",
      "14/14 - 0s - loss: 0.4927 - accuracy: 0.7647 - val_loss: 0.4860 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 415/500\n",
      "14/14 - 0s - loss: 0.4926 - accuracy: 0.7624 - val_loss: 0.4858 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 416/500\n",
      "14/14 - 0s - loss: 0.4925 - accuracy: 0.7624 - val_loss: 0.4856 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 417/500\n",
      "14/14 - 0s - loss: 0.4923 - accuracy: 0.7624 - val_loss: 0.4855 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 418/500\n",
      "14/14 - 0s - loss: 0.4922 - accuracy: 0.7624 - val_loss: 0.4856 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 419/500\n",
      "14/14 - 0s - loss: 0.4921 - accuracy: 0.7624 - val_loss: 0.4857 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 420/500\n",
      "14/14 - 0s - loss: 0.4920 - accuracy: 0.7624 - val_loss: 0.4855 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 421/500\n",
      "14/14 - 0s - loss: 0.4919 - accuracy: 0.7624 - val_loss: 0.4854 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 422/500\n",
      "14/14 - 0s - loss: 0.4918 - accuracy: 0.7624 - val_loss: 0.4854 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 423/500\n",
      "14/14 - 0s - loss: 0.4917 - accuracy: 0.7624 - val_loss: 0.4854 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 424/500\n",
      "14/14 - 0s - loss: 0.4916 - accuracy: 0.7647 - val_loss: 0.4852 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 425/500\n",
      "14/14 - 0s - loss: 0.4916 - accuracy: 0.7624 - val_loss: 0.4853 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 426/500\n",
      "14/14 - 0s - loss: 0.4914 - accuracy: 0.7647 - val_loss: 0.4851 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 427/500\n",
      "14/14 - 0s - loss: 0.4914 - accuracy: 0.7647 - val_loss: 0.4849 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 428/500\n",
      "14/14 - 0s - loss: 0.4913 - accuracy: 0.7624 - val_loss: 0.4849 - val_accuracy: 0.7850 - 35ms/epoch - 2ms/step\n",
      "Epoch 429/500\n",
      "14/14 - 0s - loss: 0.4911 - accuracy: 0.7624 - val_loss: 0.4847 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 430/500\n",
      "14/14 - 0s - loss: 0.4910 - accuracy: 0.7647 - val_loss: 0.4847 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 431/500\n",
      "14/14 - 0s - loss: 0.4909 - accuracy: 0.7647 - val_loss: 0.4845 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 432/500\n",
      "14/14 - 0s - loss: 0.4908 - accuracy: 0.7647 - val_loss: 0.4845 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 433/500\n",
      "14/14 - 0s - loss: 0.4907 - accuracy: 0.7671 - val_loss: 0.4843 - val_accuracy: 0.7850 - 63ms/epoch - 5ms/step\n",
      "Epoch 434/500\n",
      "14/14 - 0s - loss: 0.4906 - accuracy: 0.7671 - val_loss: 0.4843 - val_accuracy: 0.7850 - 44ms/epoch - 3ms/step\n",
      "Epoch 435/500\n",
      "14/14 - 0s - loss: 0.4905 - accuracy: 0.7671 - val_loss: 0.4845 - val_accuracy: 0.7850 - 30ms/epoch - 2ms/step\n",
      "Epoch 436/500\n",
      "14/14 - 0s - loss: 0.4904 - accuracy: 0.7671 - val_loss: 0.4843 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 437/500\n",
      "14/14 - 0s - loss: 0.4904 - accuracy: 0.7671 - val_loss: 0.4841 - val_accuracy: 0.7850 - 30ms/epoch - 2ms/step\n",
      "Epoch 438/500\n",
      "14/14 - 0s - loss: 0.4903 - accuracy: 0.7694 - val_loss: 0.4839 - val_accuracy: 0.7850 - 30ms/epoch - 2ms/step\n",
      "Epoch 439/500\n",
      "14/14 - 0s - loss: 0.4902 - accuracy: 0.7671 - val_loss: 0.4837 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 440/500\n",
      "14/14 - 0s - loss: 0.4901 - accuracy: 0.7671 - val_loss: 0.4837 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 441/500\n",
      "14/14 - 0s - loss: 0.4900 - accuracy: 0.7671 - val_loss: 0.4836 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 442/500\n",
      "14/14 - 0s - loss: 0.4899 - accuracy: 0.7671 - val_loss: 0.4835 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 443/500\n",
      "14/14 - 0s - loss: 0.4898 - accuracy: 0.7671 - val_loss: 0.4836 - val_accuracy: 0.7850 - 35ms/epoch - 2ms/step\n",
      "Epoch 444/500\n",
      "14/14 - 0s - loss: 0.4897 - accuracy: 0.7694 - val_loss: 0.4835 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 445/500\n",
      "14/14 - 0s - loss: 0.4896 - accuracy: 0.7694 - val_loss: 0.4836 - val_accuracy: 0.7850 - 30ms/epoch - 2ms/step\n",
      "Epoch 446/500\n",
      "14/14 - 0s - loss: 0.4895 - accuracy: 0.7694 - val_loss: 0.4833 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 447/500\n",
      "14/14 - 0s - loss: 0.4894 - accuracy: 0.7694 - val_loss: 0.4834 - val_accuracy: 0.7850 - 30ms/epoch - 2ms/step\n",
      "Epoch 448/500\n",
      "14/14 - 0s - loss: 0.4893 - accuracy: 0.7694 - val_loss: 0.4831 - val_accuracy: 0.7850 - 29ms/epoch - 2ms/step\n",
      "Epoch 449/500\n",
      "14/14 - 0s - loss: 0.4892 - accuracy: 0.7671 - val_loss: 0.4833 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 450/500\n",
      "14/14 - 0s - loss: 0.4891 - accuracy: 0.7694 - val_loss: 0.4830 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 451/500\n",
      "14/14 - 0s - loss: 0.4891 - accuracy: 0.7694 - val_loss: 0.4832 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 452/500\n",
      "14/14 - 0s - loss: 0.4889 - accuracy: 0.7694 - val_loss: 0.4830 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 453/500\n",
      "14/14 - 0s - loss: 0.4888 - accuracy: 0.7694 - val_loss: 0.4829 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 454/500\n",
      "14/14 - 0s - loss: 0.4888 - accuracy: 0.7694 - val_loss: 0.4829 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 455/500\n",
      "14/14 - 0s - loss: 0.4887 - accuracy: 0.7718 - val_loss: 0.4829 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 456/500\n",
      "14/14 - 0s - loss: 0.4885 - accuracy: 0.7718 - val_loss: 0.4829 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 457/500\n",
      "14/14 - 0s - loss: 0.4884 - accuracy: 0.7718 - val_loss: 0.4829 - val_accuracy: 0.7850 - 35ms/epoch - 2ms/step\n",
      "Epoch 458/500\n",
      "14/14 - 0s - loss: 0.4884 - accuracy: 0.7694 - val_loss: 0.4828 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 459/500\n",
      "14/14 - 0s - loss: 0.4883 - accuracy: 0.7718 - val_loss: 0.4829 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 460/500\n",
      "14/14 - 0s - loss: 0.4882 - accuracy: 0.7694 - val_loss: 0.4828 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 461/500\n",
      "14/14 - 0s - loss: 0.4881 - accuracy: 0.7694 - val_loss: 0.4827 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 462/500\n",
      "14/14 - 0s - loss: 0.4880 - accuracy: 0.7694 - val_loss: 0.4827 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 463/500\n",
      "14/14 - 0s - loss: 0.4879 - accuracy: 0.7671 - val_loss: 0.4828 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 464/500\n",
      "14/14 - 0s - loss: 0.4878 - accuracy: 0.7671 - val_loss: 0.4826 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 465/500\n",
      "14/14 - 0s - loss: 0.4878 - accuracy: 0.7671 - val_loss: 0.4827 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 466/500\n",
      "14/14 - 0s - loss: 0.4877 - accuracy: 0.7671 - val_loss: 0.4827 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 467/500\n",
      "14/14 - 0s - loss: 0.4876 - accuracy: 0.7671 - val_loss: 0.4827 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 468/500\n",
      "14/14 - 0s - loss: 0.4875 - accuracy: 0.7671 - val_loss: 0.4827 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 469/500\n",
      "14/14 - 0s - loss: 0.4874 - accuracy: 0.7671 - val_loss: 0.4827 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 470/500\n",
      "14/14 - 0s - loss: 0.4873 - accuracy: 0.7671 - val_loss: 0.4825 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 471/500\n",
      "14/14 - 0s - loss: 0.4873 - accuracy: 0.7671 - val_loss: 0.4825 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 472/500\n",
      "14/14 - 0s - loss: 0.4872 - accuracy: 0.7671 - val_loss: 0.4827 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 473/500\n",
      "14/14 - 0s - loss: 0.4871 - accuracy: 0.7671 - val_loss: 0.4827 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 474/500\n",
      "14/14 - 0s - loss: 0.4870 - accuracy: 0.7671 - val_loss: 0.4826 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 475/500\n",
      "14/14 - 0s - loss: 0.4870 - accuracy: 0.7671 - val_loss: 0.4827 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 476/500\n",
      "14/14 - 0s - loss: 0.4869 - accuracy: 0.7671 - val_loss: 0.4826 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 477/500\n",
      "14/14 - 0s - loss: 0.4868 - accuracy: 0.7694 - val_loss: 0.4824 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 478/500\n",
      "14/14 - 0s - loss: 0.4867 - accuracy: 0.7671 - val_loss: 0.4824 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 479/500\n",
      "14/14 - 0s - loss: 0.4866 - accuracy: 0.7694 - val_loss: 0.4823 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 480/500\n",
      "14/14 - 0s - loss: 0.4865 - accuracy: 0.7694 - val_loss: 0.4821 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 481/500\n",
      "14/14 - 0s - loss: 0.4864 - accuracy: 0.7671 - val_loss: 0.4821 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 482/500\n",
      "14/14 - 0s - loss: 0.4864 - accuracy: 0.7694 - val_loss: 0.4822 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 483/500\n",
      "14/14 - 0s - loss: 0.4863 - accuracy: 0.7694 - val_loss: 0.4820 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 484/500\n",
      "14/14 - 0s - loss: 0.4862 - accuracy: 0.7671 - val_loss: 0.4818 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 485/500\n",
      "14/14 - 0s - loss: 0.4861 - accuracy: 0.7694 - val_loss: 0.4818 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 486/500\n",
      "14/14 - 0s - loss: 0.4861 - accuracy: 0.7718 - val_loss: 0.4816 - val_accuracy: 0.7850 - 36ms/epoch - 3ms/step\n",
      "Epoch 487/500\n",
      "14/14 - 0s - loss: 0.4859 - accuracy: 0.7694 - val_loss: 0.4816 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 488/500\n",
      "14/14 - 0s - loss: 0.4859 - accuracy: 0.7694 - val_loss: 0.4815 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 489/500\n",
      "14/14 - 0s - loss: 0.4858 - accuracy: 0.7718 - val_loss: 0.4814 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 490/500\n",
      "14/14 - 0s - loss: 0.4857 - accuracy: 0.7694 - val_loss: 0.4816 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 491/500\n",
      "14/14 - 0s - loss: 0.4857 - accuracy: 0.7694 - val_loss: 0.4816 - val_accuracy: 0.7850 - 35ms/epoch - 2ms/step\n",
      "Epoch 492/500\n",
      "14/14 - 0s - loss: 0.4856 - accuracy: 0.7694 - val_loss: 0.4814 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 493/500\n",
      "14/14 - 0s - loss: 0.4856 - accuracy: 0.7718 - val_loss: 0.4814 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 494/500\n",
      "14/14 - 0s - loss: 0.4855 - accuracy: 0.7718 - val_loss: 0.4813 - val_accuracy: 0.7850 - 31ms/epoch - 2ms/step\n",
      "Epoch 495/500\n",
      "14/14 - 0s - loss: 0.4854 - accuracy: 0.7718 - val_loss: 0.4814 - val_accuracy: 0.7850 - 32ms/epoch - 2ms/step\n",
      "Epoch 496/500\n",
      "14/14 - 0s - loss: 0.4853 - accuracy: 0.7718 - val_loss: 0.4813 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 497/500\n",
      "14/14 - 0s - loss: 0.4852 - accuracy: 0.7718 - val_loss: 0.4813 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 498/500\n",
      "14/14 - 0s - loss: 0.4852 - accuracy: 0.7718 - val_loss: 0.4810 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "Epoch 499/500\n",
      "14/14 - 0s - loss: 0.4851 - accuracy: 0.7718 - val_loss: 0.4809 - val_accuracy: 0.7850 - 33ms/epoch - 2ms/step\n",
      "Epoch 500/500\n",
      "14/14 - 0s - loss: 0.4850 - accuracy: 0.7718 - val_loss: 0.4809 - val_accuracy: 0.7850 - 34ms/epoch - 2ms/step\n",
      "\n",
      "Elapsed Time =>  0:00:18.410961\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start_time=datetime.now()\n",
    "\n",
    "hist=model.fit(training_x_data, training_t_data, epochs=500, validation_split=0.2, verbose=2)\n",
    "# verbose mode\n",
    "# 0=silent\n",
    "# 1=progress bar,\n",
    "# 2=one line per epoch\n",
    "\n",
    "end_time=datetime.now()\n",
    "\n",
    "print('\\nElapsed Time => ', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad6eccb5-ca50-4108-a6b3-1e30671c4659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5197 - accuracy: 0.7357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5197427272796631, 0.7356828451156616]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x_data, test_t_data) # 오류, 정확도 \n",
    "# -> 훈련데이터에 비해 정확도 떨어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e84b70-5c37-4f1d-bcc6-7240f9d743d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(hist.history['loss'], label='train loss')\n",
    "plt.plot(hist.history['val_loss'], label='validation loss')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4323d47b-9969-42af-9aed-3bb15ea333c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA5MUlEQVR4nO3deXhV1bn48e+beSQzAcKQMMkk86AFNKg4z4JQtThUsVVRW3uvtvVWetXftQ6tWhWlFocWRatF0eJQlYBWmWUeZIaQMCWEzDnT+v2xT0ISTkIIOTnJ2e/nec5zzt577b3XCuS8WWvttZYYY1BKKWVfIYHOgFJKqcDSQKCUUjangUAppWxOA4FSStmcBgKllLI5DQRKKWVzGgiUUsrmNBAo2xCRHBE5KiKRgc6LUm2JBgJlCyKSCYwHDHBlK943rLXupVRzaSBQdjENWAq8DtxcvVNEuonIP0XksIgUiMgLtY7dISKbRaRERDaJyHDvfiMivWule11EHvN+zhaRXBF5UEQOAK+JSJKIfOy9x1Hv5661zk8WkddEJM97/APv/g0ickWtdOEickREhvrpZ6RsSgOBsotpwFzv6yIRSReRUOBjYA+QCWQA8wBEZDIw03teB6xaREET79UJSAZ6ANOxfs9e8253ByqAF2ql/xsQAwwEOgJ/8u5/E7ipVrpLgXxjzJom5kOpJhGda0gFOxEZBywCOhtjjojIFuAVrBrCAu9+V71zPgMWGmOe83E9A/Qxxmz3br8O5BpjHhaRbOBzoIMxprKB/AwFFhljkkSkM7AfSDHGHK2XrguwFcgwxhSLyHvAcmPMk838USjlk9YIlB3cDHxujDni3X7Lu68bsKd+EPDqBuxo5v0O1w4CIhIjIq+IyB4RKQaWAIneGkk3oLB+EAAwxuQB/wGuE5FE4BKsGo1SLUo7slRQE5Fo4Hog1NtmDxAJJAIHge4iEuYjGOwDejVw2XKsppxqnYDcWtv1q9kPAGcAY4wxB7w1gu8B8d4nWUQSjTFFPu71BnA71u/qd8aY/Q3kSalm0xqBCnZXA25gADDU++oPfO09lg88ISKxIhIlImO9570K/EpERoilt4j08B5bA9wgIqEicjFw7knyEI/VL1AkIsnAI9UHjDH5wCfAS95O5XAROafWuR8Aw4H7sPoMlGpxGghUsLsZeM0Ys9cYc6D6hdVZ+2PgCqA3sBfrr/opAMaYfwCPYzUjlWB9ISd7r3mf97wi4EbvscY8C0QDR7D6JT6td/wngBPYAhwC7q8+YIypAN4HsoB/Nr3YSjWddhYr1caJyO+AvsaYm06aWKlm0D4Cpdowb1PST7FqDUr5hTYNKdVGicgdWJ3JnxhjlgQ6Pyp4adOQUkrZnNYIlFLK5tpdH0FqaqrJzMxs1rllZWXExsa2bIbaOC2zPWiZ7eF0yrxq1aojxpg0X8faXSDIzMxk5cqVzTo3JyeH7Ozsls1QG6dltgctsz2cTplFZE9Dx7RpSCmlbE4DgVJK2ZwGAqWUsjm/BgIRuVhEtorIdhF5yMfxBBH5SETWishGEbnVn/lRSil1Ir8FAu8Uuy9iTZ07APixiAyol+xuYJMxZgiQDTwjIhH+ypNSSqkT+bNGMBrYbozZaYxxYK38dFW9NAaIFxEB4oBCwNfc8EoppfzEbyOLRWQScLEx5nbv9k+w5mO/p1aaeKwVovphTdU7xRjzLx/Xmo615B/p6ekj5s2b16w8lZaWEhcX16xz2ystsz1ome3hdMo8YcKEVcaYkb6O+XMcgfjYVz/qXIQ1t/t5WIuA/FtEvjbGFNc5yZjZwGyAkSNHmuY+R6vPHdtDuynzoc1Qdhh6jIU1c0FCwFEOo+8A8fXr07B2U+YWpGVuOf4MBLlYy/BV6wrk1UtzK/CEsaol20VkF1btYLkf86VU2/DSWdb7dX+FBTOO788aDx37ByZPypb82UewAugjIlneDuCpWM1Ate0FzgcQkXSs5fx2+jFPSrU9R3fV3S7UXwHVuvxWIzDGuETkHuAzIBSYY4zZKCI/8x5/GXgUeF1E1mM1JT1Ya4FxpYKXx3P8857v6h7TQKBamV/nGjLGLAQW1tv3cq3PecCF/syDUgFVnAfF+dZnARIz4ehuKK/1986uxcc/h4RB7grIXXVKt4kv/gFy4083t76l9gGPCyLiIKyNPd1dsAMqigKdi1YTWXnYL9dtd5POKdVueDww60dQcfQk6Wo9Md11NGz60HqdghEAq085h01zxmWw9V/Q73KYOtdPNzl1kZVH4M9Xc+IzKMEro9u1wOQWv64GAqX8pSTfCgJjfga9zofPH4YjW6H7j2DcLyAiFiLjoeQApPa2agMhYXBgwynfat36dQw+c3DLl+G7P8PORdbnLR+3/PVPQ0z5XsDAhY9Dat9AZ6dVHNh2iO5+uK4GAqX8pXCH9d73Yug1Ada+ZQWCbqOgb60W0c71vsA7dDn1W+VFQN/s5ue1IbnLYVfbXCUzptzb5HbmJIjvFNjMtJLyvBy/XFcnnVPKX6o7fVN6We9hUdZ7Qjff6dui5J51tz3uwOTDh+iKfAiPhbj0QGel3dMagQpurir460Sr0/ZkohLg1k8hrtYiTsV58MYV8ON3rOabalWl8NcLoexQw9dzlENoBHTIsLZjU733STzlYgRM/UDwdB9r4Fsb0KW8CDr2O+XBd+pEGghUcCvYDvlrofcFkNhI62p5gdVBm/d93WabDe9b11g2Cy575vj+Q5vh0Eboewl06NzwdTsPgZBQ6/O5D0J0Mgy85vTK1JoyRsC4X0J4DFQUgqsy0DmqkZ+XR8a5gZ2wuLDMQagIMZGhhIeeXoB0uj2EhQhujyEsNITiSicVjro1sAqXfzrGNRCo4FbdPHPew9BlWMPpSg9ZgaD+M/yuKuu9/pxc1ekm/i+kNbGjMjIexv+yaWnbitBwuOCRQOfCp205OWSckd3i1zXGILVqGR6Pwe3996/+ojbAvOV7+Z8PN9akm3Feb+47vw9hJwkI1fO7eYz1RLEBXvhqO28t30NmSiwVTjcPXHgGd7y5EofLU+fcS7PCueSCFilmHRoIVHAr8HbY1m/iqC82DSLiTwwERXut9/J64xwLd1pNJEk9Wiafqk0wxnDHm6tIjAnn6clD2FtQznUvf8vhkqqaNCFifYkDjMpMYu2+YzjcHv781XZmL9nJizcM54IBvvstjlU4mTTrW7YdKvV5/GCxdZ+b5yyna1I0d2X3rnO8In9bC5TyRBoI7OzrZ6wvtLAomPBbiElu/Twc2Qbf/hlMy3VCnpF/AI79w9rIXQkxqVb7f2NEIDkLtn4CzrLj+3d4H53c8y18ePfx/XuXQkJXCItssXyrwDDGMOPt71n8w2GMgdIqa1zHZxsO4HB7CA0RHpjYl7xjFby9fB9dEqOZOqobYaEhTB7RlQqnm415xeQereCtZXv42d9XER0R6vNeLrehwun7//ozk4dQ5fIQFR5CXlEFl57ZmZ5pdWcazcnxz6hzDQR2VXYEvvxfiOwAVcVWW/DQG1o/H9//DVa/2axHJhuSVFUJ5VHHdwy6tmknDrwaVvz1+Jd/tdS+Vsdv/f2DrjutfLZXewrK+O/31nH9yG68vHgHTren0fSpcZE8/+NhdEmMrrP/3ZX7eGXxDsJCrKYUp9uD5xSmxa+oqCBpzRL+79ozWbG7kEPFVZQ53Hy3w6q9dUuO4cUbh9MhKrzmnI15x3j4gw0UVzjJSo1jVGYSf1+2h32FFVx2Zmc6dogkLT4SY+BIqfXX+QX90xnbOxWPx3BmRiLn9+9IeoeoOnnpmhQDwHn9OjJ36Z6apiRfslJjCRFhdFYym/OL6ZwQjcPlYVyf1CaXvaVpILCr6iaQq1+Cd28O3Pw2hTutKQzuWdFil1za3Kl6xz9gvRRgfWk+/q/NhIYIN53Vg882HuB3lw/gha+2s2xXIct2FdItOZrh3ZMavc6nGw5w/Svf0TG+bu1p9d6iOtudOkQxpmfTa6UHD1axo9TBHW+u5Eipo2b/+D6pJESH8/G6fK5+4T8kxhwPBFsPlFDm7YDdcbiMLzYf5MyMBK45vyv3nd+H0JCGn0AKCRFuGNP4cK6s1Fgevrz+QowN65vup2lBTpEGAruq/uJP6weJ3QIYCHadvP1e+d2rX+9k6c7COvs25xdT5nDhdhu+3nbEu6+k5i9lgKcmDeGsnimNXvvDNft5b1XuCfsvGdSJiLAQrhmWwVdbDvHTcVn0SIltcp5zcnKIzRzMC19tJzbSaorpGB/Fw5f1Jyw0hOHdd7Foa93He0dkJpMeH8ngbokUlTnYdqiUR64YQEqcvZv4NBDYQVWJNTrU1KrCb/u31dmZ2MP6It7wT7j8TydvS2+uXUug8tiJ+wt3QtY5/rmnzZVWuXj6s60M6ZZAZFgoi7f6nrCsyuXmgzV59EiJITbi+FdCSlwET1x0JuUONy8t2k50RCjFFS7S4iLplRbL0G5JJw0CAFcNzeCqoRmNpsk+o+OpFc5rVGYyb9w22uex28Zlcdu4rGZd1240ENjBf56HJU+euD+tvzWbZKczYcdX8M2f4IKZLX//AxusQVkNSR/U8ve0sQ37j3Gswsn+ogpe/3Z3zf6E6HCiw313Yo7vk8pfpo0kqoHjFw20xxQOdqWBwA6ObLX+8q8/c2T1iNfz/gdWzIHDP/jv/gDX/816Mqe2kHDbTBh2uo5VOPlwzX6y+3Zkb2E5IlbzDcD2XU62h1rNe4/9azMAQ7omEB0eSo+UGBxuD+9MP5u0eHs3gSjfNBDYQcFOSDvD+svfl9Bw6HmuNYLWX/cHa3RvRIx/7mEDD72/jk82HAA2+k6wdXOdzbW5x7htbBa/u6LpnZfKnjQQBDtjoGAbZI5rPF1yltVv4Kpq+blkCrZDfBcNAvXsKSgjRITIsBCiI0KJ9z7muCmvmEMldadyyD9WyScbDjC0WyIb847hdFuPJ350zzgyU2P45ptvGDfO+jeOCAtBEKpcbuIi9VdcnZz+Lwly/bY8a80PUz0DZkOSe4G7Ch5rXqfdSWWO9891W5nHYyipdHG4tIouiVHERISx60gZJZVOOnWIIjk2gs35JaTFW8+jl1a5OFxSSbn3kcWkmAjCQ0N4b9U+nv78eFNcpw5RLJgxlkVbDvHg++t93rtrUjRv33EW5Q4Xsd4v+Oo2/egwqQkk1SLC2sbkcKrt00AQ5DoU/wASas3Z3phB11pPF7mrGk/XXL39MEFKKyp3uCgsc/D4vzZ7m2egb3oc087O5OEPrIVkIsNCSI6NIP9YJeGhQlxkGEfLnU26/oHiSkY//iUAPVNjeWrykBMm1ezdMY7oiNAGR60q1VwaCIKZ20VU5SEYey9ENz7oh6gEK506Qf6xCi5//hsKyhx19v9wsJSHP9jA4K4J3JXdm1+8s4b8Y5XcOjaT+d/vrwkCvdJi+c2l/QH46RsrAfjLtJEM6ZaAy22ocnmYOvs7DhZX8cINwxjXO5XEmDa2NrAKahoIgllxLiHGpQO2TsO63CKufOE/hIUIj141kOTYSLolR9MjJZYN+4+RV1TB+f3TSY6NoHfHsew4XMYF/dOZdnYmW/KL6ZIYTVZabM00B0t/fT4VTjdZqXUHTn163zkcLKmkX6cOgSimsjkNBMHkh8+tefOrVQ/g0kBwyiqdbi57/mv2FJQTERbCX28eyfg+aXXSjO1dd26Y3h3j6d3RmjIgKzX2hC97gE4JUSfsA0iKjSApVmsBKjA0EAST1W/A3mWQ7n1cUEIoSB5BSuehAc1WIBVXOrnzzVW4jWHu7WOavHjIsl2F7DhcxjXDMrhtbBZndvXTiGul2gANBMGkcCf0zIYfv1Wza31ODtmRcQ2fE4TKnIb7533PweIqcovK2VdYAcAjCzay+0gZTreHKu+CH9VP3VR6pwa+cEA6xZUuPlqbR2RYCP937ZkNjrZVKlhoIAgWHo8VCHqdF+icBMx3OwpYvquQzTscfLr7+BrFEwek8+9NB3lr2d6afRPOSMMAOd75d7LPSONIaVXNI53Duidy69hMDQLKFjQQtHVF+6zFUjjJPO2OUmu8gE37A8odLn78l6U1250ToshKjSU5NoLfXT6AXmlxrN1XREgIzDivT81kaa9+vRMR4afjsiircvHf760jMjyEP1w3+LTXoFWqvdBA0NYtfgK+/3vT0kpI4+vytmNuj+EfK/fhMXC03EFkWAgHi4+Pvt1x2FpVLC0+koRQJ3PvGltn8ZCHLunn87q3jz8eOGMjw3jxxuF+KoFSbZcGgrauYAd0GwM/nnfytKHh1gLpQej9Vbk89M+6I26jwkMIqTXqqnpenZycnBNWkFJKNUwDQVtXsAP6XhiY9YQDzBjD4h8Oc7C4kme8bfdZqbEMykjgUHElb91xVqMrSimlmkYDQVtWVQJlh6x5gGxm5+FSFq7Pr+m8jQoPYf5dP2Jot0REBGMMUn8OBqVUs2ggaMsKd1nvQd4BfKikktyjFfRKjSO3qJxDxVXc+bdVONweBndN4KUbhxMfFU5C9PFJ1TQIKNVyNBC0ZYU7rPcgDgSVTjeTX/6OPQXldfbHR4bx99vHMLhrgj7CqZSfaSBoy6oXlA+iQGCMobTKBcC+wgouff5rAK4dlsE/v9/PqMwk7p7Qm77p8XRJjA5kVpWyDQ0EbVnhTohLhyAZGWyM4c6/reLzTQdPOPbARWdw27gseneM0xqAUq1MA0FbsncpLHsZrn0V3rkJdn/d8PKS7cyrX+/kqc+2UuXyMGVkN/qkW8Gtb3o8TreHjMRoMrQGoFRAaCBoS969GUoPwKjb4YdPIGMEnH1PoHN1Wr7dfoTfzF/PvqMVjOiexGWDO3PjmO6E6ahdpdoMvwYCEbkYeA4IBV41xjxR7/h/ATfWykt/IM0YU+jPfLVZ0YlWINj+hbV9wUzIOieQOTotDpeH38xfj9NtuPnsTO6e0IuUuMhAZ0spVY/fAoGIhAIvAhOBXGCFiCwwxmyqTmOMeQp4ypv+CuAXtg0CcHwVsW3eQNCOO4lfytlOzpbD7C4o57VbRzHhDD+thayUOm3+rBGMBrYbY3YCiMg84CpgUwPpfwy87cf8BMbyv8CxfSdJJDD4egjz/rV8cD2ERUF8F79nr6W5PYaZCzbyt6V7ABjSNYHsvmknOUspFUj+DAQZQO1vwFxgjK+EIhIDXAy07wbx+orzYeGvICTMejXEVQnFecdXFAuLshZ7D2lf7egfr8tj4fp8Fq4/ULNvQJcEHfylVBsnxpxkeuPmXlhkMnCRMeZ27/ZPgNHGmBk+0k4BbjLGXNHAtaYD0wHS09NHzJvXhAnYfCgtLSUurvUexUwo2sCwNb9l7eDfczR5aIPpBq/9HWGuCsKdxziW0J8t/X/RYnnwd5nXH3bhNrCjyMPHO52Eh8LwjqFc1yeCZ1dXcs/QKDrHtW5Aa+1/57ZAy2wPp1PmCRMmrDLGjPR1zJ81glygW63trkBeA2mn0kizkDFmNjAbYOTIkSY7O7tZGcrJyaG55zbLqj2wBoZMuBqSMhtOVzoCNs4H4yE6sx+dWjCP/iqzMYY3v9vDM6s21uzrmRbL/LvG1kwFMfnSFr9tk7T6v3MboGW2B3+V2Z+BYAXQR0SygP1YX/Y31E8kIgnAucBNfsyL/x3cBJVFdfft+RZCwiGhm89TaiT3hIqj1ufqDuM27sM1eTyy4HgQmH/XjxjSNZEQnQ1UqXbHb4HAGOMSkXuAz7AeH51jjNkoIj/zHn/Zm/Qa4HNjTJm/8uJ3hbtg1tm+j6UPgpCTjJTt2P/45/jOLZcvP9h+qJSC0ir+8OkW+nSMY8E94wgLFV3NS6l2zK/jCIwxC4GF9fa9XG/7deB1f+bD7w5ttt4v+yOk1JsyOqXPyc/veR7c9jl4XNBtdMvnr4V8vvEA0/+2CgARmHfHWURH6HQQSrV3OrK4JVRPDjfwmuYtIBMSAt19PlAVcE63h7IqFy6P4f8t3EzvjnHMvGIgnRIi6d0xOFdDU8puNBC0hMKdEJUYlKuIXfvSt6zff6xm+/VbRzGuT2oAc6SUamkaCJpjx1fw3k+tphwAZzl0GhzYPLWwg8WVnPPkIqpcHq4e2oUh3RLpnhxDto4QViroaCBojp2LrWUkR99xfF/fiwKXn9M0b/lePt90kEevHsT0N1dysLiSSqeHKpcHgPsu6EtWamyAc6mU8hcNBM1RuBOSs+Di/wt0Tk7bM59v5c9fbQdg9fNfU17lZtLIrgjQPTmGsNAQMlNiAptJpZRfaSBojsJd7XpCOICF6/N5/dvdLN91fI6/s3umcMmZnblySPub40gp1XwaCE6mrAC+nAk9xsKQqWCMVSPIGh/onJ2WWTk72F9UwYQz0ggLDeGRKwbQNUn/8lfKjjQQnMy2z2H1m7DuXSsQlB4EZ1m7qRFsLnCz+z+72FNYTs/UWFwew97CctbvP8avLuzLPec1YZyDUiqoaSA4mcId1rurElxV7WJBeZfbwzsr9xEWIvxhRSW+Zv7OSIzm0jPb9ihmpVTr0EBwMtVf/ABFe6HAGxjaaCCocLj59T/X8cGauvP73T4ui7eX76XM4ea5qUO5amhGgHKolGprNBD4sn+V1SEMkL8WIjtAVTGseQsKtjdtIrkAKK50Mv3NlSzdWchZPZPJK6pkcKKTF6ZfCMDDlw8IcA6VUm2RBoL63C54/XJrkFi1EbdaQeCbP1rbnYdAaNv50W0/VMKhkiqe/mwrq/cWcdNZ3Xn0qkF4DHy9ZHGgs6eUauPazrdZW1GcawWB8/4H+l9pza6WlAUTfnt8qugObadt/UhpFZc89zVOt7XA0AMT+3LPeb0REUJ1RmilVBNoIKivuk+g+1mQ1vf4/rg069WGON0ePt1wAKfb8PTkIfTvHM/ALgmBzpZSqp3RQFDN7QJMrc7gXo0mbw0ej/G50IvHY6w5gF78D1sPlpAaF8G1wzJ0URilVLNoIAArCPyxv9UkNPBqCIuG+E4BzdKCtXn86t21TB7ZlcevObNm/6o9hfx49jIcbmseoPsv6MO5fdM0CCilmk0DAcCxfVB2yPr8w2fWo6ESuC/WWTk7+MOnWwCYu2wvn2w4wP9c3p9rhnXl43X5iMAvLuhL745xXDa47fRXKKXaJw0EcHzQGEDZYat/oJWVVDqZ8fb37C0sZ09BOf06xfPCDcP5ZH0+c5ftZVbODv6xMpfv9xZxVs8U7rtARwQrpVqGBgI4PmagWgAGi83K2UHO1sNcemYnftQrhQcmnkFSbAQzzu9DqcPFK4t38sPBUi4f3JlpZ2e2ev6UUsFLAwFYTwqFx4JxW1NJtHIgyD1azqvf7OKaYRn8acrQE45fP7IbW/JLuGRQJ6aO7t6qeVNKBT8NBAAl+dbYgJG3WauP9Tqv1W69/VApP31jBSEC/3XRGT7T9EqL443b2u6i9kqp9k0DAVgDxaKT4ey7rZefbT9Uyodr9mOMtS7AnoJyHr16EF0So/1+b6WUqk8DAUB5IXRo2cVYnG4P32w7wvg+qYSFhli3cbhYsfso85ZbTwKFhgjR4aHMuWUk5/VLb9H7K6VUU2kgAKgogvSBJ0225UAxWw+UEBYSwvn9OxIVHtpg2me/+IEXF+3gxjHdGZ2VDMBby/ayzLsi2NRR3XjiuuBa8F4p1T5pIABv01DSSZPd/sZKco9WANTp2D1SWoUAKXGRAHy3o4DZS6ypKuYu28vcZXtPuNbEAVoDUEq1DRoI3E5wlJw0EOQeLSf3aAW3jc3CYHjtP7u5eFAnMlNiuejZJUSHh7L01+fzzfYj3P3WaiJCQ1j0q3MxxtRcIywkhPSESA6XVJGh/QFKqTZCA0FFkfXeSCA4WFzJuD8sAmBItwTO69eRj9bmceffVh2/jNPN0Ec/xxhIjAln/l1jyUqN9Xk9XRtYKdWWaCConlq6kUCw+IfDNZ8zU2KJjwrn4xnjWZtbxKtf72TFbusaxsDTk4dwTp9UOnaI8mu2lVKqpWggqAkEiQ0mqR8IADolRNEpoRPn9k1j2a5CyqtcdEqIYlj3k/c1KKVUW6KBwFFivUd28HnY5X0M9Px+HZk6ujsJMeF1jkeFh3Ju37a1ToFSSp2KkEBnIOAcZdZ7hO/2/FtfX8GxCifXDM/QJ32UUkFJA0EjgcAYw3c7CgA4Xwd8KaWClAaCmkAQd8Khcocbl8fw0CX9iI5oePCYUkq1ZxoIqrx9BD5qBIVlDgCSYyNaM0dKKdWqNBA4ykBCIOzExz0LqgNBjAYCpVTw0kDgKIOIeJ9LUx6tDgRxGgiUUsFLA4GjtMEnhqprBCnaNKSUCmJ+DQQicrGIbBWR7SLyUANpskVkjYhsFJHF/syPT46yBgNBYVkVAEkaCJRSQcxvA8pEJBR4EZgI5AIrRGSBMWZTrTSJwEvAxcaYvSLS0V/5aVAjgeD7vUXERoQSH6nj7pRSwcufNYLRwHZjzE5jjAOYB1xVL80NwD+NMXsBjDGH/Jgf3xxlPh8d/XzjAT7ZcIDbx/dEfPQfKKVUsPBnIMgA9tXazvXuq60vkCQiOSKySkSm+TE/JyrcBZVFJ9QIvtpykOl/W0VKbAR3ntu6C9krpVRr82ebh68/o0297TBgBHA+EA18JyJLjTE/1LmQyHRgOkB6ejo5OTnNylBpaWnNuWHOUsb950YADppkNte65ntbrU7i/xoeyvJvv2nWvdqK2mW2Cy2zPWiZW44/A0Eu0K3Wdlcgz0eaI8aYMqBMRJYAQ4A6gcAYMxuYDTBy5EiTnZ3drAzl5ORQc27hTviP9TH9jFGk17rmnJ3LGdiliqmXjW/WfdqSOmW2CS2zPWiZW44/m4ZWAH1EJEtEIoCpwIJ6aT4ExotImIjEAGOAzX7M03HV008DJPdi9d6j3PCXpVQ63WzKK6Z/Z9+zkSqlVLBpUiAQkftEpINY/ioiq0XkwsbOMca4gHuAz7C+3N81xmwUkZ+JyM+8aTYDnwLrgOXAq8aYDadToCarEwh68ot31vDtjgLW7iviSGkVmSm6iphSyh6a2jR0mzHmORG5CEgDbgVeAz5v7CRjzEJgYb19L9fbfgp4qsk5binltQNBFmVVVvxZm1sEQFp8ZKtnSSmlAqGpTUPVHb+XAq8ZY9biuzO4/aiuEVz0/zBx6ZRWOQFqlp3UQKCUsoumBoJVIvI5ViD4TETiAY//stUKqgPB6Ds5VFJFpdMqzr83HQQgLU7XHFZK2UNTm4Z+CgwFdhpjykUkGat5qP2qOAqRHXBLKI/9az0AGYnR7C+qALRGoJSyj6bWCM4GthpjikTkJuBh4Jj/stUKKo5CdCJLth3mo7XWU62f3H/8cdEUnXFUKWUTTQ0Es4ByERkC/DewB3jTb7lqDcX7IS6d3MJyAGIiQukQFc4VQ7oAEB6qE7Mqpeyhqd92LmOMwZor6DljzHNAvP+y1QoKd0FyL3YXlBMRFsKa31lPwz43ZSjbH78kwJlTSqnW09Q+ghIR+TXwE6wBYKFAuP+y5WfOCijOheSe7NlTRs/UWCLCrJgYEiKEtPMHopRS6lQ0tUYwBajCGk9wAGvyuNZ/9r+lHN1tvaf0Ym9hOd2TdfCYUsq+mhQIvF/+c4EEEbkcqDTGtN8+gmO51ntidw6XVJHeQR8VVUrZV1OnmLgeawqIycD1wDIRmeTPjPlVVTEAzvA4jpY79QkhpZStNbWP4LfAqOqFY0QkDfgCeM9fGfMrRxkAx1xWAEiN0zEDSin7amofQUi91cMKTuHctscbCI44rP7uVK0RKKVsrKk1gk9F5DPgbe/2FOpNJteuOEoBOOKwip+iNQKllI01KRAYY/5LRK4DxmJNNjfbGDPfrznzJ0cZhEZwxJpNgpRYrREopeyrySuUGWPeB973Y15aj6MMImI5UloFQKrOK6SUsrFGA4GIlHDiOsNg1QqMMaZ9LuPlKIOIOI6UOogIDSE+0p8rdiqlVNvW6DegMaZ9TyPREEcpRMRSUFpFSlwEIjqSWCllX+33yZ/T4W0aKihz6BgCpZTt2TYQuMJi+OFgCSmx2j+glLI3ewaCqlJW7HeQe7RCB5MppWzPnoHAUcqBKqt7pHqtYqWUsiubBoIyqiQagKQY7SNQStmbPZ+b9HYWp0ZE8pvL+gc6N0opFVD2qxF4POAso8gVwaVndqJDVPtdX0cppVqC/QKB01qjuNAVrkFAKaWwYyDwzjxaZqLoEG3PljGllKrNhoHAmnm0zERpjUAppbBlILBqBOVEkhCtgUAppWwbCMqIooMGAqWUsmMgsJqGyommW1JMgDOjlFKBZ9tAMLRXBt1TNBAopZQNA4HVNJSclBTgjCilVNtgu0DgqbJqBOHRwbnUglJKnSrbBQJHRQkAETHtc3E1pZRqabYLBM7KMtxGiI6KDnRWlFKqTbBdIHBVVeAgnFgdTKaUUoAdA4HTgZMw4iJDA50VpZRqE/waCETkYhHZKiLbReQhH8ezReSYiKzxvn7nz/w4PYbP1+2lijBiI3SeIaWUAj+uRyAiocCLwEQgF1ghIguMMZvqJf3aGHO5v/JR2/4SDzHGiYNw4qI0ECilFPi3RjAa2G6M2WmMcQDzgKv8eL+TcnogXFw4TRhxkRoIlFIK/BsIMoB9tbZzvfvqO1tE1orIJyIy0I/5odRpiMCJgzBiNRAopRTg36Uqxcc+U297NdDDGFMqIpcCHwB9TriQyHRgOkB6ejo5OTnNylBBSSURuHASxupl3xIR6iuLwaW0tLTZP6/2SstsD1rmluPPQJALdKu13RXIq53AGFNc6/NCEXlJRFKNMUfqpZsNzAYYOXKkyc7OblaGPtv9b28gCGfiedmIBH8gyMnJobk/r/ZKy2wPWuaW48+moRVAHxHJEpEIYCqwoHYCEekk3m9jERntzU+BvzJU6jREiIshmR1tEQSUUqop/FYjMMa4ROQe4DMgFJhjjNkoIj/zHn8ZmAT8XERcQAUw1RhTv/moxZQ5DdEhbiQswl+3UEqpdsevPabGmIXAwnr7Xq71+QXgBX/mobYyhyE6xAWhka11S6WUavNsNbK4zAmR4oJQnV5CKaWq2SoQVLoNEeKGMK0RKKVUNVsFgio3RODUpiGllKrFVoHA4TaEG6c2DSmlVC22CgRVbgjDpU1DSilVi80CQXWNQB8fVUqparYJBMYYqtwQqoFAKaXqsE0gcLg9GOMh1GjTkFJK1WabQFDhcBOBy9rQzmKllKphm0BQ7nATXhMItEaglFLVbBUIjtcItI9AKaWq2SYQWE1DTmtDJ51TSqkatgkEZQ4X4aJNQ0opVZ9tAoF2FiullG+2CQR1+gj08VGllKphm0AwuGsCk3p5N7RpSCmlatgmEHRLjmF4qnfxM20aUkqpGrYJBABitGlIKaXqs1UgCPF4Hx/VcQRKKVXDZoFAB5QppVR9tgoEYqoHlGnTkFJKVbNVIDjeNKSdxUopVc1mgUBHFiulVH22CgQ1TUPaR6CUUjVsFQhqagQ66ZxSStWwWSCorhFo05BSSlWzVSCoGVCmTUNKKVXDVoEgxOOEkDAIsVWxlVKqUbb6RgzxuLQ2oJRS9YQFOgOtSYxTA4EKKk6nk9zcXBISEti8eXOgs9OqtMy+RUVF0bVrV8LDmz5eylaBIMTj0lHFKqjk5uYSHx9PSkoKHTp0CHR2WlVJSQnx8fGBzkarOlmZjTEUFBSQm5tLVlZWk69rs6YhrRGo4FJZWUlKSgoiEuisqDZAREhJSaGysvKUzrNVINCmIRWMNAio2prz/8FWgUA7i5VS6kQ2CwROHVWsVAsqKiripZdeata5l156KUVFRS2bIdUstgoE2jSkVMtqLBC43e5Gz124cCGJiYl+yNXpMcbg8XgCnY1WZbOnhpwQZq8nK5R9/P6jjWzKK27Raw7o0oFHrhjY4PGHHnqIHTt2MHToUCZOnMhll13G73//ezp37syaNWvYtGkTV199Nfv27aOyspL77ruP6dOnA5CZmcnKlSspLS3lkksuYdy4cXz77bdkZGTw4YcfEh0dXedeH330EY899hgOh4OUlBReeeUV4uPjKS0tZcaMGaxcuRIR4ZFHHuG6667j008/5Te/+Q1ut5vU1FS+/PJLZs6cSVxcHL/61a8AGDRoEB9//DEAl1xyCRMmTOC7777jgw8+4IknnmDFihVUVFQwadIkfv/73wOwYsUK7rvvPsrKyoiMjOTLL7/k0ksv5c9//jNDhw4FYOzYscyaNYvBgwe36L+Hv/i1RiAiF4vIVhHZLiIPNZJulIi4RWSSP/OjTw0p1bKeeOIJevXqxZo1a3jqqacAWL58OY8//jibNm0CYM6cOaxatYqVK1fy/PPPU1BQcMJ1tm3bxt13383GjRtJTEzk/fffPyHNuHHjWLp0Kd9//z1Tp07l2WefBeDRRx8lISGB9evXs27dOs477zwOHz7MHXfcwfvvv8/atWv5xz/+cdKybN26lWnTpvH999/To0cPHn/8cVauXMm6detYvHgx69atw+FwMGXKFJ577jnWrl3LF198QXR0NLfffjuvv/46AD/88ANVVVXtJgiAH2sEIhIKvAhMBHKBFSKywBizyUe6PwCf+Ssv1XQcgQpmjf3l3ppGjx5d5xn2559/nvnz5wOwb98+tm3bRkpKSp1zsrKyav6aHjFiBLt37z7hurm5uUyZMoX8/HwcDgfdunUD4IsvvmDevHk16ZKSkvjoo48455xzavKRnJx80nz36NGDs846q2b73XffZfbs2bhcLvLz89m0aRMiQufOnRk1ahRAzdiNyZMn8+ijj/LUU08xZ84cbrnllpPery3xZ41gNLDdGLPTGOMA5gFX+Ug3A3gfOOTHvADaR6BUa4iNja35nJOTwxdffMF3333H2rVrGTZsmM9n3CMjj/+BFhoaisvlOiHNjBkzuOeee1i/fj2vvPIKVVVVgNWmX/+RSV/7AMLCwuq0/9fOS+1879q1i6effpovv/ySdevWcdlll1FZWdngdWNiYpg4cSIffvgh7777LjfccIPPn01b5c9AkAHsq7Wd691XQ0QygGuAl/2YjxohHgeERbXGrZSyhfj4eEpKSho8fuzYMZKSkoiJiWHLli0sXbq02fc6duwYGRnWV8gbb7xRs//CCy/khRdeqNk+evQoZ599NosXL2bXrl0AFBYWAla/xOrVqwFYvXp1zfH6iouLiY2NJSEhgYMHD/LJJ58A0K9fP/Ly8lixYgVgjfStDlq333479957L6NGjWpSDaQt8Wdnsa9RDabe9rPAg8YYd2ODIERkOjAdID09nZycnGZl6Cy3g7zDBfzQzPPbo9LS0mb/vNorO5U5ISGBkpIS3G53o1/I/hIREcHo0aMZMGAAEydO5KKLLsLlctXkZezYsbzwwgsMGjSIPn36MGrUKMrLyykpKcEYQ2lpKaWlpXg8nppzqqqqqKqqOqE8Dz74IJMmTappmjHGUFJSwn333ccDDzzAgAEDCA0N5aGHHuLKK6/k2Wef5eqrr8bj8ZCWlsaHH37IhRdeyJw5cxg8eDDDhw+nd+/elJaWAtTJQ8+ePRk0aBD9+/cnMzOTMWPGUFlZSVVVFXPmzOGuu+6isrKSqKgoFixYQFxcHH379iUuLo4pU6b47d+iqf/OlZWVp/Y7YIzxyws4G/is1vavgV/XS7ML2O19lWI1D13d2HVHjBhhmsvxaIYxHz/Q7PPbo0WLFgU6C63OTmXetGmTMcaY4uLiAOek9bW1Mu/fv9/06dPHuN1uv92jqWWu/n9RG7DSNPC96s+moRVAHxHJEpEIYCqwoF4QyjLGZBpjMoH3gLuMMR/4K0PW46PaWayUallvvvkmY8aM4fHHHyekHa534remIWOMS0TuwXoaKBSYY4zZKCI/8x5vlX6B2qw+Ag0ESqmWNW3aNKZNmxbobDSbXweUGWMWAgvr7fMZAIwxt/gzL3jcCB5dr1gppeppf3WY5nJZj5rpXENKKVWXfQKB2xsItEaglFJ12CcQ1NQINBAopVRtGgiUUq0qLi4OgLy8PCZN8j29WHZ2NitXrmz0Oi+++CLl5eU12zqtdfPZJxC4Hda7Ng0p1SZ06dKF9957r9nnz5o1q04gaKvTWjfEtKHpru0zDbV2Fqtg98lDcGB9y16z05lwyRMNHn7wwQfp0aMHd911FwAzZ84kPj6eO++8k6uuuoqjR4/idDp57LHHuOqqulON7d69m8svv5wNGzZQUVHBrbfeyqZNm+jfvz8VFRU16X7+85+fMB30888/T35+PhMmTCA1NZVFixbVTGudmprKH//4R+bMmQNYUz/cf//97N69u1nTXc+dO5f09PQ2Md31RRddxEsvvdTi013bMBDoXENKtZSpU6dy//331wSCd999l08//ZSoqCjmz59Phw4dOHLkCGeddRZXXnllg+vpzpo1i5iYGNatW8e6desYPnx4zbHHH3+c5ORk3G43559/PuvWrePee+/lmWeeYdGiRaSmpta51qpVq3jttddYtmwZxhjGjBnDueeeS1JSEtu2bePtt9/mL3/5C9dffz3vv/8+N910U53zq6e7FhFeffVVnnzySZ555pk6012DNadR9XTXS5YsISsrq2ZOo8Zs3bqV1157rWZBH1/l69evH1OmTOGdd95h1KhRFBcXEx0dzbRp03j99dd59tlnW3S6a/sEgpqnhrRGoIJUI3+5+8uwYcM4dOgQeXl5HD58mKSkJLp3747T6eQ3v/kNS5YsISQkhP3793Pw4EE6derk8zpLlizh3nvvBWDw4MF1vtx8TQfd2JffN998wzXXXFMzm+i1117L119/zZVXXtms6a6rp7JuC9NdX3PNNYwdO7bFp7u2TyDQzmKl/GLSpEm89957HDhwgKlTpwIwd+5cDh8+zKpVqwgPDyczM9Pn9NO1+aotVE8HvWLFCpKSkrjllltOeh1rWh3f6k93XbsJqtqMGTP45S9/yZVXXklOTg4zZ86sua6/pruuX76Grlt/uuuTdag3lXYWK6VOy9SpU5k3bx7vvfdezVNAx44do2PHjoSHh7No0SL27NnT6DXOOecc5s6dC8CGDRtYt24d0PB00GA9feRrJs5zzjmHDz74gPLycsrKypg/fz7jx49vcnnsON21fQKByxuRtUagVIsaOHAgJSUlZGRk0LlzZwBuvPFGVq5cyciRI5k7dy79+vVr9Bo///nPKS0tZfDgwTz55JOMHj0agCFDhjBs2DAGDhzIbbfdxtixY2vOueWWW2o6XmsbPnw4t9xyC6NHj2bMmDHcfvvtDBs2rMnlmTlzJpMnT2b8+PF1+h8efvhhjh49yqBBgxgyZAiLFi0iLS2N2bNnc+211zJkyBCmTJkCwHXXXUdhYSFDhw5l1qxZ9O3b1+e9GipfREQE77zzDjNmzGDIkCFMnDixplYxYsQIOnTowK233trkMp2MNFaNaotGjhxpmlUd2ruMQx/NpONNr0JCxsnTB4mcnByys7MDnY1WZacyb968mf79+1NSUkJ8fHygs9Oq7FrmkpISsrOz2bJlS4MznVb/v6hNRFYZY0b6Sm+fGkH3MWwa+KCtgoBSKri89dZbfpnu2j6dxUop1c7dcMMN3HnnnS1+XfvUCJQKUu2teVf5V3P+P2ggUKodi4qKoqCgQIOBAqwgUFBQQFTUqQ2c1aYhpdqxrl27kpubS1FR0Sn/8rd31YvH20lTyhwVFUXXrl1P6boaCJRqx8LDw8nKyiInJ+eUHpEMBlrmlqNNQ0opZXMaCJRSyuY0ECillM21u5HFInIYaHzikoalAkdaMDvtgZbZHrTM9nA6Ze5hjEnzdaDdBYLTISIrGxpiHay0zPagZbYHf5VZm4aUUsrmNBAopZTN2S0QzA50BgJAy2wPWmZ78EuZbdVHoJRS6kR2qxEopZSqRwOBUkrZnG0CgYhcLCJbRWS7iDwU6Py0FBGZIyKHRGRDrX3JIvJvEdnmfU+qdezX3p/BVhG5KDC5Pj0i0k1EFonIZhHZKCL3efcHbblFJEpElovIWm+Zf+/dH7RlBhCRUBH5XkQ+9m4HdXkBRGS3iKwXkTUistK7z7/lNsYE/QsIBXYAPYEIYC0wIND5aqGynQMMBzbU2vck8JD380PAH7yfB3jLHglkeX8moYEuQzPK3BkY7v0cD/zgLVvQlhsQIM77ORxYBpwVzGX2luOXwFvAx97toC6vtyy7gdR6+/xabrvUCEYD240xO40xDmAecFWA89QijDFLgMJ6u68C3vB+fgO4utb+ecaYKmPMLmA71s+mXTHG5BtjVns/lwCbgQyCuNzGUurdDPe+DEFcZhHpClwGvFprd9CW9yT8Wm67BIIMYF+t7VzvvmCVbozJB+tLE+jo3R90PwcRyQSGYf2FHNTl9jaTrAEOAf82xgR7mZ8F/hvw1NoXzOWtZoDPRWSViEz37vNrue2yHoH42GfH52aD6ucgInHA+8D9xphiEV/Fs5L62Nfuym2McQNDRSQRmC8igxpJ3q7LLCKXA4eMMatEJLspp/jY127KW89YY0yeiHQE/i0iWxpJ2yLltkuNIBfoVmu7K5AXoLy0hoMi0hnA+37Iuz9ofg4iEo4VBOYaY/7p3R305QYwxhQBOcDFBG+ZxwJXishurKbc80Tk7wRveWsYY/K874eA+VhNPX4tt10CwQqgj4hkiUgEMBVYEOA8+dMC4Gbv55uBD2vtnyoikSKSBfQBlgcgf6dFrD/9/wpsNsb8sdahoC23iKR5awKISDRwAbCFIC2zMebXxpiuxphMrN/Xr4wxNxGk5a0mIrEiEl/9GbgQ2IC/yx3oHvJW7Im/FOvpkh3AbwOdnxYs19tAPuDE+uvgp0AK8CWwzfueXCv9b70/g63AJYHOfzPLPA6r+rsOWON9XRrM5QYGA997y7wB+J13f9CWuVY5sjn+1FBQlxfryca13tfG6u8qf5dbp5hQSimbs0vTkFJKqQZoIFBKKZvTQKCUUjangUAppWxOA4FSStmcBgKl/ExEsqtnz1SqLdJAoJRSNqeBQCkvEbnJO+f/GhF5xTvJW6mIPCMiq0XkSxFJ86YdKiJLRWSdiMyvnh9eRHqLyBfedQNWi0gv7+XjROQ9EdkiInO9o6MRkSdEZJP3Ok8HqOjK5jQQKAWISH9gCtaEX0MBN3AjEAusNsYMBxYDj3hPeRN40BgzGFhfa/9c4EVjzBDgR1ijvsGaIfV+rPnjewJjRSQZuAYY6L3OY/4so1IN0UCglOV8YASwwjvV8/lYX9ge4B1vmr8D40QkAUg0xiz27n8DOMc7R0yGMWY+gDGm0hhT7k2z3BiTa4zxYE2JkQkUA5XAqyJyLVCdVqlWpYFAKYsAbxhjhnpfZxhjZvpI19icLA3Ogw1U1frsBsKMMS6smSXfx1po5NNTy7JSLUMDgVKWL4FJ3jngq9eI7YH1OzLJm+YG4BtjzDHgqIiM9+7/CbDYGFMM5IrI1d5rRIpITEM39K6nkGCMWYjVbDS0xUulVBPYZWEapRpljNkkIg9jrQwVgjWb691AGTBQRFYBx7D6EcCaCvhl7xf9TuBW7/6fAK+IyP96rzG5kdvGAx+KSBRWbeIXLVwspZpEZx9VqhEiUmqMiQt0PpTyJ20aUkopm9MagVJK2ZzWCJRSyuY0ECillM1pIFBKKZvTQKCUUjangUAppWzu/wPbRqU3YR6w8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(hist.history['accuracy'], label='train accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], label='validation accuracy')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef55c444-6d5c-4121-b005-95e11e3c7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44dd440-fccd-4992-b7b1-9571f5780a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.shape =  (532, 8) , t_data.shape =  (532, 1)\n"
     ]
    }
   ],
   "source": [
    "x_data=training_x_data\n",
    "t_data=training_t_data\n",
    "\n",
    "print(\"x_data.shape = \", x_data.shape, \", t_data.shape = \", t_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46c8b977-6aa2-4b80-beac-7d2047fd0a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.48966118]\n",
      " [0.08741489]\n",
      " [0.89959056]\n",
      " [0.92359205]\n",
      " [0.53823739]\n",
      " [0.02233202]\n",
      " [0.53096257]\n",
      " [0.96048907]] , W.shape =  (8, 1) b =  [0.88993076] , b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "W=np.random.rand(8,1) # 8*1 정렬\n",
    "b=np.random.rand(1)\n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \"b = \", b, \", b.shape = \", b.shape,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "740f9361-bb3f-4fdb-99e3-db314f0455ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def loss_func(x,t):\n",
    "    delta=1e-7 # log 무한대 발산 방지\n",
    "    \n",
    "    z=np.dot(x,W)+b\n",
    "    y=sigmoid(z)\n",
    "    \n",
    "    # cross-entropy\n",
    "    return -np.sum(t*np.log(y+delta) + (1-t)*np.log((1-y)+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46335331-e036-40d2-ac2a-0f1c69afb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f,x):\n",
    "    delta_x= 1e-4\n",
    "    grad=np.zeros_like(x) \n",
    "    # 수치미분 결과를 grad에 저장\n",
    "    # x와 똑같은 shape을 가진 배열(혹은 행렬) 생성 (요소값은 0임)\n",
    "    \n",
    "    it=np.nditer(x,flags=['multi_index'],op_flags=['readwrite'])\n",
    "    #모든 입력변수에 대해 편미분하기 위해 iterator 획득\n",
    "    # iterator 하나 생성. 플래그는 멀티 인덱스 설정\n",
    "    # 멀티 인덱스를 설정하면 다차원 배열이라도 \n",
    "    # 순차적으로 interator 생성 후 값을 꺼낼 수 있음\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx=it.multi_index\n",
    "        \n",
    "        tmp_val=x[idx] \n",
    "        # 임시 변수로 x[idx] 의 원값을 저장\n",
    "        # numpy 타입은 mutable 이므로 원래 값 보관\n",
    "        \n",
    "        x[idx]=float(tmp_val)+delta_x # 전향 차분\n",
    "        fx1=f(x) #f(x+delta_x) # 첫번째 인자로 들어온 함수를 대입\n",
    "        \n",
    "        x[idx]=tmp_val-delta_x # 후향 차분\n",
    "        fx2=f(x) #f(x-delta_x) # 첫번째 인자로 들어온 함수를 다시 대입\n",
    "        \n",
    "        grad[idx]=(fx1-fx2)/(2*delta_x) # 미분 결과를 grad에...\n",
    "        \n",
    "        x[idx]=tmp_val\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0590e80d-f042-499a-a43f-bc1a118921d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_val(x,t):\n",
    "    delta=1e-7 # log 무한대 발산 방지\n",
    "    \n",
    "    z=np.dot(x,W)+b\n",
    "    y=sigmoid(z)\n",
    "    \n",
    "    return -np.sum(t*np.log(y+delta) + (1-t)*np.log((1-y)+delta))\n",
    "\n",
    "def predict(x):\n",
    "    \n",
    "    z=np.dot(x,W)+b\n",
    "    y=sigmoid(z)\n",
    "    \n",
    "    if y>=0.5:\n",
    "        result=1 #True\n",
    "    else:\n",
    "        result=0 #False\n",
    "    return y, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "651608e0-cf92-4fe1-a411-922df27df68a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  508.30906095234667 Initial W =  [[0.48966118]\n",
      " [0.08741489]\n",
      " [0.89959056]\n",
      " [0.92359205]\n",
      " [0.53823739]\n",
      " [0.02233202]\n",
      " [0.53096257]\n",
      " [0.96048907]] \n",
      " ,b= [0.88993076]\n",
      "step =  0 error value =  502.15408137335294 W =  [[0.4800334 ]\n",
      " [0.08549461]\n",
      " [0.90065801]\n",
      " [0.91698787]\n",
      " [0.53034839]\n",
      " [0.02010416]\n",
      " [0.52047991]\n",
      " [0.94958983]] ,b =  [0.90342675]\n",
      "step =  400 error value =  289.7593687933703 W =  [[-0.45503958]\n",
      " [-1.30113828]\n",
      " [ 0.56173413]\n",
      " [ 0.32294464]\n",
      " [-0.15002585]\n",
      " [-0.54121554]\n",
      " [ 0.07126182]\n",
      " [ 0.13875209]] ,b =  [0.93967457]\n",
      "step =  800 error value =  263.6728739815805 W =  [[-0.7130194 ]\n",
      " [-2.09200504]\n",
      " [ 0.38770345]\n",
      " [ 0.11813656]\n",
      " [-0.2853704 ]\n",
      " [-0.89786764]\n",
      " [-0.10071682]\n",
      " [-0.03813442]] ,b =  [0.73996686]\n",
      "step =  1200 error value =  254.3996540306363 W =  [[-8.21165254e-01]\n",
      " [-2.56624567e+00]\n",
      " [ 3.15831368e-01]\n",
      " [-1.23628025e-03]\n",
      " [-3.16603843e-01]\n",
      " [-1.16072860e+00]\n",
      " [-2.62368348e-01]\n",
      " [-1.03363722e-01]] ,b =  [0.64897695]\n",
      "step =  1600 error value =  250.1023904009353 W =  [[-0.87108888]\n",
      " [-2.879282  ]\n",
      " [ 0.28895053]\n",
      " [-0.0784455 ]\n",
      " [-0.31349565]\n",
      " [-1.36659126]\n",
      " [-0.3939651 ]\n",
      " [-0.12915648]] ,b =  [0.59777783]\n",
      "step =  2000 error value =  247.80244071505547 W =  [[-0.89606782]\n",
      " [-3.09794708]\n",
      " [ 0.28487103]\n",
      " [-0.131745  ]\n",
      " [-0.29960043]\n",
      " [-1.53347885]\n",
      " [-0.4973983 ]\n",
      " [-0.14131436]] ,b =  [0.5636475]\n",
      "step =  2400 error value =  246.46010408408102 W =  [[-0.90956225]\n",
      " [-3.25622325]\n",
      " [ 0.29343903]\n",
      " [-0.16999404]\n",
      " [-0.28354333]\n",
      " [-1.67190876]\n",
      " [-0.57790853]\n",
      " [-0.14922102]] ,b =  [0.53778698]\n",
      "step =  2800 error value =  245.62768658189023 W =  [[-0.917461  ]\n",
      " [-3.37354016]\n",
      " [ 0.30938102]\n",
      " [-0.19804879]\n",
      " [-0.26853301]\n",
      " [-1.78868393]\n",
      " [-0.64049788]\n",
      " [-0.15622931]] ,b =  [0.51642045]\n",
      "step =  3200 error value =  245.08602273123586 W =  [[-0.92248798]\n",
      " [-3.46194646]\n",
      " [ 0.3296853 ]\n",
      " [-0.21883393]\n",
      " [-0.25564589]\n",
      " [-1.88849284]\n",
      " [-0.68926046]\n",
      " [-0.1634027 ]] ,b =  [0.49780767]\n",
      "step =  3600 error value =  244.71868201323582 W =  [[-0.9259595 ]\n",
      " [-3.529363  ]\n",
      " [ 0.3525044 ]\n",
      " [-0.23424407]\n",
      " [-0.2450892 ]\n",
      " [-1.97470951]\n",
      " [-0.7273904 ]\n",
      " [-0.17094491]] ,b =  [0.48109702]\n",
      "step =  4000 error value =  244.46022523973346 W =  [[-0.9285374 ]\n",
      " [-3.58122665]\n",
      " [ 0.37664392]\n",
      " [-0.24557825]\n",
      " [-0.23672263]\n",
      " [-2.04984124]\n",
      " [-0.75733712]\n",
      " [-0.17876343]] ,b =  [0.465845]\n",
      "step =  4400 error value =  244.27227768949268 W =  [[-0.93056836]\n",
      " [-3.62139209]\n",
      " [ 0.40130319]\n",
      " [-0.25376733]\n",
      " [-0.23028247]\n",
      " [-2.11579898]\n",
      " [-0.78096886]\n",
      " [-0.18669069]] ,b =  [0.45180268]\n",
      "step =  4800 error value =  244.1315399346791 W =  [[-0.93224256]\n",
      " [-3.65266004]\n",
      " [ 0.42593347]\n",
      " [-0.25950147]\n",
      " [-0.22547867]\n",
      " [-2.17406963]\n",
      " [-0.79971025]\n",
      " [-0.19456604]] ,b =  [0.4388168]\n",
      "step =  5200 error value =  244.0234268631246 W =  [[-0.93366993]\n",
      " [-3.67710367]\n",
      " [ 0.45015528]\n",
      " [-0.26330595]\n",
      " [-0.22203573]\n",
      " [-2.22583053]\n",
      " [-0.81465   ]\n",
      " [-0.20226121]] ,b =  [0.42678259]\n",
      "step =  5600 error value =  243.93854566572685 W =  [[-0.93491772]\n",
      " [-3.69627956]\n",
      " [ 0.47370706]\n",
      " [-0.26558857]\n",
      " [-0.21970807]\n",
      " [-2.2720279 ]\n",
      " [-0.82662282]\n",
      " [-0.20968362]] ,b =  [0.41562079]\n",
      "step =  6000 error value =  243.87067951279272 W =  [[-0.93602941]\n",
      " [-3.71136919]\n",
      " [ 0.49641181]\n",
      " [-0.26667072]\n",
      " [-0.21828383]\n",
      " [-2.31343213]\n",
      " [-0.836271  ]\n",
      " [-0.21677184]] ,b =  [0.40526623]\n",
      "step =  6400 error value =  243.81560125504586 W =  [[-0.93703453]\n",
      " [-3.72327692]\n",
      " [ 0.51815422]\n",
      " [-0.26680853]\n",
      " [-0.2175837 ]\n",
      " [-2.35067754]\n",
      " [-0.84409058]\n",
      " [-0.22348912]] ,b =  [0.39566211]\n",
      "step =  6800 error value =  243.77035900090652 W =  [[-0.93795398]\n",
      " [-3.73269939]\n",
      " [ 0.53886457]\n",
      " [-0.26620774]\n",
      " [-0.21745773]\n",
      " [-2.38429172]\n",
      " [-0.85046603]\n",
      " [-0.22981731]] ,b =  [0.38675708]\n",
      "step =  7200 error value =  243.73283684616473 W =  [[-0.93880289]\n",
      " [-3.74017597]\n",
      " [ 0.558507  ]\n",
      " [-0.26503458]\n",
      " [-0.21778157]\n",
      " [-2.41471733]\n",
      " [-0.85569634]\n",
      " [-0.23575175]] ,b =  [0.37850372]\n",
      "step =  7600 error value =  243.70147952835248 W =  [[-0.93959251]\n",
      " [-3.74612587]\n",
      " [ 0.57707077]\n",
      " [-0.26342379]\n",
      " [-0.21845271]\n",
      " [-2.44232883]\n",
      " [-0.86001479]\n",
      " [-0.2412972 ]] ,b =  [0.37085778]\n",
      "step =  8000 error value =  243.67511658952446 W =  [[-0.94033121]\n",
      " [-3.7508759 ]\n",
      " [ 0.59456369]\n",
      " [-0.26148485]\n",
      " [-0.21938712]\n",
      " [-2.46744519]\n",
      " [-0.86360394]\n",
      " [-0.2464649 ]] ,b =  [0.36377773]\n",
      "step =  8400 error value =  243.65284797566179 W =  [[-0.94102535]\n",
      " [-3.75468142]\n",
      " [ 0.61100714]\n",
      " [-0.25930667]\n",
      " [-0.22051624]\n",
      " [-2.49033998]\n",
      " [-0.86660711]\n",
      " [-0.25127027]] ,b =  [0.35722452]\n",
      "step =  8800 error value =  243.63396817283854 W =  [[-0.94167971]\n",
      " [-3.75774234]\n",
      " [ 0.62643216]\n",
      " [-0.25696146]\n",
      " [-0.22178442]\n",
      " [-2.5112492 ]\n",
      " [-0.86913711]\n",
      " [-0.25573127]] ,b =  [0.35116138]\n",
      "step =  9200 error value =  243.61791489072158 W =  [[-0.94229798]\n",
      " [-3.76021541]\n",
      " [ 0.64087654]\n",
      " [-0.25450767]\n",
      " [-0.2231467 ]\n",
      " [-2.5303776 ]\n",
      " [-0.87128304]\n",
      " [-0.25986727]] ,b =  [0.34555378]\n",
      "step =  9600 error value =  243.60423362593 W =  [[-0.94288299]\n",
      " [-3.76222363]\n",
      " [ 0.65438255]\n",
      " [-0.25199245]\n",
      " [-0.224567  ]\n",
      " [-2.54790372]\n",
      " [-0.87311545]\n",
      " [-0.26369815]] ,b =  [0.34036927]\n",
      "step =  10000 error value =  243.59255266356337 W =  [[-0.94343695]\n",
      " [-3.76386365]\n",
      " [ 0.66699513]\n",
      " [-0.24945366]\n",
      " [-0.22601657]\n",
      " [-2.563984  ]\n",
      " [-0.87469044]\n",
      " [-0.26724379]] ,b =  [0.33557741]\n",
      "step =  10400 error value =  243.58256505621003 W =  [[-0.94396166]\n",
      " [-3.76521146]\n",
      " [ 0.67876063]\n",
      " [-0.24692145]\n",
      " [-0.22747267]\n",
      " [-2.57875614]\n",
      " [-0.87605274]\n",
      " [-0.2705236 ]] ,b =  [0.33114972]\n",
      "step =  10800 error value =  243.57401535092367 W =  [[-0.9444586 ]\n",
      " [-3.76632685]\n",
      " [ 0.68972575]\n",
      " [-0.2444196 ]\n",
      " [-0.22891751]\n",
      " [-2.59234183]\n",
      " [-0.87723815]\n",
      " [-0.2735563 ]] ,b =  [0.32705953]\n",
      "step =  11200 error value =  243.56668960846883 W =  [[-0.944929  ]\n",
      " [-3.76725686]\n",
      " [ 0.69993678]\n",
      " [-0.24196668]\n",
      " [-0.23033738]\n",
      " [-2.60484903]\n",
      " [-0.87827546]\n",
      " [-0.27635972]] ,b =  [0.32328196]\n",
      "step =  11600 error value =  243.56040775081325 W =  [[-0.94537401]\n",
      " [-3.76803859]\n",
      " [ 0.70943908]\n",
      " [-0.23957691]\n",
      " [-0.23172185]\n",
      " [-2.61637391]\n",
      " [-0.87918793]\n",
      " [-0.27895075]] ,b =  [0.31979377]\n",
      "step =  12000 error value =  243.55501758867172 W =  [[-0.94579463]\n",
      " [-3.76870125]\n",
      " [ 0.71827664]\n",
      " [-0.23726094]\n",
      " [-0.2330632 ]\n",
      " [-2.62700237]\n",
      " [-0.87999444]\n",
      " [-0.28134524]] ,b =  [0.31657335]\n",
      "step =  12400 error value =  243.5503900861151 W =  [[-0.94619182]\n",
      " [-3.76926791]\n",
      " [ 0.72649179]\n",
      " [-0.23502653]\n",
      " [-0.2343559 ]\n",
      " [-2.63681143]\n",
      " [-0.8807104 ]\n",
      " [-0.28355803]] ,b =  [0.31360056]\n",
      "step =  12800 error value =  243.54641555416094 W =  [[-0.94656653]\n",
      " [-3.76975676]\n",
      " [ 0.73412503]\n",
      " [-0.23287902]\n",
      " [-0.23559619]\n",
      " [-2.64587032]\n",
      " [-0.88134846]\n",
      " [-0.28560296]] ,b =  [0.31085669]\n",
      "step =  13200 error value =  243.54300055512715 W =  [[-0.94691965]\n",
      " [-3.7701822 ]\n",
      " [ 0.74121495]\n",
      " [-0.2308218 ]\n",
      " [-0.2367817 ]\n",
      " [-2.65424148]\n",
      " [-0.88191907]\n",
      " [-0.28749285]] ,b =  [0.30832437]\n",
      "step =  13600 error value =  243.5400653602227 W =  [[-0.94725208]\n",
      " [-3.77055562]\n",
      " [ 0.74779809]\n",
      " [-0.22885668]\n",
      " [-0.23791121]\n",
      " [-2.66198132]\n",
      " [-0.88243091]\n",
      " [-0.28923963]] ,b =  [0.30598748]\n",
      "step =  14000 error value =  243.5375418444629 W =  [[-0.94756472]\n",
      " [-3.77088605]\n",
      " [ 0.753909  ]\n",
      " [-0.22698421]\n",
      " [-0.23898439]\n",
      " [-2.66914097]\n",
      " [-0.88289128]\n",
      " [-0.29085433]] ,b =  [0.30383106]\n",
      "step =  14400 error value =  243.53537173200885 W =  [[-0.94785844]\n",
      " [-3.77118069]\n",
      " [ 0.75958021]\n",
      " [-0.22520392]\n",
      " [-0.24000162]\n",
      " [-2.67576683]\n",
      " [-0.88330629]\n",
      " [-0.29234711]] ,b =  [0.30184128]\n",
      "step =  14800 error value =  243.53350512559837 W =  [[-0.94813413]\n",
      " [-3.77144526]\n",
      " [ 0.76484231]\n",
      " [-0.22351451]\n",
      " [-0.2409638 ]\n",
      " [-2.68190114]\n",
      " [-0.88368117]\n",
      " [-0.2937274 ]] ,b =  [0.30000532]\n",
      "step =  15200 error value =  243.53189926858394 W =  [[-0.94839265]\n",
      " [-3.77168433]\n",
      " [ 0.76972398]\n",
      " [-0.22191409]\n",
      " [-0.24187225]\n",
      " [-2.6875824 ]\n",
      " [-0.88402037]\n",
      " [-0.29500387]] ,b =  [0.29831132]\n",
      "step =  15600 error value =  243.53051749901147 W =  [[-0.94863486]\n",
      " [-3.7719016 ]\n",
      " [ 0.77425206]\n",
      " [-0.22040026]\n",
      " [-0.24272862]\n",
      " [-2.69284575]\n",
      " [-0.88432773]\n",
      " [-0.29618451]] ,b =  [0.2967483]\n",
      "step =  16000 error value =  243.52932836334088 W =  [[-0.9488616 ]\n",
      " [-3.77210002]\n",
      " [ 0.77845166]\n",
      " [-0.2189703 ]\n",
      " [-0.24353475]\n",
      " [-2.69772336]\n",
      " [-0.88460657]\n",
      " [-0.29727669]] ,b =  [0.29530614]\n",
      "step =  16400 error value =  243.5283048636321 W =  [[-0.94907368]\n",
      " [-3.77228201]\n",
      " [ 0.7823462 ]\n",
      " [-0.21762118]\n",
      " [-0.24429265]\n",
      " [-2.70224466]\n",
      " [-0.88485979]\n",
      " [-0.29828722]] ,b =  [0.29397547]\n",
      "step =  16800 error value =  243.52742381683166 W =  [[-0.94927192]\n",
      " [-3.77244954]\n",
      " [ 0.78595755]\n",
      " [-0.21634974]\n",
      " [-0.24500443]\n",
      " [-2.70643666]\n",
      " [-0.88508994]\n",
      " [-0.29922234]] ,b =  [0.29274763]\n",
      "step =  17200 error value =  243.52666530857792 W =  [[-0.94945708]\n",
      " [-3.77260424]\n",
      " [ 0.78930606]\n",
      " [-0.21515268]\n",
      " [-0.24567222]\n",
      " [-2.71032417]\n",
      " [-0.88529927]\n",
      " [-0.30008782]] ,b =  [0.29161465]\n",
      "step =  17600 error value =  243.52601222694872 W =  [[-0.94962992]\n",
      " [-3.77274745]\n",
      " [ 0.79241068]\n",
      " [-0.21402665]\n",
      " [-0.24629821]\n",
      " [-2.71392998]\n",
      " [-0.88548976]\n",
      " [-0.30088897]] ,b =  [0.29056915]\n",
      "step =  18000 error value =  243.5254498640009 W =  [[-0.94979117]\n",
      " [-3.77288031]\n",
      " [ 0.79528906]\n",
      " [-0.2129683 ]\n",
      " [-0.24688456]\n",
      " [-2.71727506]\n",
      " [-0.88566319]\n",
      " [-0.30163069]] ,b =  [0.28960433]\n",
      "step =  18400 error value =  243.5249655749184 W =  [[-0.94994151]\n",
      " [-3.77300377]\n",
      " [ 0.79795758]\n",
      " [-0.2119743 ]\n",
      " [-0.24743339]\n",
      " [-2.72037874]\n",
      " [-0.88582114]\n",
      " [-0.30231748]] ,b =  [0.28871393]\n",
      "step =  18800 error value =  243.52454848620442 W =  [[-0.95008162]\n",
      " [-3.77311866]\n",
      " [ 0.80043149]\n",
      " [-0.21104135]\n",
      " [-0.24794679]\n",
      " [-2.72325884]\n",
      " [-0.88596503]\n",
      " [-0.30295351]] ,b =  [0.28789214]\n",
      "step =  19200 error value =  243.5241892456871 W =  [[-0.95021213]\n",
      " [-3.77322569]\n",
      " [ 0.80272494]\n",
      " [-0.21016624]\n",
      " [-0.24842676]\n",
      " [-2.72593178]\n",
      " [-0.88609615]\n",
      " [-0.3035426 ]] ,b =  [0.28713364]\n",
      "step =  19600 error value =  243.52387980821882 W =  [[-0.95033365]\n",
      " [-3.77332548]\n",
      " [ 0.80485106]\n",
      " [-0.20934585]\n",
      " [-0.24887527]\n",
      " [-2.72841273]\n",
      " [-0.88621565]\n",
      " [-0.3040883 ]] ,b =  [0.2864335]\n",
      "step =  20000 error value =  243.52361325187786 W =  [[-0.95044674]\n",
      " [-3.77341857]\n",
      " [ 0.80682205]\n",
      " [-0.20857716]\n",
      " [-0.24929417]\n",
      " [-2.73071571]\n",
      " [-0.88632456]\n",
      " [-0.30459385]] ,b =  [0.28578718]\n",
      "step =  20400 error value =  243.5233836202587 W =  [[-0.95055196]\n",
      " [-3.77350546]\n",
      " [ 0.80864923]\n",
      " [-0.20785724]\n",
      " [-0.24968527]\n",
      " [-2.73285367]\n",
      " [-0.88642384]\n",
      " [-0.30506227]] ,b =  [0.28519051]\n",
      "step =  20800 error value =  243.5231857870939 W =  [[-0.95064983]\n",
      " [-3.77358658]\n",
      " [ 0.8103431 ]\n",
      " [-0.20718329]\n",
      " [-0.25005027]\n",
      " [-2.73483858]\n",
      " [-0.88651434]\n",
      " [-0.30549633]] ,b =  [0.28463963]\n",
      "step =  21200 error value =  243.52301534000384 W =  [[-0.95074082]\n",
      " [-3.77366233]\n",
      " [ 0.81191339]\n",
      " [-0.20655265]\n",
      " [-0.25039079]\n",
      " [-2.73668154]\n",
      " [-0.88659685]\n",
      " [-0.30589859]] ,b =  [0.28413098]\n",
      "step =  21600 error value =  243.52286848064045 W =  [[-0.9508254 ]\n",
      " [-3.77373307]\n",
      " [ 0.81336914]\n",
      " [-0.20596273]\n",
      " [-0.25070839]\n",
      " [-2.73839279]\n",
      " [-0.88667206]\n",
      " [-0.30627143]] ,b =  [0.28366129]\n",
      "step =  22000 error value =  243.52274193888806 W =  [[-0.950904  ]\n",
      " [-3.77379914]\n",
      " [ 0.81471872]\n",
      " [-0.20541111]\n",
      " [-0.25100451]\n",
      " [-2.73998184]\n",
      " [-0.88674063]\n",
      " [-0.30661703]] ,b =  [0.28322753]\n",
      "step =  22400 error value =  243.5226328991239 W =  [[-0.95097702]\n",
      " [-3.77386084]\n",
      " [ 0.8159699 ]\n",
      " [-0.20489546]\n",
      " [-0.25128055]\n",
      " [-2.74145749]\n",
      " [-0.88680313]\n",
      " [-0.3069374 ]] ,b =  [0.28282693]\n",
      "step =  22800 error value =  243.52253893682843 W =  [[-0.95104485]\n",
      " [-3.77391845]\n",
      " [ 0.81712986]\n",
      " [-0.20441358]\n",
      " [-0.2515378 ]\n",
      " [-2.74282788]\n",
      " [-0.88686011]\n",
      " [-0.30723441]] ,b =  [0.28245692]\n",
      "step =  23200 error value =  243.52245796408096 W =  [[-0.95110784]\n",
      " [-3.77397225]\n",
      " [ 0.81820528]\n",
      " [-0.20396339]\n",
      " [-0.2517775 ]\n",
      " [-2.74410057]\n",
      " [-0.88691206]\n",
      " [-0.30750979]] ,b =  [0.28211513]\n",
      "step =  23600 error value =  243.52238818268697 W =  [[-0.95116634]\n",
      " [-3.77402248]\n",
      " [ 0.81920235]\n",
      " [-0.20354291]\n",
      " [-0.25200079]\n",
      " [-2.74528256]\n",
      " [-0.88695941]\n",
      " [-0.30776513]] ,b =  [0.28179938]\n",
      "step =  24000 error value =  243.52232804385977 W =  [[-0.95122064]\n",
      " [-3.77406936]\n",
      " [ 0.82012679]\n",
      " [-0.20315028]\n",
      " [-0.25220876]\n",
      " [-2.74638035]\n",
      " [-0.88700257]\n",
      " [-0.30800191]] ,b =  [0.28150766]\n",
      "step =  24400 error value =  243.52227621353623 W =  [[-0.95127105]\n",
      " [-3.77411311]\n",
      " [ 0.82098391]\n",
      " [-0.20278373]\n",
      " [-0.25240243]\n",
      " [-2.74739997]\n",
      " [-0.88704191]\n",
      " [-0.30822149]] ,b =  [0.28123812]\n",
      "step =  24800 error value =  243.52223154253326 W =  [[-0.95131784]\n",
      " [-3.77415394]\n",
      " [ 0.82177863]\n",
      " [-0.20244161]\n",
      " [-0.25258276]\n",
      " [-2.748347  ]\n",
      " [-0.88707778]\n",
      " [-0.30842514]] ,b =  [0.28098905]\n",
      "step =  25200 error value =  243.52219304086702 W =  [[-0.95136126]\n",
      " [-3.77419202]\n",
      " [ 0.82251552]\n",
      " [-0.20212235]\n",
      " [-0.25275063]\n",
      " [-2.74922662]\n",
      " [-0.88711046]\n",
      " [-0.30861401]] ,b =  [0.28075888]\n",
      "step =  25600 error value =  243.52215985565022 W =  [[-0.95140156]\n",
      " [-3.77422755]\n",
      " [ 0.8231988 ]\n",
      " [-0.20182449]\n",
      " [-0.2529069 ]\n",
      " [-2.75004365]\n",
      " [-0.88714026]\n",
      " [-0.3087892 ]] ,b =  [0.28054615]\n",
      "step =  26000 error value =  243.52213125206816 W =  [[-0.95143895]\n",
      " [-3.77426068]\n",
      " [ 0.82383239]\n",
      " [-0.20154663]\n",
      " [-0.25305235]\n",
      " [-2.75080255]\n",
      " [-0.88716742]\n",
      " [-0.3089517 ]] ,b =  [0.28034952]\n",
      "step =  26400 error value =  243.5221065970014 W =  [[-0.95147364]\n",
      " [-3.77429157]\n",
      " [ 0.82441991]\n",
      " [-0.20128747]\n",
      " [-0.25318771]\n",
      " [-2.75150746]\n",
      " [-0.88719217]\n",
      " [-0.30910243]] ,b =  [0.28016777]\n",
      "step =  26800 error value =  243.52208534492624 W =  [[-0.95150582]\n",
      " [-3.77432037]\n",
      " [ 0.82496473]\n",
      " [-0.20104581]\n",
      " [-0.25331366]\n",
      " [-2.75216224]\n",
      " [-0.88721473]\n",
      " [-0.30924227]] ,b =  [0.27999974]\n",
      "step =  27200 error value =  243.52206702577527 W =  [[-0.95153568]\n",
      " [-3.77434721]\n",
      " [ 0.82546997]\n",
      " [-0.20082048]\n",
      " [-0.25343086]\n",
      " [-2.75277044]\n",
      " [-0.88723529]\n",
      " [-0.30937199]] ,b =  [0.2798444]\n",
      "step =  27600 error value =  243.5220512344833 W =  [[-0.95156337]\n",
      " [-3.77437222]\n",
      " [ 0.82593852]\n",
      " [-0.20061042]\n",
      " [-0.25353989]\n",
      " [-2.7533354 ]\n",
      " [-0.88725404]\n",
      " [-0.30949234]] ,b =  [0.27970076]\n",
      "step =  28000 error value =  243.52203762198593 W =  [[-0.95158906]\n",
      " [-3.77439552]\n",
      " [ 0.82637304]\n",
      " [-0.20041462]\n",
      " [-0.25364133]\n",
      " [-2.75386018]\n",
      " [-0.88727112]\n",
      " [-0.309604  ]] ,b =  [0.27956794]\n",
      "step =  28400 error value =  243.522025887466 W =  [[-0.95161289]\n",
      " [-3.77441723]\n",
      " [ 0.82677603]\n",
      " [-0.20023213]\n",
      " [-0.25373568]\n",
      " [-2.75434764]\n",
      " [-0.8872867 ]\n",
      " [-0.3097076 ]] ,b =  [0.27944512]\n",
      "step =  28800 error value =  243.5220157716766 W =  [[-0.95163499]\n",
      " [-3.77443745]\n",
      " [ 0.82714977]\n",
      " [-0.20006207]\n",
      " [-0.25382345]\n",
      " [-2.75480044]\n",
      " [-0.88730089]\n",
      " [-0.30980372]] ,b =  [0.27933152]\n",
      "step =  29200 error value =  243.52200705118906 W =  [[-0.95165549]\n",
      " [-3.77445628]\n",
      " [ 0.82749641]\n",
      " [-0.19990362]\n",
      " [-0.25390508]\n",
      " [-2.75522105]\n",
      " [-0.88731383]\n",
      " [-0.30989291]] ,b =  [0.27922645]\n",
      "step =  29600 error value =  243.52199953343865 W =  [[-0.95167451]\n",
      " [-3.77447381]\n",
      " [ 0.82781792]\n",
      " [-0.19975599]\n",
      " [-0.25398099]\n",
      " [-2.75561175]\n",
      " [-0.88732563]\n",
      " [-0.30997567]] ,b =  [0.27912926]\n",
      "step =  30000 error value =  243.52199305245637 W =  [[-0.95169214]\n",
      " [-3.77449014]\n",
      " [ 0.82811611]\n",
      " [-0.19961846]\n",
      " [-0.2540516 ]\n",
      " [-2.75597467]\n",
      " [-0.88733638]\n",
      " [-0.31005247]] ,b =  [0.27903936]\n",
      "step =  30400 error value =  243.5219874651919 W =  [[-0.9517085 ]\n",
      " [-3.77450533]\n",
      " [ 0.8283927 ]\n",
      " [-0.19949035]\n",
      " [-0.25411725]\n",
      " [-2.75631179]\n",
      " [-0.88734619]\n",
      " [-0.31012373]] ,b =  [0.27895618]\n",
      "step =  30800 error value =  243.52198264834587 W =  [[-0.95172367]\n",
      " [-3.77451948]\n",
      " [ 0.82864926]\n",
      " [-0.19937104]\n",
      " [-0.2541783 ]\n",
      " [-2.75662493]\n",
      " [-0.88735512]\n",
      " [-0.31018986]] ,b =  [0.27887923]\n",
      "step =  31200 error value =  243.52197849564027 W =  [[-0.95173773]\n",
      " [-3.77453264]\n",
      " [ 0.82888724]\n",
      " [-0.19925992]\n",
      " [-0.25423507]\n",
      " [-2.75691581]\n",
      " [-0.88736328]\n",
      " [-0.31025123]] ,b =  [0.27880802]\n",
      "step =  31600 error value =  243.52197491546656 W =  [[-0.95175078]\n",
      " [-3.77454489]\n",
      " [ 0.82910798]\n",
      " [-0.19915644]\n",
      " [-0.25428785]\n",
      " [-2.757186  ]\n",
      " [-0.88737071]\n",
      " [-0.31030818]] ,b =  [0.27874213]\n",
      "step =  32000 error value =  243.52197182885942 W =  [[-0.95176288]\n",
      " [-3.77455629]\n",
      " [ 0.82931275]\n",
      " [-0.19906009]\n",
      " [-0.25433692]\n",
      " [-2.75743698]\n",
      " [-0.88737748]\n",
      " [-0.31036104]] ,b =  [0.27868115]\n",
      "step =  32400 error value =  243.52196916775034 W =  [[-0.9517741 ]\n",
      " [-3.7745669 ]\n",
      " [ 0.82950271]\n",
      " [-0.19897038]\n",
      " [-0.25438254]\n",
      " [-2.75767012]\n",
      " [-0.88738367]\n",
      " [-0.31041009]] ,b =  [0.27862472]\n",
      "step =  32800 error value =  243.52196687346293 W =  [[-0.95178451]\n",
      " [-3.77457676]\n",
      " [ 0.82967893]\n",
      " [-0.19888687]\n",
      " [-0.25442495]\n",
      " [-2.75788667]\n",
      " [-0.8873893 ]\n",
      " [-0.31045561]] ,b =  [0.27857248]\n",
      "step =  33200 error value =  243.52196489541606 W =  [[-0.95179416]\n",
      " [-3.77458594]\n",
      " [ 0.8298424 ]\n",
      " [-0.19880912]\n",
      " [-0.25446438]\n",
      " [-2.75808782]\n",
      " [-0.88739444]\n",
      " [-0.31049787]] ,b =  [0.27852413]\n",
      "step =  33600 error value =  243.52196319000632 W =  [[-0.95180312]\n",
      " [-3.77459447]\n",
      " [ 0.82999406]\n",
      " [-0.19873675]\n",
      " [-0.25450104]\n",
      " [-2.75827466]\n",
      " [-0.88739913]\n",
      " [-0.31053708]] ,b =  [0.27847938]\n",
      "step =  34000 error value =  243.52196171964485 W =  [[-0.95181142]\n",
      " [-3.77460241]\n",
      " [ 0.83013476]\n",
      " [-0.19866939]\n",
      " [-0.25453511]\n",
      " [-2.75844821]\n",
      " [-0.88740341]\n",
      " [-0.31057348]] ,b =  [0.27843795]\n",
      "step =  34400 error value =  243.52196045192724 W =  [[-0.95181912]\n",
      " [-3.7746098 ]\n",
      " [ 0.83026529]\n",
      " [-0.19860669]\n",
      " [-0.25456678]\n",
      " [-2.75860942]\n",
      " [-0.88740732]\n",
      " [-0.31060726]] ,b =  [0.27839959]\n",
      "step =  34800 error value =  243.52195935891814 W =  [[-0.95182626]\n",
      " [-3.77461666]\n",
      " [ 0.83038639]\n",
      " [-0.19854835]\n",
      " [-0.25459622]\n",
      " [-2.75875915]\n",
      " [-0.88741088]\n",
      " [-0.31063861]] ,b =  [0.27836407]\n",
      "step =  35200 error value =  243.52195841653446 W =  [[-0.95183289]\n",
      " [-3.77462304]\n",
      " [ 0.83049875]\n",
      " [-0.19849405]\n",
      " [-0.25462358]\n",
      " [-2.75889824]\n",
      " [-0.88741413]\n",
      " [-0.31066772]] ,b =  [0.27833119]\n",
      "step =  35600 error value =  243.52195760401415 W =  [[-0.95183903]\n",
      " [-3.77462898]\n",
      " [ 0.830603  ]\n",
      " [-0.19844352]\n",
      " [-0.25464901]\n",
      " [-2.75902743]\n",
      " [-0.8874171 ]\n",
      " [-0.31069473]] ,b =  [0.27830073]\n",
      "step =  36000 error value =  243.52195690345766 W =  [[-0.95184473]\n",
      " [-3.7746345 ]\n",
      " [ 0.83069972]\n",
      " [-0.19839651]\n",
      " [-0.25467264]\n",
      " [-2.75914743]\n",
      " [-0.8874198 ]\n",
      " [-0.3107198 ]] ,b =  [0.27827253]\n",
      "step =  36400 error value =  243.5219562994335 W =  [[-0.95185002]\n",
      " [-3.77463963]\n",
      " [ 0.83078947]\n",
      " [-0.19835276]\n",
      " [-0.25469461]\n",
      " [-2.75925889]\n",
      " [-0.88742227]\n",
      " [-0.31074308]] ,b =  [0.27824642]\n",
      "step =  36800 error value =  243.5219557786375 W =  [[-0.95185492]\n",
      " [-3.7746444 ]\n",
      " [ 0.83087274]\n",
      " [-0.19831206]\n",
      " [-0.25471503]\n",
      " [-2.75936241]\n",
      " [-0.88742453]\n",
      " [-0.31076468]] ,b =  [0.27822223]\n",
      "step =  37200 error value =  243.52195532959962 W =  [[-0.95185947]\n",
      " [-3.77464883]\n",
      " [ 0.83095   ]\n",
      " [-0.1982742 ]\n",
      " [-0.254734  ]\n",
      " [-2.75945857]\n",
      " [-0.88742659]\n",
      " [-0.31078474]] ,b =  [0.27819983]\n",
      "step =  37600 error value =  243.5219549424309 W =  [[-0.95186369]\n",
      " [-3.77465295]\n",
      " [ 0.8310217 ]\n",
      " [-0.19823898]\n",
      " [-0.25475164]\n",
      " [-2.75954789]\n",
      " [-0.88742847]\n",
      " [-0.31080335]] ,b =  [0.27817908]\n",
      "step =  38000 error value =  243.52195460860557 W =  [[-0.9518676 ]\n",
      " [-3.77465679]\n",
      " [ 0.83108822]\n",
      " [-0.19820621]\n",
      " [-0.25476802]\n",
      " [-2.75963085]\n",
      " [-0.88743019]\n",
      " [-0.31082063]] ,b =  [0.27815985]\n",
      "step =  38400 error value =  243.52195432077295 W =  [[-0.95187123]\n",
      " [-3.77466035]\n",
      " [ 0.83114996]\n",
      " [-0.19817573]\n",
      " [-0.25478325]\n",
      " [-2.7597079 ]\n",
      " [-0.88743176]\n",
      " [-0.31083667]] ,b =  [0.27814205]\n",
      "step =  38800 error value =  243.52195407259546 W =  [[-0.9518746 ]\n",
      " [-3.77466366]\n",
      " [ 0.83120724]\n",
      " [-0.19814738]\n",
      " [-0.2547974 ]\n",
      " [-2.75977947]\n",
      " [-0.88743319]\n",
      " [-0.31085156]] ,b =  [0.27812555]\n",
      "step =  39200 error value =  243.521953858609 W =  [[-0.95187772]\n",
      " [-3.77466673]\n",
      " [ 0.8312604 ]\n",
      " [-0.19812102]\n",
      " [-0.25481055]\n",
      " [-2.75984594]\n",
      " [-0.8874345 ]\n",
      " [-0.31086538]] ,b =  [0.27811027]\n",
      "step =  39600 error value =  243.52195367410252 W =  [[-0.95188062]\n",
      " [-3.77466959]\n",
      " [ 0.83130972]\n",
      " [-0.19809649]\n",
      " [-0.25482277]\n",
      " [-2.75990768]\n",
      " [-0.8874357 ]\n",
      " [-0.31087821]] ,b =  [0.2780961]\n",
      "step =  40000 error value =  243.52195351501422 W =  [[-0.95188331]\n",
      " [-3.77467225]\n",
      " [ 0.8313555 ]\n",
      " [-0.19807368]\n",
      " [-0.25483412]\n",
      " [-2.75996503]\n",
      " [-0.88743679]\n",
      " [-0.31089012]] ,b =  [0.27808298]\n",
      "step =  40400 error value =  243.521953377842 W =  [[-0.9518858 ]\n",
      " [-3.77467471]\n",
      " [ 0.83139798]\n",
      " [-0.19805247]\n",
      " [-0.25484467]\n",
      " [-2.76001829]\n",
      " [-0.88743779]\n",
      " [-0.31090118]] ,b =  [0.27807083]\n",
      "step =  40800 error value =  243.52195325956635 W =  [[-0.95188812]\n",
      " [-3.77467701]\n",
      " [ 0.8314374 ]\n",
      " [-0.19803275]\n",
      " [-0.25485448]\n",
      " [-2.76006776]\n",
      " [-0.88743871]\n",
      " [-0.31091144]] ,b =  [0.27805956]\n",
      "step =  41200 error value =  243.52195315758382 W =  [[-0.95189026]\n",
      " [-3.77467914]\n",
      " [ 0.83147399]\n",
      " [-0.1980144 ]\n",
      " [-0.25486359]\n",
      " [-2.76011371]\n",
      " [-0.88743954]\n",
      " [-0.31092097]] ,b =  [0.27804912]\n",
      "step =  41600 error value =  243.52195306964978 W =  [[-0.95189225]\n",
      " [-3.77468112]\n",
      " [ 0.83150794]\n",
      " [-0.19799735]\n",
      " [-0.25487205]\n",
      " [-2.76015638]\n",
      " [-0.8874403 ]\n",
      " [-0.31092982]] ,b =  [0.27803944]\n",
      "step =  42000 error value =  243.5219529938288 W =  [[-0.9518941 ]\n",
      " [-3.77468296]\n",
      " [ 0.83153945]\n",
      " [-0.19798149]\n",
      " [-0.25487991]\n",
      " [-2.76019602]\n",
      " [-0.887441  ]\n",
      " [-0.31093803]] ,b =  [0.27803047]\n",
      "step =  42400 error value =  243.52195292845212 W =  [[-0.95189581]\n",
      " [-3.77468467]\n",
      " [ 0.8315687 ]\n",
      " [-0.19796674]\n",
      " [-0.25488722]\n",
      " [-2.76023283]\n",
      " [-0.88744164]\n",
      " [-0.31094566]] ,b =  [0.27802216]\n",
      "step =  42800 error value =  243.52195287208093 W =  [[-0.95189741]\n",
      " [-3.77468626]\n",
      " [ 0.83159584]\n",
      " [-0.19795303]\n",
      " [-0.25489401]\n",
      " [-2.76026702]\n",
      " [-0.88744223]\n",
      " [-0.31095273]] ,b =  [0.27801446]\n",
      "step =  43200 error value =  243.52195282347463 W =  [[-0.95189888]\n",
      " [-3.77468774]\n",
      " [ 0.83162103]\n",
      " [-0.19794028]\n",
      " [-0.25490032]\n",
      " [-2.76029877]\n",
      " [-0.88744276]\n",
      " [-0.3109593 ]] ,b =  [0.27800732]\n",
      "step =  43600 error value =  243.5219527815636 W =  [[-0.95190025]\n",
      " [-3.77468911]\n",
      " [ 0.8316444 ]\n",
      " [-0.19792843]\n",
      " [-0.25490618]\n",
      " [-2.76032826]\n",
      " [-0.88744325]\n",
      " [-0.3109654 ]] ,b =  [0.2780007]\n",
      "step =  44000 error value =  243.52195274542555 W =  [[-0.95190152]\n",
      " [-3.77469038]\n",
      " [ 0.8316661 ]\n",
      " [-0.19791741]\n",
      " [-0.25491162]\n",
      " [-2.76035565]\n",
      " [-0.8874437 ]\n",
      " [-0.31097107]] ,b =  [0.27799456]\n",
      "step =  44400 error value =  243.52195271426515 W =  [[-0.9519027 ]\n",
      " [-3.77469157]\n",
      " [ 0.83168624]\n",
      " [-0.19790717]\n",
      " [-0.25491668]\n",
      " [-2.76038109]\n",
      " [-0.88744411]\n",
      " [-0.31097632]] ,b =  [0.27798888]\n",
      "step =  44800 error value =  243.52195268739683 W =  [[-0.95190379]\n",
      " [-3.77469267]\n",
      " [ 0.83170493]\n",
      " [-0.19789764]\n",
      " [-0.25492138]\n",
      " [-2.76040472]\n",
      " [-0.88744448]\n",
      " [-0.31098121]] ,b =  [0.2779836]\n",
      "step =  45200 error value =  243.5219526642293 W =  [[-0.95190481]\n",
      " [-3.77469369]\n",
      " [ 0.83172228]\n",
      " [-0.19788879]\n",
      " [-0.25492574]\n",
      " [-2.76042666]\n",
      " [-0.88744482]\n",
      " [-0.31098574]] ,b =  [0.27797871]\n",
      "step =  45600 error value =  243.52195264425282 W =  [[-0.95190575]\n",
      " [-3.77469464]\n",
      " [ 0.83173838]\n",
      " [-0.19788056]\n",
      " [-0.2549298 ]\n",
      " [-2.76044704]\n",
      " [-0.88744514]\n",
      " [-0.31098994]] ,b =  [0.27797418]\n",
      "step =  46000 error value =  243.52195262702782 W =  [[-0.95190662]\n",
      " [-3.77469552]\n",
      " [ 0.83175333]\n",
      " [-0.19787291]\n",
      " [-0.25493357]\n",
      " [-2.76046597]\n",
      " [-0.88744543]\n",
      " [-0.31099385]] ,b =  [0.27796998]\n",
      "step =  46400 error value =  243.5219526121753 W =  [[-0.95190743]\n",
      " [-3.77469634]\n",
      " [ 0.8317672 ]\n",
      " [-0.1978658 ]\n",
      " [-0.25493707]\n",
      " [-2.76048355]\n",
      " [-0.88744569]\n",
      " [-0.31099748]] ,b =  [0.27796608]\n",
      "step =  46800 error value =  243.5219525993685 W =  [[-0.95190818]\n",
      " [-3.7746971 ]\n",
      " [ 0.83178008]\n",
      " [-0.19785919]\n",
      " [-0.25494032]\n",
      " [-2.76049988]\n",
      " [-0.88744593]\n",
      " [-0.31100084]] ,b =  [0.27796247]\n",
      "step =  47200 error value =  243.52195258832563 W =  [[-0.95190888]\n",
      " [-3.77469781]\n",
      " [ 0.83179203]\n",
      " [-0.19785305]\n",
      " [-0.25494334]\n",
      " [-2.76051504]\n",
      " [-0.88744615]\n",
      " [-0.31100397]] ,b =  [0.27795912]\n",
      "step =  47600 error value =  243.52195257880373 W =  [[-0.95190953]\n",
      " [-3.77469846]\n",
      " [ 0.83180312]\n",
      " [-0.19784734]\n",
      " [-0.25494614]\n",
      " [-2.76052912]\n",
      " [-0.88744636]\n",
      " [-0.31100687]] ,b =  [0.27795601]\n",
      "step =  48000 error value =  243.52195257059324 W =  [[-0.95191013]\n",
      " [-3.77469907]\n",
      " [ 0.83181342]\n",
      " [-0.19784204]\n",
      " [-0.25494875]\n",
      " [-2.7605422 ]\n",
      " [-0.88744654]\n",
      " [-0.31100957]] ,b =  [0.27795313]\n",
      "step =  48400 error value =  243.52195256351362 W =  [[-0.95191069]\n",
      " [-3.77469964]\n",
      " [ 0.83182298]\n",
      " [-0.1978371 ]\n",
      " [-0.25495117]\n",
      " [-2.76055435]\n",
      " [-0.88744671]\n",
      " [-0.31101207]] ,b =  [0.27795046]\n",
      "step =  48800 error value =  243.52195255740907 W =  [[-0.95191121]\n",
      " [-3.77470017]\n",
      " [ 0.83183185]\n",
      " [-0.19783252]\n",
      " [-0.25495342]\n",
      " [-2.76056563]\n",
      " [-0.88744687]\n",
      " [-0.31101439]] ,b =  [0.27794798]\n",
      "step =  49200 error value =  243.52195255214528 W =  [[-0.95191169]\n",
      " [-3.77470066]\n",
      " [ 0.83184009]\n",
      " [-0.19782826]\n",
      " [-0.25495551]\n",
      " [-2.7605761 ]\n",
      " [-0.88744701]\n",
      " [-0.31101655]] ,b =  [0.27794569]\n",
      "step =  49600 error value =  243.52195254760647 W =  [[-0.95191213]\n",
      " [-3.77470111]\n",
      " [ 0.83184773]\n",
      " [-0.1978243 ]\n",
      " [-0.25495745]\n",
      " [-2.76058583]\n",
      " [-0.88744714]\n",
      " [-0.31101855]] ,b =  [0.27794355]\n",
      "step =  50000 error value =  243.5219525436928 W =  [[-0.95191255]\n",
      " [-3.77470153]\n",
      " [ 0.83185483]\n",
      " [-0.19782062]\n",
      " [-0.25495926]\n",
      " [-2.76059487]\n",
      " [-0.88744726]\n",
      " [-0.31102041]] ,b =  [0.27794158]\n",
      "step =  50400 error value =  243.5219525403181 W =  [[-0.95191293]\n",
      " [-3.77470192]\n",
      " [ 0.83186142]\n",
      " [-0.1978172 ]\n",
      " [-0.25496093]\n",
      " [-2.76060326]\n",
      " [-0.88744737]\n",
      " [-0.31102213]] ,b =  [0.27793974]\n",
      "step =  50800 error value =  243.52195253740823 W =  [[-0.95191329]\n",
      " [-3.77470229]\n",
      " [ 0.83186753]\n",
      " [-0.19781403]\n",
      " [-0.25496249]\n",
      " [-2.76061105]\n",
      " [-0.88744748]\n",
      " [-0.31102373]] ,b =  [0.27793804]\n",
      "step =  51200 error value =  243.52195253489913 W =  [[-0.95191362]\n",
      " [-3.77470263]\n",
      " [ 0.83187321]\n",
      " [-0.19781107]\n",
      " [-0.25496393]\n",
      " [-2.76061829]\n",
      " [-0.88744757]\n",
      " [-0.31102522]] ,b =  [0.27793647]\n",
      "step =  51600 error value =  243.52195253273555 W =  [[-0.95191393]\n",
      " [-3.77470294]\n",
      " [ 0.83187848]\n",
      " [-0.19780833]\n",
      " [-0.25496527]\n",
      " [-2.76062501]\n",
      " [-0.88744765]\n",
      " [-0.3110266 ]] ,b =  [0.277935]\n",
      "step =  52000 error value =  243.52195253086998 W =  [[-0.95191421]\n",
      " [-3.77470323]\n",
      " [ 0.83188337]\n",
      " [-0.19780578]\n",
      " [-0.25496652]\n",
      " [-2.76063125]\n",
      " [-0.88744773]\n",
      " [-0.31102789]] ,b =  [0.27793365]\n",
      "step =  52400 error value =  243.52195252926134 W =  [[-0.95191448]\n",
      " [-3.7747035 ]\n",
      " [ 0.83188791]\n",
      " [-0.19780341]\n",
      " [-0.25496768]\n",
      " [-2.76063705]\n",
      " [-0.8874478 ]\n",
      " [-0.31102908]] ,b =  [0.27793239]\n",
      "step =  52800 error value =  243.52195252787425 W =  [[-0.95191472]\n",
      " [-3.77470376]\n",
      " [ 0.83189213]\n",
      " [-0.19780121]\n",
      " [-0.25496876]\n",
      " [-2.76064243]\n",
      " [-0.88744787]\n",
      " [-0.31103018]] ,b =  [0.27793122]\n",
      "step =  53200 error value =  243.52195252667815 W =  [[-0.95191495]\n",
      " [-3.77470399]\n",
      " [ 0.83189604]\n",
      " [-0.19779916]\n",
      " [-0.25496975]\n",
      " [-2.76064743]\n",
      " [-0.88744793]\n",
      " [-0.31103121]] ,b =  [0.27793014]\n",
      "step =  53600 error value =  243.52195252564684 W =  [[-0.95191516]\n",
      " [-3.77470421]\n",
      " [ 0.83189968]\n",
      " [-0.19779726]\n",
      " [-0.25497068]\n",
      " [-2.76065208]\n",
      " [-0.88744799]\n",
      " [-0.31103216]] ,b =  [0.27792913]\n",
      "step =  54000 error value =  243.52195252475752 W =  [[-0.95191536]\n",
      " [-3.77470441]\n",
      " [ 0.83190305]\n",
      " [-0.1977955 ]\n",
      " [-0.25497155]\n",
      " [-2.76065639]\n",
      " [-0.88744804]\n",
      " [-0.31103305]] ,b =  [0.2779282]\n",
      "step =  54400 error value =  243.52195252399073 W =  [[-0.95191554]\n",
      " [-3.7747046 ]\n",
      " [ 0.83190618]\n",
      " [-0.19779385]\n",
      " [-0.25497235]\n",
      " [-2.76066039]\n",
      " [-0.88744809]\n",
      " [-0.31103387]] ,b =  [0.27792734]\n",
      "step =  54800 error value =  243.5219525233295 W =  [[-0.95191571]\n",
      " [-3.77470477]\n",
      " [ 0.83190909]\n",
      " [-0.19779233]\n",
      " [-0.25497309]\n",
      " [-2.76066411]\n",
      " [-0.88744813]\n",
      " [-0.31103463]] ,b =  [0.27792653]\n",
      "step =  55200 error value =  243.52195252275936 W =  [[-0.95191586]\n",
      " [-3.77470493]\n",
      " [ 0.83191178]\n",
      " [-0.19779091]\n",
      " [-0.25497378]\n",
      " [-2.76066757]\n",
      " [-0.88744817]\n",
      " [-0.31103534]] ,b =  [0.27792579]\n",
      "step =  55600 error value =  243.52195252226772 W =  [[-0.95191601]\n",
      " [-3.77470508]\n",
      " [ 0.83191429]\n",
      " [-0.1977896 ]\n",
      " [-0.25497442]\n",
      " [-2.76067077]\n",
      " [-0.88744821]\n",
      " [-0.311036  ]] ,b =  [0.2779251]\n",
      "step =  56000 error value =  243.52195252184382 W =  [[-0.95191615]\n",
      " [-3.77470522]\n",
      " [ 0.83191661]\n",
      " [-0.19778837]\n",
      " [-0.25497502]\n",
      " [-2.76067375]\n",
      " [-0.88744824]\n",
      " [-0.31103661]] ,b =  [0.27792446]\n",
      "step =  56400 error value =  243.52195252147825 W =  [[-0.95191627]\n",
      " [-3.77470535]\n",
      " [ 0.83191877]\n",
      " [-0.19778724]\n",
      " [-0.25497557]\n",
      " [-2.76067652]\n",
      " [-0.88744827]\n",
      " [-0.31103717]] ,b =  [0.27792386]\n",
      "step =  56800 error value =  243.5219525211631 W =  [[-0.95191639]\n",
      " [-3.77470547]\n",
      " [ 0.83192078]\n",
      " [-0.19778618]\n",
      " [-0.25497609]\n",
      " [-2.76067909]\n",
      " [-0.8874483 ]\n",
      " [-0.3110377 ]] ,b =  [0.27792331]\n",
      "step =  57200 error value =  243.5219525208913 W =  [[-0.9519165 ]\n",
      " [-3.77470558]\n",
      " [ 0.83192264]\n",
      " [-0.1977852 ]\n",
      " [-0.25497656]\n",
      " [-2.76068147]\n",
      " [-0.88744832]\n",
      " [-0.31103819]] ,b =  [0.2779228]\n",
      "step =  57600 error value =  243.52195252065692 W =  [[-0.9519166 ]\n",
      " [-3.77470569]\n",
      " [ 0.83192437]\n",
      " [-0.19778429]\n",
      " [-0.25497701]\n",
      " [-2.76068369]\n",
      " [-0.88744835]\n",
      " [-0.31103864]] ,b =  [0.27792232]\n",
      "step =  58000 error value =  243.52195252045485 W =  [[-0.95191669]\n",
      " [-3.77470578]\n",
      " [ 0.83192597]\n",
      " [-0.19778344]\n",
      " [-0.25497742]\n",
      " [-2.76068574]\n",
      " [-0.88744837]\n",
      " [-0.31103907]] ,b =  [0.27792188]\n",
      "step =  58400 error value =  243.5219525202806 W =  [[-0.95191678]\n",
      " [-3.77470587]\n",
      " [ 0.83192746]\n",
      " [-0.19778266]\n",
      " [-0.2549778 ]\n",
      " [-2.76068765]\n",
      " [-0.88744839]\n",
      " [-0.31103946]] ,b =  [0.27792147]\n",
      "step =  58800 error value =  243.52195252013033 W =  [[-0.95191686]\n",
      " [-3.77470595]\n",
      " [ 0.83192884]\n",
      " [-0.19778193]\n",
      " [-0.25497816]\n",
      " [-2.76068943]\n",
      " [-0.88744841]\n",
      " [-0.31103982]] ,b =  [0.2779211]\n",
      "step =  59200 error value =  243.52195252000075 W =  [[-0.95191693]\n",
      " [-3.77470603]\n",
      " [ 0.83193013]\n",
      " [-0.19778125]\n",
      " [-0.25497849]\n",
      " [-2.76069108]\n",
      " [-0.88744843]\n",
      " [-0.31104016]] ,b =  [0.27792074]\n",
      "step =  59600 error value =  243.52195251988906 W =  [[-0.951917  ]\n",
      " [-3.7747061 ]\n",
      " [ 0.83193132]\n",
      " [-0.19778062]\n",
      " [-0.25497879]\n",
      " [-2.76069261]\n",
      " [-0.88744844]\n",
      " [-0.31104047]] ,b =  [0.27792042]\n",
      "step =  60000 error value =  243.5219525197927 W =  [[-0.95191706]\n",
      " [-3.77470617]\n",
      " [ 0.83193242]\n",
      " [-0.19778003]\n",
      " [-0.25497908]\n",
      " [-2.76069403]\n",
      " [-0.88744846]\n",
      " [-0.31104076]] ,b =  [0.27792011]\n",
      "step =  60400 error value =  243.52195251970966 W =  [[-0.95191712]\n",
      " [-3.77470623]\n",
      " [ 0.83193345]\n",
      " [-0.19777949]\n",
      " [-0.25497934]\n",
      " [-2.76069535]\n",
      " [-0.88744847]\n",
      " [-0.31104103]] ,b =  [0.27791983]\n",
      "step =  60800 error value =  243.52195251963803 W =  [[-0.95191718]\n",
      " [-3.77470629]\n",
      " [ 0.8319344 ]\n",
      " [-0.19777898]\n",
      " [-0.25497959]\n",
      " [-2.76069657]\n",
      " [-0.88744848]\n",
      " [-0.31104128]] ,b =  [0.27791957]\n",
      "step =  61200 error value =  243.52195251957625 W =  [[-0.95191723]\n",
      " [-3.77470634]\n",
      " [ 0.83193529]\n",
      " [-0.19777851]\n",
      " [-0.25497982]\n",
      " [-2.76069771]\n",
      " [-0.88744849]\n",
      " [-0.31104151]] ,b =  [0.27791933]\n",
      "step =  61600 error value =  243.521952519523 W =  [[-0.95191728]\n",
      " [-3.77470639]\n",
      " [ 0.83193611]\n",
      " [-0.19777808]\n",
      " [-0.25498003]\n",
      " [-2.76069877]\n",
      " [-0.88744851]\n",
      " [-0.31104173]] ,b =  [0.2779191]\n",
      "step =  62000 error value =  243.52195251947708 W =  [[-0.95191732]\n",
      " [-3.77470644]\n",
      " [ 0.83193688]\n",
      " [-0.19777767]\n",
      " [-0.25498023]\n",
      " [-2.76069975]\n",
      " [-0.88744851]\n",
      " [-0.31104193]] ,b =  [0.27791889]\n",
      "step =  62400 error value =  243.5219525194375 W =  [[-0.95191736]\n",
      " [-3.77470648]\n",
      " [ 0.83193758]\n",
      " [-0.1977773 ]\n",
      " [-0.25498041]\n",
      " [-2.76070066]\n",
      " [-0.88744852]\n",
      " [-0.31104212]] ,b =  [0.2779187]\n",
      "step =  62800 error value =  243.52195251940333 W =  [[-0.9519174 ]\n",
      " [-3.77470652]\n",
      " [ 0.83193824]\n",
      " [-0.19777695]\n",
      " [-0.25498058]\n",
      " [-2.76070151]\n",
      " [-0.88744853]\n",
      " [-0.31104229]] ,b =  [0.27791852]\n",
      "step =  63200 error value =  243.5219525193739 W =  [[-0.95191744]\n",
      " [-3.77470656]\n",
      " [ 0.83193885]\n",
      " [-0.19777662]\n",
      " [-0.25498074]\n",
      " [-2.76070229]\n",
      " [-0.88744854]\n",
      " [-0.31104245]] ,b =  [0.27791835]\n",
      "step =  63600 error value =  243.5219525193485 W =  [[-0.95191747]\n",
      " [-3.77470659]\n",
      " [ 0.83193942]\n",
      " [-0.19777632]\n",
      " [-0.25498088]\n",
      " [-2.76070302]\n",
      " [-0.88744855]\n",
      " [-0.3110426 ]] ,b =  [0.2779182]\n",
      "step =  64000 error value =  243.52195251932662 W =  [[-0.9519175 ]\n",
      " [-3.77470662]\n",
      " [ 0.83193995]\n",
      " [-0.19777604]\n",
      " [-0.25498102]\n",
      " [-2.7607037 ]\n",
      " [-0.88744855]\n",
      " [-0.31104274]] ,b =  [0.27791806]\n",
      "step =  64400 error value =  243.52195251930772 W =  [[-0.95191753]\n",
      " [-3.77470665]\n",
      " [ 0.83194044]\n",
      " [-0.19777578]\n",
      " [-0.25498114]\n",
      " [-2.76070433]\n",
      " [-0.88744856]\n",
      " [-0.31104287]] ,b =  [0.27791792]\n",
      "step =  64800 error value =  243.52195251929146 W =  [[-0.95191756]\n",
      " [-3.77470668]\n",
      " [ 0.83194089]\n",
      " [-0.19777554]\n",
      " [-0.25498126]\n",
      " [-2.76070491]\n",
      " [-0.88744856]\n",
      " [-0.31104299]] ,b =  [0.2779178]\n",
      "step =  65200 error value =  243.52195251927745 W =  [[-0.95191758]\n",
      " [-3.77470671]\n",
      " [ 0.83194131]\n",
      " [-0.19777531]\n",
      " [-0.25498137]\n",
      " [-2.76070546]\n",
      " [-0.88744857]\n",
      " [-0.3110431 ]] ,b =  [0.27791768]\n",
      "step =  65600 error value =  243.5219525192653 W =  [[-0.9519176 ]\n",
      " [-3.77470673]\n",
      " [ 0.8319417 ]\n",
      " [-0.1977751 ]\n",
      " [-0.25498147]\n",
      " [-2.76070596]\n",
      " [-0.88744857]\n",
      " [-0.3110432 ]] ,b =  [0.27791758]\n",
      "step =  66000 error value =  243.5219525192549 W =  [[-0.95191762]\n",
      " [-3.77470675]\n",
      " [ 0.83194207]\n",
      " [-0.19777491]\n",
      " [-0.25498157]\n",
      " [-2.76070643]\n",
      " [-0.88744858]\n",
      " [-0.3110433 ]] ,b =  [0.27791748]\n",
      "step =  66400 error value =  243.52195251924587 W =  [[-0.95191764]\n",
      " [-3.77470677]\n",
      " [ 0.8319424 ]\n",
      " [-0.19777473]\n",
      " [-0.25498165]\n",
      " [-2.76070686]\n",
      " [-0.88744858]\n",
      " [-0.31104339]] ,b =  [0.27791739]\n",
      "step =  66800 error value =  243.5219525192381 W =  [[-0.95191766]\n",
      " [-3.77470679]\n",
      " [ 0.83194272]\n",
      " [-0.19777456]\n",
      " [-0.25498173]\n",
      " [-2.76070727]\n",
      " [-0.88744859]\n",
      " [-0.31104347]] ,b =  [0.2779173]\n",
      "step =  67200 error value =  243.52195251923143 W =  [[-0.95191768]\n",
      " [-3.77470681]\n",
      " [ 0.83194301]\n",
      " [-0.19777441]\n",
      " [-0.25498181]\n",
      " [-2.76070764]\n",
      " [-0.88744859]\n",
      " [-0.31104354]] ,b =  [0.27791722]\n",
      "step =  67600 error value =  243.52195251922564 W =  [[-0.95191769]\n",
      " [-3.77470682]\n",
      " [ 0.83194328]\n",
      " [-0.19777426]\n",
      " [-0.25498188]\n",
      " [-2.76070799]\n",
      " [-0.88744859]\n",
      " [-0.31104362]] ,b =  [0.27791715]\n",
      "step =  68000 error value =  243.5219525192207 W =  [[-0.95191771]\n",
      " [-3.77470684]\n",
      " [ 0.83194353]\n",
      " [-0.19777413]\n",
      " [-0.25498194]\n",
      " [-2.76070831]\n",
      " [-0.8874486 ]\n",
      " [-0.31104368]] ,b =  [0.27791708]\n",
      "step =  68400 error value =  243.52195251921637 W =  [[-0.95191772]\n",
      " [-3.77470685]\n",
      " [ 0.83194376]\n",
      " [-0.19777401]\n",
      " [-0.254982  ]\n",
      " [-2.76070861]\n",
      " [-0.8874486 ]\n",
      " [-0.31104374]] ,b =  [0.27791702]\n",
      "step =  68800 error value =  243.52195251921268 W =  [[-0.95191773]\n",
      " [-3.77470687]\n",
      " [ 0.83194398]\n",
      " [-0.19777389]\n",
      " [-0.25498206]\n",
      " [-2.76070889]\n",
      " [-0.8874486 ]\n",
      " [-0.3110438 ]] ,b =  [0.27791696]\n",
      "step =  69200 error value =  243.5219525192095 W =  [[-0.95191775]\n",
      " [-3.77470688]\n",
      " [ 0.83194418]\n",
      " [-0.19777378]\n",
      " [-0.25498211]\n",
      " [-2.76070915]\n",
      " [-0.8874486 ]\n",
      " [-0.31104385]] ,b =  [0.2779169]\n",
      "step =  69600 error value =  243.52195251920676 W =  [[-0.95191776]\n",
      " [-3.77470689]\n",
      " [ 0.83194436]\n",
      " [-0.19777368]\n",
      " [-0.25498216]\n",
      " [-2.76070939]\n",
      " [-0.8874486 ]\n",
      " [-0.3110439 ]] ,b =  [0.27791685]\n",
      "step =  70000 error value =  243.52195251920438 W =  [[-0.95191777]\n",
      " [-3.7747069 ]\n",
      " [ 0.83194454]\n",
      " [-0.19777359]\n",
      " [-0.2549822 ]\n",
      " [-2.76070961]\n",
      " [-0.88744861]\n",
      " [-0.31104395]] ,b =  [0.27791681]\n",
      "step =  70400 error value =  243.52195251920233 W =  [[-0.95191778]\n",
      " [-3.77470691]\n",
      " [ 0.8319447 ]\n",
      " [-0.1977735 ]\n",
      " [-0.25498225]\n",
      " [-2.76070982]\n",
      " [-0.88744861]\n",
      " [-0.31104399]] ,b =  [0.27791676]\n",
      "step =  70800 error value =  243.5219525192006 W =  [[-0.95191778]\n",
      " [-3.77470692]\n",
      " [ 0.83194485]\n",
      " [-0.19777342]\n",
      " [-0.25498229]\n",
      " [-2.76071001]\n",
      " [-0.88744861]\n",
      " [-0.31104403]] ,b =  [0.27791672]\n",
      "step =  71200 error value =  243.52195251919906 W =  [[-0.95191779]\n",
      " [-3.77470693]\n",
      " [ 0.83194498]\n",
      " [-0.19777335]\n",
      " [-0.25498232]\n",
      " [-2.76071019]\n",
      " [-0.88744861]\n",
      " [-0.31104407]] ,b =  [0.27791668]\n",
      "step =  71600 error value =  243.52195251919775 W =  [[-0.9519178 ]\n",
      " [-3.77470694]\n",
      " [ 0.83194511]\n",
      " [-0.19777328]\n",
      " [-0.25498235]\n",
      " [-2.76071036]\n",
      " [-0.88744861]\n",
      " [-0.3110441 ]] ,b =  [0.27791665]\n",
      "step =  72000 error value =  243.52195251919662 W =  [[-0.95191781]\n",
      " [-3.77470694]\n",
      " [ 0.83194523]\n",
      " [-0.19777322]\n",
      " [-0.25498239]\n",
      " [-2.76071051]\n",
      " [-0.88744861]\n",
      " [-0.31104413]] ,b =  [0.27791662]\n",
      "step =  72400 error value =  243.52195251919562 W =  [[-0.95191781]\n",
      " [-3.77470695]\n",
      " [ 0.83194534]\n",
      " [-0.19777316]\n",
      " [-0.25498241]\n",
      " [-2.76071066]\n",
      " [-0.88744862]\n",
      " [-0.31104416]] ,b =  [0.27791659]\n",
      "step =  72800 error value =  243.5219525191948 W =  [[-0.95191782]\n",
      " [-3.77470696]\n",
      " [ 0.83194545]\n",
      " [-0.1977731 ]\n",
      " [-0.25498244]\n",
      " [-2.76071079]\n",
      " [-0.88744862]\n",
      " [-0.31104419]] ,b =  [0.27791656]\n",
      "step =  73200 error value =  243.5219525191941 W =  [[-0.95191783]\n",
      " [-3.77470696]\n",
      " [ 0.83194554]\n",
      " [-0.19777305]\n",
      " [-0.25498247]\n",
      " [-2.76071091]\n",
      " [-0.88744862]\n",
      " [-0.31104421]] ,b =  [0.27791653]\n",
      "step =  73600 error value =  243.52195251919346 W =  [[-0.95191783]\n",
      " [-3.77470697]\n",
      " [ 0.83194563]\n",
      " [-0.197773  ]\n",
      " [-0.25498249]\n",
      " [-2.76071103]\n",
      " [-0.88744862]\n",
      " [-0.31104424]] ,b =  [0.27791651]\n",
      "step =  74000 error value =  243.5219525191929 W =  [[-0.95191784]\n",
      " [-3.77470697]\n",
      " [ 0.83194571]\n",
      " [-0.19777296]\n",
      " [-0.25498251]\n",
      " [-2.76071113]\n",
      " [-0.88744862]\n",
      " [-0.31104426]] ,b =  [0.27791649]\n",
      "step =  74400 error value =  243.52195251919247 W =  [[-0.95191784]\n",
      " [-3.77470698]\n",
      " [ 0.83194579]\n",
      " [-0.19777292]\n",
      " [-0.25498253]\n",
      " [-2.76071123]\n",
      " [-0.88744862]\n",
      " [-0.31104428]] ,b =  [0.27791647]\n",
      "step =  74800 error value =  243.52195251919204 W =  [[-0.95191784]\n",
      " [-3.77470698]\n",
      " [ 0.83194586]\n",
      " [-0.19777288]\n",
      " [-0.25498255]\n",
      " [-2.76071132]\n",
      " [-0.88744862]\n",
      " [-0.3110443 ]] ,b =  [0.27791645]\n",
      "step =  75200 error value =  243.52195251919173 W =  [[-0.95191785]\n",
      " [-3.77470698]\n",
      " [ 0.83194593]\n",
      " [-0.19777285]\n",
      " [-0.25498257]\n",
      " [-2.76071141]\n",
      " [-0.88744862]\n",
      " [-0.31104431]] ,b =  [0.27791643]\n",
      "step =  75600 error value =  243.5219525191914 W =  [[-0.95191785]\n",
      " [-3.77470699]\n",
      " [ 0.83194599]\n",
      " [-0.19777281]\n",
      " [-0.25498258]\n",
      " [-2.76071149]\n",
      " [-0.88744862]\n",
      " [-0.31104433]] ,b =  [0.27791641]\n",
      "step =  76000 error value =  243.52195251919113 W =  [[-0.95191785]\n",
      " [-3.77470699]\n",
      " [ 0.83194604]\n",
      " [-0.19777278]\n",
      " [-0.2549826 ]\n",
      " [-2.76071156]\n",
      " [-0.88744862]\n",
      " [-0.31104435]] ,b =  [0.2779164]\n",
      "step =  76400 error value =  243.52195251919093 W =  [[-0.95191786]\n",
      " [-3.77470699]\n",
      " [ 0.8319461 ]\n",
      " [-0.19777275]\n",
      " [-0.25498261]\n",
      " [-2.76071163]\n",
      " [-0.88744862]\n",
      " [-0.31104436]] ,b =  [0.27791638]\n",
      "step =  76800 error value =  243.52195251919073 W =  [[-0.95191786]\n",
      " [-3.774707  ]\n",
      " [ 0.83194615]\n",
      " [-0.19777273]\n",
      " [-0.25498262]\n",
      " [-2.76071169]\n",
      " [-0.88744862]\n",
      " [-0.31104437]] ,b =  [0.27791637]\n",
      "step =  77200 error value =  243.52195251919056 W =  [[-0.95191786]\n",
      " [-3.774707  ]\n",
      " [ 0.83194619]\n",
      " [-0.1977727 ]\n",
      " [-0.25498263]\n",
      " [-2.76071175]\n",
      " [-0.88744862]\n",
      " [-0.31104438]] ,b =  [0.27791636]\n",
      "step =  77600 error value =  243.52195251919042 W =  [[-0.95191787]\n",
      " [-3.774707  ]\n",
      " [ 0.83194623]\n",
      " [-0.19777268]\n",
      " [-0.25498265]\n",
      " [-2.76071181]\n",
      " [-0.88744863]\n",
      " [-0.3110444 ]] ,b =  [0.27791634]\n",
      "step =  78000 error value =  243.52195251919028 W =  [[-0.95191787]\n",
      " [-3.77470701]\n",
      " [ 0.83194627]\n",
      " [-0.19777266]\n",
      " [-0.25498266]\n",
      " [-2.76071186]\n",
      " [-0.88744863]\n",
      " [-0.31104441]] ,b =  [0.27791633]\n",
      "step =  78400 error value =  243.5219525191902 W =  [[-0.95191787]\n",
      " [-3.77470701]\n",
      " [ 0.83194631]\n",
      " [-0.19777264]\n",
      " [-0.25498266]\n",
      " [-2.76071191]\n",
      " [-0.88744863]\n",
      " [-0.31104441]] ,b =  [0.27791632]\n",
      "step =  78800 error value =  243.5219525191901 W =  [[-0.95191787]\n",
      " [-3.77470701]\n",
      " [ 0.83194634]\n",
      " [-0.19777262]\n",
      " [-0.25498267]\n",
      " [-2.76071195]\n",
      " [-0.88744863]\n",
      " [-0.31104442]] ,b =  [0.27791632]\n",
      "step =  79200 error value =  243.52195251919002 W =  [[-0.95191787]\n",
      " [-3.77470701]\n",
      " [ 0.83194637]\n",
      " [-0.1977726 ]\n",
      " [-0.25498268]\n",
      " [-2.76071199]\n",
      " [-0.88744863]\n",
      " [-0.31104443]] ,b =  [0.27791631]\n",
      "step =  79600 error value =  243.52195251918994 W =  [[-0.95191788]\n",
      " [-3.77470701]\n",
      " [ 0.8319464 ]\n",
      " [-0.19777259]\n",
      " [-0.25498269]\n",
      " [-2.76071203]\n",
      " [-0.88744863]\n",
      " [-0.31104444]] ,b =  [0.2779163]\n",
      "step =  80000 error value =  243.52195251918988 W =  [[-0.95191788]\n",
      " [-3.77470701]\n",
      " [ 0.83194643]\n",
      " [-0.19777257]\n",
      " [-0.2549827 ]\n",
      " [-2.76071206]\n",
      " [-0.88744863]\n",
      " [-0.31104445]] ,b =  [0.27791629]\n",
      "step =  80400 error value =  243.52195251918982 W =  [[-0.95191788]\n",
      " [-3.77470702]\n",
      " [ 0.83194646]\n",
      " [-0.19777256]\n",
      " [-0.2549827 ]\n",
      " [-2.7607121 ]\n",
      " [-0.88744863]\n",
      " [-0.31104445]] ,b =  [0.27791628]\n",
      "step =  80800 error value =  243.52195251918982 W =  [[-0.95191788]\n",
      " [-3.77470702]\n",
      " [ 0.83194648]\n",
      " [-0.19777255]\n",
      " [-0.25498271]\n",
      " [-2.76071213]\n",
      " [-0.88744863]\n",
      " [-0.31104446]] ,b =  [0.27791628]\n",
      "step =  81200 error value =  243.52195251918977 W =  [[-0.95191788]\n",
      " [-3.77470702]\n",
      " [ 0.8319465 ]\n",
      " [-0.19777254]\n",
      " [-0.25498271]\n",
      " [-2.76071215]\n",
      " [-0.88744863]\n",
      " [-0.31104447]] ,b =  [0.27791627]\n",
      "step =  81600 error value =  243.5219525191897 W =  [[-0.95191788]\n",
      " [-3.77470702]\n",
      " [ 0.83194652]\n",
      " [-0.19777253]\n",
      " [-0.25498272]\n",
      " [-2.76071218]\n",
      " [-0.88744863]\n",
      " [-0.31104447]] ,b =  [0.27791627]\n",
      "step =  82000 error value =  243.5219525191897 W =  [[-0.95191788]\n",
      " [-3.77470702]\n",
      " [ 0.83194654]\n",
      " [-0.19777252]\n",
      " [-0.25498272]\n",
      " [-2.7607122 ]\n",
      " [-0.88744863]\n",
      " [-0.31104448]] ,b =  [0.27791626]\n",
      "step =  82400 error value =  243.5219525191897 W =  [[-0.95191788]\n",
      " [-3.77470702]\n",
      " [ 0.83194656]\n",
      " [-0.19777251]\n",
      " [-0.25498273]\n",
      " [-2.76071223]\n",
      " [-0.88744863]\n",
      " [-0.31104448]] ,b =  [0.27791626]\n",
      "step =  82800 error value =  243.52195251918965 W =  [[-0.95191788]\n",
      " [-3.77470702]\n",
      " [ 0.83194657]\n",
      " [-0.1977725 ]\n",
      " [-0.25498273]\n",
      " [-2.76071225]\n",
      " [-0.88744863]\n",
      " [-0.31104448]] ,b =  [0.27791625]\n",
      "step =  83200 error value =  243.52195251918965 W =  [[-0.95191789]\n",
      " [-3.77470702]\n",
      " [ 0.83194659]\n",
      " [-0.19777249]\n",
      " [-0.25498274]\n",
      " [-2.76071227]\n",
      " [-0.88744863]\n",
      " [-0.31104449]] ,b =  [0.27791625]\n",
      "step =  83600 error value =  243.52195251918963 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.8319466 ]\n",
      " [-0.19777248]\n",
      " [-0.25498274]\n",
      " [-2.76071228]\n",
      " [-0.88744863]\n",
      " [-0.31104449]] ,b =  [0.27791624]\n",
      "step =  84000 error value =  243.52195251918965 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194662]\n",
      " [-0.19777247]\n",
      " [-0.25498274]\n",
      " [-2.7607123 ]\n",
      " [-0.88744863]\n",
      " [-0.3110445 ]] ,b =  [0.27791624]\n",
      "step =  84400 error value =  243.5219525191896 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194663]\n",
      " [-0.19777247]\n",
      " [-0.25498275]\n",
      " [-2.76071232]\n",
      " [-0.88744863]\n",
      " [-0.3110445 ]] ,b =  [0.27791624]\n",
      "step =  84800 error value =  243.5219525191896 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194664]\n",
      " [-0.19777246]\n",
      " [-0.25498275]\n",
      " [-2.76071233]\n",
      " [-0.88744863]\n",
      " [-0.3110445 ]] ,b =  [0.27791624]\n",
      "step =  85200 error value =  243.52195251918957 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194665]\n",
      " [-0.19777246]\n",
      " [-0.25498275]\n",
      " [-2.76071234]\n",
      " [-0.88744863]\n",
      " [-0.3110445 ]] ,b =  [0.27791623]\n",
      "step =  85600 error value =  243.52195251918957 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194666]\n",
      " [-0.19777245]\n",
      " [-0.25498276]\n",
      " [-2.76071236]\n",
      " [-0.88744863]\n",
      " [-0.31104451]] ,b =  [0.27791623]\n",
      "step =  86000 error value =  243.52195251918954 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194667]\n",
      " [-0.19777245]\n",
      " [-0.25498276]\n",
      " [-2.76071237]\n",
      " [-0.88744863]\n",
      " [-0.31104451]] ,b =  [0.27791623]\n",
      "step =  86400 error value =  243.52195251918957 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194668]\n",
      " [-0.19777244]\n",
      " [-0.25498276]\n",
      " [-2.76071238]\n",
      " [-0.88744863]\n",
      " [-0.31104451]] ,b =  [0.27791623]\n",
      "step =  86800 error value =  243.5219525191896 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194668]\n",
      " [-0.19777244]\n",
      " [-0.25498276]\n",
      " [-2.76071239]\n",
      " [-0.88744863]\n",
      " [-0.31104451]] ,b =  [0.27791622]\n",
      "step =  87200 error value =  243.52195251918957 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194669]\n",
      " [-0.19777243]\n",
      " [-0.25498276]\n",
      " [-2.7607124 ]\n",
      " [-0.88744863]\n",
      " [-0.31104452]] ,b =  [0.27791622]\n",
      "step =  87600 error value =  243.52195251918954 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.8319467 ]\n",
      " [-0.19777243]\n",
      " [-0.25498277]\n",
      " [-2.76071241]\n",
      " [-0.88744863]\n",
      " [-0.31104452]] ,b =  [0.27791622]\n",
      "step =  88000 error value =  243.52195251918957 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.8319467 ]\n",
      " [-0.19777243]\n",
      " [-0.25498277]\n",
      " [-2.76071242]\n",
      " [-0.88744863]\n",
      " [-0.31104452]] ,b =  [0.27791622]\n",
      "step =  88400 error value =  243.52195251918957 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194671]\n",
      " [-0.19777242]\n",
      " [-0.25498277]\n",
      " [-2.76071242]\n",
      " [-0.88744863]\n",
      " [-0.31104452]] ,b =  [0.27791622]\n",
      "step =  88800 error value =  243.52195251918954 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194671]\n",
      " [-0.19777242]\n",
      " [-0.25498277]\n",
      " [-2.76071243]\n",
      " [-0.88744863]\n",
      " [-0.31104452]] ,b =  [0.27791621]\n",
      "step =  89200 error value =  243.52195251918954 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194672]\n",
      " [-0.19777242]\n",
      " [-0.25498277]\n",
      " [-2.76071244]\n",
      " [-0.88744863]\n",
      " [-0.31104452]] ,b =  [0.27791621]\n",
      "step =  89600 error value =  243.52195251918954 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194672]\n",
      " [-0.19777242]\n",
      " [-0.25498277]\n",
      " [-2.76071244]\n",
      " [-0.88744863]\n",
      " [-0.31104452]] ,b =  [0.27791621]\n",
      "step =  90000 error value =  243.52195251918954 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194673]\n",
      " [-0.19777241]\n",
      " [-0.25498277]\n",
      " [-2.76071245]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.27791621]\n",
      "step =  90400 error value =  243.52195251918954 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194673]\n",
      " [-0.19777241]\n",
      " [-0.25498277]\n",
      " [-2.76071245]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.27791621]\n",
      "step =  90800 error value =  243.52195251918954 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194674]\n",
      " [-0.19777241]\n",
      " [-0.25498278]\n",
      " [-2.76071246]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.27791621]\n",
      "step =  91200 error value =  243.52195251918954 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194674]\n",
      " [-0.19777241]\n",
      " [-0.25498278]\n",
      " [-2.76071246]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.27791621]\n",
      "step =  91600 error value =  243.5219525191895 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194674]\n",
      " [-0.19777241]\n",
      " [-0.25498278]\n",
      " [-2.76071247]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.27791621]\n",
      "step =  92000 error value =  243.52195251918954 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194675]\n",
      " [-0.1977724 ]\n",
      " [-0.25498278]\n",
      " [-2.76071247]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.27791621]\n",
      "step =  92400 error value =  243.52195251918954 W =  [[-0.95191789]\n",
      " [-3.77470703]\n",
      " [ 0.83194675]\n",
      " [-0.1977724 ]\n",
      " [-0.25498278]\n",
      " [-2.76071247]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.27791621]\n",
      "step =  92800 error value =  243.52195251918948 W =  [[-0.9519179 ]\n",
      " [-3.77470703]\n",
      " [ 0.83194675]\n",
      " [-0.1977724 ]\n",
      " [-0.25498278]\n",
      " [-2.76071248]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.27791621]\n",
      "step =  93200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470703]\n",
      " [ 0.83194675]\n",
      " [-0.1977724 ]\n",
      " [-0.25498278]\n",
      " [-2.76071248]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.2779162]\n",
      "step =  93600 error value =  243.5219525191895 W =  [[-0.9519179 ]\n",
      " [-3.77470703]\n",
      " [ 0.83194675]\n",
      " [-0.1977724 ]\n",
      " [-0.25498278]\n",
      " [-2.76071248]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.2779162]\n",
      "step =  94000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470703]\n",
      " [ 0.83194676]\n",
      " [-0.1977724 ]\n",
      " [-0.25498278]\n",
      " [-2.76071248]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.2779162]\n",
      "step =  94400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470703]\n",
      " [ 0.83194676]\n",
      " [-0.1977724 ]\n",
      " [-0.25498278]\n",
      " [-2.76071249]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.2779162]\n",
      "step =  94800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194676]\n",
      " [-0.1977724 ]\n",
      " [-0.25498278]\n",
      " [-2.76071249]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.2779162]\n",
      "step =  95200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194676]\n",
      " [-0.1977724 ]\n",
      " [-0.25498278]\n",
      " [-2.76071249]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.2779162]\n",
      "step =  95600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194676]\n",
      " [-0.19777239]\n",
      " [-0.25498278]\n",
      " [-2.76071249]\n",
      " [-0.88744863]\n",
      " [-0.31104453]] ,b =  [0.2779162]\n",
      "step =  96000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194677]\n",
      " [-0.19777239]\n",
      " [-0.25498278]\n",
      " [-2.7607125 ]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  96400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194677]\n",
      " [-0.19777239]\n",
      " [-0.25498278]\n",
      " [-2.7607125 ]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  96800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194677]\n",
      " [-0.19777239]\n",
      " [-0.25498278]\n",
      " [-2.7607125 ]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  97200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194677]\n",
      " [-0.19777239]\n",
      " [-0.25498278]\n",
      " [-2.7607125 ]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  97600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194677]\n",
      " [-0.19777239]\n",
      " [-0.25498278]\n",
      " [-2.7607125 ]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  98000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194677]\n",
      " [-0.19777239]\n",
      " [-0.25498278]\n",
      " [-2.7607125 ]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  98400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194677]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.7607125 ]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  98800 error value =  243.5219525191895 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194677]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.7607125 ]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  99200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194677]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  99600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194677]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  100000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194677]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  100400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  100800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  101200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  101600 error value =  243.5219525191895 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  102000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  102400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  102800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  103200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  103600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  104000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  104400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  104800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  105200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  105600 error value =  243.5219525191895 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071251]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  106000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  106400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  106800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  107200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  107600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777239]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  108000 error value =  243.5219525191895 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  108400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  108800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  109200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  109600 error value =  243.5219525191895 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  110000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  110400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  110800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  111200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  111600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  112000 error value =  243.5219525191895 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  112400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  112800 error value =  243.5219525191895 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  113200 error value =  243.5219525191895 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  113600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  114000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  114400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  114800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  115200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  115600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  116000 error value =  243.5219525191895 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  116400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  116800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  117200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  117600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  118000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  118400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  118800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  119200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  119600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  120000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  120400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  120800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  121200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  121600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  122000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  122400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  122800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  123200 error value =  243.5219525191895 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  123600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  124000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  124400 error value =  243.5219525191895 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  124800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  125200 error value =  243.52195251918948 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  125600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  126000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  126400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  126800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  127200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  127600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  128000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  128400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  128800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  129200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  129600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  130000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  130400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  130800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  131200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  131600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  132000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  132400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  132800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  133200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  133600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  134000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  134400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  134800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  135200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  135600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  136000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  136400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  136800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  137200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  137600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  138000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  138400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  138800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  139200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  139600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  140000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  140400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  140800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  141200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  141600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  142000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  142400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  142800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  143200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  143600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  144000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  144400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  144800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  145200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  145600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  146000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  146400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  146800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  147200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  147600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  148000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  148400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  148800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  149200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  149600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  150000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  150400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  150800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  151200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  151600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  152000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  152400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  152800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  153200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  153600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  154000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  154400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  154800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  155200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  155600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  156000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  156400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  156800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  157200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  157600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  158000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  158400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  158800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  159200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  159600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  160000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  160400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  160800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  161200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  161600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  162000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  162400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  162800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  163200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  163600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  164000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  164400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  164800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  165200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  165600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  166000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  166400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  166800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  167200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  167600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  168000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  168400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  168800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  169200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  169600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  170000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  170400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  170800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  171200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  171600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  172000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  172400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  172800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  173200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  173600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  174000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  174400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  174800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  175200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  175600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  176000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  176400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  176800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  177200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  177600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  178000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  178400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  178800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  179200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  179600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  180000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  180400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  180800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  181200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  181600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  182000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  182400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  182800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  183200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  183600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  184000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  184400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  184800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  185200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  185600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  186000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  186400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  186800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  187200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  187600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  188000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  188400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  188800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  189200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  189600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  190000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  190400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  190800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  191200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  191600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  192000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  192400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  192800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  193200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  193600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  194000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  194400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  194800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  195200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  195600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  196000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  196400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  196800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  197200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  197600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  198000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  198400 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  198800 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  199200 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  199600 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n",
      "step =  200000 error value =  243.52195251918954 W =  [[-0.9519179 ]\n",
      " [-3.77470704]\n",
      " [ 0.83194678]\n",
      " [-0.19777238]\n",
      " [-0.25498279]\n",
      " [-2.76071252]\n",
      " [-0.88744863]\n",
      " [-0.31104454]] ,b =  [0.2779162]\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1e-4 # 발산할 때 1e-3~1e-6 값 바꿔 시도\n",
    "\n",
    "f=lambda x: loss_func(x_data,t_data)\n",
    "print(\"Initial error value = \", error_val(x_data,t_data), \"Initial W = \", W, \"\\n\", \",b=\",b)\n",
    "\n",
    "for step in range(200001):\n",
    "    W -= learning_rate * numerical_derivative(f,W)\n",
    "    b -= learning_rate * numerical_derivative(f,b)\n",
    "    \n",
    "    if(step % 400 == 0):\n",
    "        print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \",b = \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0779c12a-641c-4445-b6c3-dac62c4b4915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_x_data=np.array([12,0]) #(예습, 복습)=(12,0) => Pass(1)\n",
    "# predict(test_t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc91dab4-d727-4309-bc30-d048a1e4d66e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80546983] 1 [1.]\n",
      "[0.85722397] 1 [1.]\n",
      "[0.70161148] 1 [0.]\n",
      "[0.60961074] 1 [1.]\n",
      "[0.78792769] 1 [1.]\n"
     ]
    }
   ],
   "source": [
    "# 위 혹은 아래 방식으로\n",
    "for i in range(5):\n",
    "    (a,b)=predict(test_x_data[i])\n",
    "    print(a,b, test_t_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d763f41-ff90-4eeb-a22f-4fe782a80c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값 a,b 와 실제 정답[test_t_data]를 비교하여 같을 때 예측 성공한 것"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
